In the previous part we had described about the number sliding problem and in the end we had relaxed certain constraints. In the relaxed approach our computation had decreased by a significant amount. Whenever we do a relaxation we ensure that it will be acting as a lower bound. Each heuristic function has a lower bound for it to be admissible or not. So when we relax some constraint we ensure that it is the lower bound. 
For a problem, there can be various admissible heuristics. So there will be a tradeoff between accuracy of heuristics and computation of the heuristic. 
On the above example we can choose manhatten distance as the heuristic function rather than the tiles out of place. So in comaprison to previous heuristic, there will be more computation however it will provide better approximation of the problem. So using this heuristic function in relaxed part, there will be lesser calculation involved. 
So how about we administer the actual cost as the heuristic function? We can do it, also it will be the admissible heuristic function as it will always be less than or equal to the actual cost. But the thing which is wrong here is that we will have to increase the computation and the nodes visited will be larger than some other better heuristic function. So it maynot be always be feasible.
So there is always a tradeoff between a quality of estimate and the number of nodes expanded and searched. So with A*, if we use actual cost as the heuristic function, then it will become UCS and thus computation will be the same as UCS. 
A* will become UCS(Uniform Cost Search) if we choose heuristic function as 0. 0 always can be chosen as a heuristic function. 
Dominance : We can say that a heuristic function ha dominates hb if for every node , ha(n) >=hb(n) .So if ha is admissible , then hb is also admissible. Also it may happen that we are not able to compare two heuristics, but there is some ordering between them, which helps us select one.

Graph Search. Till now we have discussed Search Trees. In tree search it may happen that we have to do a redundant search. But in graph search this is avoided.
So when we do a graph search , then we store the expanded node in a set, and not in the list as in list we will have to traverse through the whole list in order to know about the existance of a particular node or not. So it will increase computation. Set helps here, it wont take long to find that the node is comouted or not.
So whenever we expand a node, we first check if it is in the expanded set or not. If it is there we will not expand it. Else we will expand it and store it. So this appraoch is complete as it will eventually give a solution. The optimality of the appraoch can be decided by the way we choose to expand the node from the fringe.
There can be cases in which , the A* approach in graph search may fail to give us the optimal solution. Lets say we have a graph with 5 nodes S,A,B,C and G with heuristic distance to G from node 2,4,1,1,0 repectively. We have edges from S to A, S to B, A to C, B to C,and C to G. Each edge has weight 1,1,1,2 and 3 respectively. Using A* search approach here will give us S,B,C,G as the answer but the optimal solution would be S,A,C,G.
This is because A(1,4) will be regarded as higher than B(1,1). The problem here is in heuristic function. So we may have to make changes in it, this is because if we ignore the heuristic part, we get correct answer. 
So this is where we will use the term called consistency of heuristics. Earlier we used term admissible heuristic function , which means that the heuristic costs<= actual costs. Now , we will use admissible and consistent heuristic function. 
In case of consistency, the h(A) - h(B) <= arc cost(AB) or the edge cost AB. Also h(A)- h(B) should also be greater than 0. So in previous example, the heuristic function was inconsistent , which is why we got wrong answer. 
We can see, that if we use a consistent heuristic function, then the function f(n) = h(n) + g(n) will not decrease.

Considering when we use consistent heuristic function in A* appraoch. So in tree search, the priority is always based on the f function.Thus we are expanding in the increasing value of f in fringe. So in graph search, we are about to expand a node and we find that there is another path which is of less cost then will explore that path first. f function will be set such a way by the consistent heuristic function that it will help us reach the optimal path.
Thus we can say that A* search gives optimal solution (given we are using consistent heuristic)
So to summarise, A* uses both forward and backward costs, is optimal with admissible/consistent heuristics. And the key part is the design of heuristic. We often try to use relaxed problem.
Algorithm is some thing like, we are given a problem and a fringe. Algorithm will return either a solution or a fringe. Add the root node in the fringe and add its adjacent nodes to the fringe. Expand the node from the fringe whose cost is lowest and remove it from the fringe and add the expanded node in a set. Repeat untill we reach the solution node. If in any case the fringe is empty, then we return a failure.  

First the question arise, why do we need search for. We assume that the world has single agent,deterministic actions, fully observable states, which change on the basis of actions. Planning is needed because we are given a goal and we are interested in identifying the path of the goal. And there can be multiple paths for the goal like different depths, cost, hops etc. So lets say in A* search algorithm, it has a heuristic function which helps us guide to the final destination. So characteristics like these are characteristics of planning.
Secondly it is identification of tasks. In this we are given goals and we have to look for the goal. The goal is important. For example we are two values x,and y. So lets say x can take values from positive integers and y takes values from negative integers. This puts a constraint over the values of x and y.Our objective may be to assign these values as per the constraint. So it may happen that the goals might be at the same depth, but the approach might be different.

Standard Search Problems. In this, each state is considered an arbitrary data structure which defines the current moment or value based on our actions. It basically behaves like a black box. A state changes on the basis of the successor function which is determined by the algorithm. 
Constraint Satisfaction Problem is a subset of search problems. In this we are given variables(X) which are to be set in accordance to the domain given to us(D). So in this goal test can be the set of constraints with which we can set the variables to the values equal to some subset of D in accordance to the constraints. An example is map coloring problem. This is basically we have a set of states in a country and we have to colour these states. The constraints are such that no two adjacent state should have the same colour. We are also given the set of colors to choose upon. 
The method of showing these constraints can be different, i;e implicit and explicit way. Implicit can be like Na != Nb (node a != Node b). Explicit way can be like (Na,Nb) should belong to {(red,green), (green, blue) ....}. So the solution can be  assignment of the variables from one of the given possibility.
Similarly we can have an example of having a chess board of 4 X 4 in which we have to place 4 queens such that they cannot kill eachother. So the variables can the the co-ordinates of the chess boards. We can set the constraints such that the queens cannot kill each other. Like in a single row, place only 1 queen and in a single column place a single queen. Don't place a queen on the diagonal of already placed queen. We need one more constraint in which we can check if the total queens placed is equal to 4. Because placing no queen also satisfies the given constraint. The constraint mentioned here is implicit constraint. 
Better approach can be to associate the variables to each row. Because in each row, these will be one queen. lets say Q1,Q2...QN (generalizing the problem to N) . (Q1,Q2) will belong to {(2,4),(1,3)...} (difference between the indicies should be greater than or equal to 2 so that they dont lie in the diagonal). These are just the constraint for a pair. Similarly we have to form constraints for all pairs and ensure that all of them are being followed. 
A binary constraint problem is a problem in which every constraint has atmost 2 variables. For example a graph coloring problem. Also the binary graph constraint, in which the nodes are the variables and edges determine the constraint. So a the n-queens problem can also be designed according to the binary grah constraint. The nodes determining the positions of the queen and edges determining constraints like only a single queen in a row or something like that.
Another example is cryptarathmetic. In cryptorathmetic , we give input numbers in encrypted form and it should give result in the encrypted form itself. So we maintain the variables as the characters, and in case of addition another variable is needed which determines the carry for the addition.  Their domain can be numbers ranging from 0 to 9. constraints should be such that if two variable's sum exceed, then they should be split into two (ten's place and one's place) and the two should be assigned to the correct variables. Another constraint can be that the variables should be different and each should take different value. So when there are more than two constraints to take care of, we put different kind of auxillary nodes and we connect variables to these nodes. So it would denote the constraint containing more than 2 variables. This is how we can represent more than 2 constraints in a tree. These auxillary nodes do not correspond to any variable.
Another example can be a sudoku. In this we are given 9x9 grid and we have to fill the grid with numbers 1-9. So the variables can be the emppty grid and the constraints can be that no number should repeat in the row,column and 3x3 grid. So connect these variables of rows using a auxilary node to represent the constraint that only 1 number can occur only 1 time.

The Waltz Algorithm is used for interpreting the line diagram of the solid 3-D objects that we show to it. It is one of the early example of CSP. It basically studies the geometry. So lets say we are given a cube in which there is a cube shape hole on one of the corner. So the corner in the cube shape hole can be said as the inner corner. So if the input as inner corner is given, then using waltz algorithm we can find all other inner corners , and similarly we can find all outer corners. All the constraints are laid such that we are able to find the desired result. 
There can be various varieties of CSPs. For example discrete variable, which have further two categories, finite domain, and infinite domain. (Queen placing problem and length problem etc) . Another is continuous variable problems in which we can have variables like timestamp or some other linear constraints. In general we have methods with which we can solve continuous variable problems more quickly than the discrete variable problems.
Another classification can be on the basis of varierty of constraints. There are three types of such constraints. 
One is unary constraint in which a variable cannot be equal to a certain value, so all the other values except that value become its domain. Second is binary constraints which involve pair of variables. example in map colouring problem. Third is higher order constraint which involve more than 2 variables. For example in sudoku problem. We saw that in CSP the paths are not usually important but to reach the goal is our primary job. For example in, map coloring problem we can have multiple solutions or in queen placing problem too. In real world too, we mostly prefer constraint satisfaction part and not the optimization part. 
Some examples of CSPs can be scheduling problems, hardware configurations, Fault diagnosis etc. In scheduling problems, for example in class scheduling problems, we have to schedule the classes such that minimum numbwer of classes are used for conducting the classes etc.

Standard Search Formulations of the CSP. In this, we can come up with any search algorithm like bfs, dfs etc.Intially all the variables shall be unassigned. The states (or nodes) can be made as the values filled in the variables. The successor function will assign values to the varaible in accordance to the constraints. Our final goal would be to assign values to all the variables and satisfying all the constraints given to us. 
Let us study about the results of applying the bfs in CSP. We are discussing map coloring problem here. Initally none of the node is assigned any colour. Then one of the node will be coloured with one of the possible colour. So in total there can be (number of nodes)*(number of colours) children of the root node in search tree. This will give us a significant branching factor. First we will have to complete the whole tree then we will have to check if all the constraints are being matched or not. This will lead to a very high computational time, since we are forming the whole tree everytime.
For DFS, we will first color the entire tree, then check of the constraints. So basically we are able to get a solution within n steps(n is the number of nodes). So this is comparitively faster than bfs. So we will mostly use dfs in CSPs. 
Obviously we can make make changes to the DFS approach here inorder to reach the solution more quickly. In dfs, we saw that we assign some color to the node and then we keep on proceeding with that colour, even if the assignment of color is violating the constraint. So this is like the extra computation which can be avoided if we check the violation of constraint at early stage and stop that iteration there itself. This will be done using Backtracking Search. 
Backtracking search algorithm is uninformed search algorithm since we are using dfs as base algorithm and we are not using any other extra information about the goal. We just assign order to the variables. This means that we assign values to the variable in a certain order. Also we will check the constraints on every assignment. If violation of any constraint take place then we stop at that point, go back and check other nodes. So using these are two improvements in our dfs algorithm and we name this method as backtracking search.
By ordering we mean that we assign to some variable lets say x1, first. Then x2,x3 and so on. So at level one, we will assign lets some value , lets say 3 to it, and then go to variable x2 and assign some other value lets say 4 to it. Now we check if any of the constraint is being violated or not, if it is then we stop there , go back and change the value assigned to x2. if for all values possible for x2 the constraints are violated then we assign some other value to x1 and proceed similarly.
First of all we are given the variables , so we first arrange them in order. Then we also arrange the values in the domain too. Then we assign the values and proceed as per the method explained above. Ordering the variables and values improves the performance of backtracking.
Basically if we order the values which are to be assigned, we can overcome some of the ways which could lead to the violation of restrictions. So arranging the values to be assigned is a kind of filtering. Also filtering means to detect inevitable failure early. We also have to think about the structure of the problem, which will help us to validate the constraints of the given problem.
Forward Filtering is basically keeping the track of the domain of unasssigned variables and reject the options that could lead to a possible failure or violation of constraint. So a forward checking will be to cross the values from the domain of unassingned varaible which will lead to violation. For example in map coloring problem, we have 2 nodes n1,n2, and 3 possible color R,B,G. If we assign n1 to R then we will cross off R from the domain of n2. This will help in forward checking. So in forward checking we basically check for the empty domain for the variables that are yet to be assigned values.
We define consistency of an arc as follows. An arc from A->B is said to be consistent if and only if for every A in tail there is some B on the head which could be assigned without conflicting with any constraint. To maintain the consistency of the arc, we will remove values only from the domain of the tail of the arc, not from domain of head of arc. Once we remove the value from any of the domain, we will have to check for rest of the arcs too. So in case of n nodes, we will have to check for n*n arcs again if we delete some value. This means we have to do a lot of computation on each assignment. So it can be said to be a disadvantage of this method. The runtime of algorithm is O(n^2*d^3) where n is the number of head nodes and d is size of domain. This is because of we have to check for each arc again after editing the value of the domain of a node. Also we then will be checking for each value in domain of tail node to domain of head node that makes it d^2. But since in case of changing the value, we have
to again iterate over the domain and again check for the domain of the tail and head node. Thus it becomes d^3.
Limitations of arc consistency can be said as that we are checking for only a pair at a time, so this may lead to having no solution left(without knowing it), or one or two solutions left. Also arc consistency runs inside a backtracking search, it is not run differently.
We earlier listed that ordering helps in finding the solution and helps in filtering. So usually in ordering, we start with the variables which have the smallest domains. This method is called Minimum Remaining Values(MRV). We always choose the minimum sized domain variable so that in future we donot have any constraint violation. This is kind of greedy approach and the variable is called most constrained variable. When we have multiple choice to assign some value to the variable, then we assign only that value which will lead to very less shrinking of domain of other varaibles. This value ordering is called least constraining value. Again we choose the part least, so that the chances of violating the constraints also reduce. 

In all of the above search algorithms, the one thing that was common was that we were given start state and the goal state, and we somehow had to make it to the goal state. So we saw that the agent basically simulates the paths first, and when it finds one as per the conditions, then it moves on the path. So it first plans ,and then moves. But in case of local search algorithms, the agent will not work in this way. In local search algorithm(Hill Climb Algorithm), the agent first looks at the neighbouring states and then makes a move. 
Some examples of algorithms for the local search. Random search, Random walk in which we randomly pick a neighbour of the node and go to that node. In case of dead end, we go back and choose another and return the path if we reach the destination. 
lets check another example, Hill climbing(greedy local search) max version. We will be given input variables, problem and other necessary details. So for a given node, we will check the neighbouring states and check their values. If the value is greater than the current node, we move to that node. Else we return that node as the local maximum. We donot look ahead of the immediate neighbors of the nodes. Analogically we can say that it is similar to walking through the maze with amnesia.
So suppose we are solving the problem of n queens using the local search algorithm. Lets say we are given one queen in each column. So successor function will move the queen from one square to another in the same column. The heuristic function can be the said as the nmber of queens that are attacking each other, so we would like to minimize it. (in this case it should be zero) 

Incase of local search algorithms , we just take care of the current state and no other state. We move only to the neighboring states, so very little amount of memory is used. We don't store the paths. Thus this will help in finding solutions in infinite and continuous state spaces. 

When we say complete solution, we mean a solution that meets our requirements. We saw that whenever we start from some value, we it will have a value corresponding to that state (the objective function). We try to optimize this with respect to our requirements but not always acheive a complete solution. We might reach a solution, but it may not be complete. For example in queen placing problem , (lets say we have to place 8 queens on 8x8 chess board such that they wont be able to kill each other) we observed that in 14% of the times we were able to find the solution and 86% of time we get stuck on a local solution. But we reach solution in like 4 steps and get stuck in 3. So when we have very high number of states, then we will be able to find the solution quickly.
The drawbacks are as follows. : 
1. If there are too many local maximas then we will not get complete solutions most of the time. 
2.This class is said to be as plateaus. If there is only a single maxima and rest of the values are constant, then at most cases we will not get the solution, as in the immediate neighbourhood there is no increase/decrease in values. So it will tend to stay on that state or may change the state but the final solution will not be complete. The program will stop before it comes to complete solution in most cases. 
3. If we are moving in 2 dimensional space, and we have the neighbourhood function such that it moves in either x direction or in y direction, but in certain cases we might get a solution by moving along the diagonal which the neighbourhood function doesnot allow. So we might not get a complete solution there too. This class is called the diagonal ridges.      
We can allow sideways moves which will be explained later, which will help us prevent the plateau problem and reach the complete solution. Also it will improve in solving the queen placing problem by increasing the 14% solve rate to 94% solve rate.This is called tabu search. The drawback of this is that the number of steps increase significantly, because in sideways walk we are allowing the algorithm to store some states contrary to the hill climb algorithm in which we didnt store anything and didnt check anything except for current and neighbouring states. So this will increase the number of steps involved.
What we can do is maintain a queue, and we can keep adding the states in it. We also have to fix the length of the queue so that we dont add too many states. Also we have to ensure that we are not getting stuck in a loop or not checking a bunch of states again and again. That would be waste of computation. 
We may add up a few stochastic variations.
If we take a look at standard hill climbing algorithm, there are quite high chances that we may get stuck in local minima. To prevent this, we may add some random selection or random restart in our algorithm. While choosing the next state we can maintain a probablity p with which we will choose the next appropriate state and with probability 1-p we will choose some other state randomly. With this we may not be going in the best direction everytime, but we will do some improvements in terms of finding the solution. With random restart we mean that we randomly choose the starting point and run the algorithm and find the solution. If we do it several number of times, there is high chance that we end up getting a complete solution. 
So in general if we have to perform local search, this algorithm gives the best result. We may add some conditions for terminating the program, for example we add a condition like search for 100 states, or perform 100 iterations etc. Also if we are not always itnerested in getting the best solution, or wih some relaxed conditions we can add that too. For example in placing queen problem, we can add a condition that we are okay with atmost 2 queens in attacking position. This way we can terminate the program earlier than it should. 
We can also combine both greedy and random walk approach. What we can do is initially choose a start state which has the best neighbouring states and start from there, chooose the neighbor with probability p or choose random state with probability 1-p. This way we are using greedy and randomness together. Since both individually are asymptotically complete , so together they will also be asymptotically complete.  

In certain cases where we have a plateau /shoulder along with the increasing and decreasing curve of the objective function, we can do a local search/hill climb search when the values are either increasing/decreasing, and when we reach a plateau we do an exhaustive search which will lead to the end of the plateau and we may begin to do the local search/hill climb search again. This way we are saving the extra computation of exhaustive search for the whole plot. The disadvantage is the time lost in exahustive search during the plateau period. 

Next comes the simulated Annealing algorithm which is inspired from the idea of random walk. So we will be using the modified hill climb algorithm with randomness included in it. We will be choosing the next state with probablity p and some provisions will also be made to also choose the less relevant states too. We will do this by providing shake , which can be assumed as a ball being stuck in a local minima and we provide a shake to it so it starts moving and goes to another minima. Gradually the amount of shake provided will keep reducing.
The algorithm is explained as follows. : 
function SIMULATED -ANNEALING(problem ,schedule) return solution state
 input: a problem, problem
	schedule,a mapping from time to temperature.
 local variables, current, node
		next, a node
		T , a temperature which will be controlling the amount of shake
current<-MAKE-NODE(INITIAL-STATE[problem])
for t 1 to infinity
	T<-schedule[t]
	if T=0 then return current
	next<-a randomly selected successor of current
	E = VALUE[next]-VALUE[current]
	if E>0 then current<-next
	else current<-next only with probablity e^(-E/T).

The way we reduce the temperature is schedule and if we ensure that we reduce the temperature very slowly then we can say that we will reach global minima.
In general it takes quite a lot of time, and is not very popular. Some problems which can be solved using this algorithm are Traveling salesman problem ,graph partitioning,graph coloring etc.

Another variant of hill climbing is local beam search algorithm 
In this we are allowed to use memory, unlike in hill climb algorithm. So we can keep track of k states simultaneously unlike in hill climb algorithm, where we used to keep track of only one state. So we choose the k next based successors and again choose the k best successors from the successors of the successsors.
The disadvantage is that we are affecting the diversity as we are choosing the best successors from all the neighbours. So we might reach to the same minima.
Again we can improvise by adding randomness here. Instead of choosing k best successors, choose the successors probabilistically by giving bias to some possibilities.

Genetic Algorithms.
This is something different from the local search algorithms. In this the successor is generated by two parent nodes and a fitness fucntion calculates the fitness of the node. It basically gives an idea of how probable the state is to survive.More the fitness better the chance of survival.
The case of the n queens problem, lets say we represent the posititions of the queens on the chess borad by a string. We assume that there is a queen in every row, so for the position of queen at the ith row, the ith position of the string will give the column number. So in genetic algorithm, we will use some randomly generated k states just like in beam search algorithm, except the fact that the next node will be decided using two states. For each state in the given population, we get the fitness of the state by the fitness function which tells how close we are to the goal.So we will use number of non attacking pair as the fitness function. Calculate the probabilities using approapriate functions. Using these probability we can find the k strongest states. Now we perform a cross over. We make a partition in a state, and take the first i bits of that state and n-i bits of the other state and form a new state. This way we form a new state. We add this partition randomly. Now the last step, we randomly change the digits of the state and we change it. This is called mutation. This sums up the genetic algorithm.  Using the crossover here is really helpful as we are able to move from one state to another in a single step,which would have taken a huge number of steps. We are joining two random states which ought to give higher fitness in the end.This may also taken out of local minimas. So its use is justified.
Genetic Algorithm is derived from Stochastic beam search algorithm.
Some advantages are as follows: it helps to jump from one step to another and also avoid local minima/maxima.
Some disadvantages are that there is alot of randomness so it is difficult to get the same result again.
Lets say we have to solve traveling salesman problem using genetic algorithm. How do we do this? Arrange the cities to be visited in order and randomly select k cities. (say first we visite 1 then 3 then 4 then 2. So 1342. and similarly we make all the states, use fitness function and find k best successors. Do crossovers and generate new states. Now we do the mutation by selecting 2 values of the string and swaping it. The values selected are such that would lead to an increase in fitness after mutation. We can also do an exhaustive swap. Iterate over all pairs, and choose the one with lowest cost.
How do we do the cross over in the traveling salesman problem? It is a kind of greedy approach. We select the first city from the parent, and compare the leaving city for that city in both the parents and choose the closer one to extend the tour. If one city has already appreared in the tour select the other one. If both have appeared in the tour then we randomly select the non appeared city.

One negative point of the genetic algorithm is that the algorithm doesnot necessarily give the population which is fitter tahn the previous one but also has characterstics of the parents which are easily mixable with others. Lets say in parent population, two states are repeating. So there are high chances that more characterstics are inherited from these classes. 

We observed that hill climb algorithm is used in discrete space. What if we want to use such an approach for a continuous space. Lets say we have a function f(x,y) = e^-(x^2+y^2) + 2* e^-((x-1.7)^2+(y-1.7)^2)). One way to do it is to discretize the space and do the normal hill climb algorithm there. Also we can adopt the idea of gradient descent or gradient ascent. So in continuous space if we have to maximise the objective function, then we apply gradient ascent, else we do gradient descent. In general we can have n variables in place of the two variables used in the previous example. We first calculate the gradient of the function with respect to each variable. The gradient gives us the direction in which we may acheive minima or maxima. So we move in that direction. In continuouse space we also have to use the step size, which will determine how big our jump/step would be unlike in discrete space where we moved from one state to another. So the stepsize (lambda) can be determined by the user. What is done in practice is that we keep the stepsize small and we keep increasing it. Another way is to initially choose a large value of step size then iteratively decrease it. This second appraoch is inspired from simulated annealing algorithm we mentioned earlier.
Artificial Intelligence - Lecture Notes (Lecture 9 to 16)
Manul Goyal (B18CSE031)
Instructor: Dr. Yashaswi Verma

Tradeoff between Quality and Computation of a Heuristic
The more closely a heuristic function estimates the actual forward cost, the better it becomes. A good quality heuristic function, which approximates the actual cost closely, results in a reduced search space while searching for the solution. In other words, lesser nodes need to be expanded to reach a solution when a better heuristic is used, because it guides the search towards a solution in a more effective way, wasting lesser time in exploring nodes which do not lead to a solution. But, better heuristics usually require more computation per node. Thus, there is a tradeoff between the quality of the heuristic and the amount of computation it requires. This tradeoff is clearly evident in the two extreme cases, one in which we define the heuristic to be zero for every node, and the second in which the heuristic is equal to the actual forward cost for each node (called the exact heuristic). Both these heuristics are admissible since they are a lower bound for the actual forward cost, and therefore both of them lead to optimal solutions when used with the A* search. In the first case, no computation is required for the heuristic, since it is constant, but it is a useless heuristic since an A* search using it is equivalent to a uniform cost search, which is an uninformed search. In the second case, the heuristic is equal to the actual forward cost, which means that at the start state, the value of the heuristic is equal to the actual cost of the optimal solution. This heuristic would guide the search straight to the optimal solution, without expanding any extra nodes (which do not lead to an optimal solution) in between. Thus, it greatly reduces the search space, but requires a significant amount of computation per node. So, an appropriate heuristic, somewhere betweeen these two extremes, should be chosen keeping this tradeoff in mind.

Dominance of Heuristics
A heuristic h_1 is said to be dominant over another heuristic h_2, denoted by h_1 >= h_2, if for each node n, h_1(n) >= h_2(n). In this case, if h_1 is an admissible heuristic, h_2 is also an admissible heuristic, since h_2 is a lower bound of h_1 and h_1 is a lower bound of the true cost, which implies that h_2 is a lower bound of the true cost as well. 

Semi-lattice of Heuristics
The set of all admissible heuristics forms a semi-lattice (a partially ordered set). The ordering between heuristics is defined in the above section (dominance of heuristics). Not all pairs of heuristics can be compared, only some specific ones, which satisfy the definition of dominance stated above. Hence, it is a partially ordered set. The lowest and greatest elements of this set are the zero heuristic (which is always zero) and the exact heuristic (which equals the true cost) respectively. Every admissible heuristic is dominant over the zero heuristic and is dominated by the exact heuristic. These two heuristics are also called the trivial heuristics, and every admissible heuristic lies between them. Also, each set of admissible heuristics, {h_1, h_2, ..., h_k}, has a least upper bound, H = max(h_1, h_2, ..., h_k). For any given node n, H(n) = max(h_1(n), h_2(n), ..., h_k(n)). H is also admissible, since h_i(n) cannot exceed the true cost for any node n and any (admissible) heuristic h_i. Thus, the maximum of admissible heuristics is also admissible. 

Graph Search
It is a type of informed search, which searches the state space graph, ensuring that no state is expanded more than once. In all the search algorithms we have seen so far, the search tree was being searched, and states which had already been expanded may get expanded again, leading to redundant computations. In a search tree, multiple nodes may correspond to the same state, since there can be multiple paths to reach the same state (and there is a separate node corresponding to each such path in the search tree). Therefore, expanding more than one node corresponding to the same state leads to redundancy in tree search algorithms. In fact, expanding repeated states may lead to exponentially more work, especially in the case of breadth-first search. This can be illustrated using a path graph, in which there are two directed edges between each pair of adjacent nodes, both from the previous node to the next node. In such a graph, there are 2^(n-1) paths possible to reach the n-th node, and bfs would expand 2^(n-1)-1 nodes before finding the n-th node (while dfs would find it after expanding just n-1 nodes). In order to remove this redundant work, graph search algorithm maintains a closed set of expanded states, S, which stores all the states that have already been expanded. The algorithm proceeds like normal tree search, but every time a node is taken out of the fringe, it is first checked whether this node corresponds to a state which is already present in set S. If it is already present in S, then this node is ignored, and the next node is popped from the fringe. Otherwise, the state corresponding to this node is added to S, and then the node is expanded. This ensures that no state is expanded twice. This closed set can be implemented as a set rather than as a list, since sets can be searched more efficiently than lists.

Completeness and Optimality of Graph Search
Given adequate time, graph search would eventually expand every state (once). So, if there is a solution, the states leading to that solution will be expanded at some step, and therefore, the solution will definitely be found. Thus, graph search is complete. Uniform cost search (UCS) carried out using graph search will always give an optimal solution, because UCS maintains the invariant that every node which is not yet expanded will have a greater (or equal) cumulative cost than the node which is chosen from the fringe for expansion at any given step. So, the first time a state is expanded in UCS, it is guaranteed that the corresponding node has the least cumulative cost among all other nodes representing the same state, so the state need not be expanded again. On the other hand, A* search carried out using graph search may not give an optimal solution even when an admissible heuristic is used. It may happen that some state, say X, can be reached via two different paths, and the algorithm expanded the node corresponding to the more costly path that leads to X, because some ancestor of X on the less costly path had a heuristic value which overestimated the arc cost. This overestimation by the heuristic on the less costly path led the algorithm to expand X along the more costly path, and the state X won't be expanded again along the optimal path since each state is expanded at most once in graph search. In this case, the search would result in a suboptimal solution. To overcome this problem, we need to impose tighter constraints on the heuristic function, which is explored next.

Consistency of Heuristics
A heuristic h is said to be consistent if for every arc in the state space graph, from a state X to a state Y, h(X) - h(Y) <= arc cost from X to Y. The cost of the arc from X to Y just represents the actual decrease in the forward cost (cost to the nearest goal) when one transitions from X to Y. h(X) - h(Y) represents the heuristic 'arc' cost, i.e., the estimated decrease in the forward cost during this transition. A consistent heuristic, therefore, is such that the heuristic arc cost always underestimates the actual arc cost for every arc. A consistent heuristic is always admissible, but the converse does not hold. Assume that a sequence of states on the path from a state X_0 to the nearest goal state X_n be X_0, X_1, X_2, ..., X_n. If h is a consistent heuristic, then h(X_i) - h(X_(i+1)) <= cost of arc from X_i to X_(i+1), for all i. Summing on both sides, we get h(X_0) <= actual path cost from X_0 to X_n (since h(X_n) = 0 as it is a goal state). This implies that h is admissible. Recall that function f is defined for each node n as f(n) = g(n) + h(n), where g(n) is the backward cost or cumulative cost of n and h(n) is the value of the heuristic for n, which is an estimate of the forward cost to the nearest goal state from n. The consistency of h implies that the value of function f never decreases along a path leading to a goal state. Since h(X) - h(Y) <= arc_cost(X to Y), h(X) <= arc_cost(X to Y) + h(Y). Adding g(X) to both sides gives f(X) <= f(Y) (because g(X) + arc_cost(X to Y) = g(Y)). The result of this property of f is that the invariant which holds in UCS for cumulative cost (mentioned in previous section), also holds for f in A* search. Thus, A* graph search always gives an optimal solution when used with a consistent heuristic. Recall that A* tree search only requires an admissible heuristic for optimality, but, A* graph search requires a tighter constraint on the heuristic, namely, consistency. Also, UCS is optimal when carried out using graph search, because it is a special case of A* search with heuristic function as 0, which is a consistent heuristic. Generally, admissible heuristics which naturally arise from relaxed versions of problems tend to be consistent.

Proof of Optimality of A* Graph Search using a Consistent Heuristic
A consistent heuristic guarantees that the value of function f never decreases along a path leading towards a goal. Consequently, whenever a node with the smallest value of f is taken out of the fringe for expansion, all nodes with smaller values of f have already been expanded, and all the nodes remaining for expansion have greater or equal values of f than the one which is being taken out. Therefore, the first time a node corresponding to some state is popped from the fringe, it has the least possible f value among all nodes corresponding to the same state, so it is not needed to expand the same state via a different node (i.e., along a different path) in the future. Finally, when a goal node is popped from the fringe, it has the minimum f value (which, for a goal node, is equal to the cumulative backward cost) among all the goal nodes, thus giving us an optimal solution.

Planning and Identification Tasks
Planning tasks: These are the type of problems we have seen so far. Here, the objective is to come up with a plan, which is a sequence of actions, to reach a goal state starting at some initial state. This translates to finding paths in a state space graph, and each path is associated with some cost, which may be required to be minimized. Heuristic functions are used for informed searches, which guide the search towards a goal, instead of blindly searching the state space as in uninformed search. 
Identification tasks: These are problems in which the goal is to find an assignment of values to a given set of variables, from a given domain of values. The assignment may need to satisfy some given constraints (imposed by the problem). The path itself is not important in such problems, the resulting assignment produced is important. In fact, in many formulations of such problems, all paths have the same depth. Constraint satisfaction problems come under this category.

Constraint Satisfaction Problems (CSPs)
Given a set of variables and a domain of values (or different domains for each variable), the problem is to find an assignment of values to variables, which satisfy a set of given constraints. CSPs are a special subset of search problems, where the states represent assignment of values (from the domain(s)) to variables, and the goal test represents a set of constraints on the assignment, where each constraint specifies the allowed combinations of values for a set of variables. Examples of CSPs are the map coloring problem and the N-queens problem. Let's take a simple CSP example, which is searching for a Pythagorean triplet. Here, the set of variables is X = {a, b, c} and the domain D is the set of all natural numbers, i.e., D = {1, 2, 3, ...}. There is only one constraint in this problem, which can be represented implicitly or explicitly. The constraint can be represented implicitly as a^2 + b^2 = c^2, and explicitly as the set of assignments satisfying the constraint, i.e., (a, b, c) = {(3, 4, 5), (8, 15, 17), ...}. A solution is an assignment of values from D to all variables in X which satisfies the given constraint, for example, {a=7, b=24, c=25} is a solution to this CSP. Next, we present two other examples, the N-queens problem and the map coloring problem.

N-Queens Problem
We are given an N by N chessboard and we have to put N queens on the chessboard such that no two queens are in an attacking position. We will show two formulations of this problem as a CSP.
Formulation 1: Here, we choose the set of variables to be X = {X[i,j]}, where each X[i,j] corresponds to the square at the i-th row and the j-th column on the chessboard. The domain D = {0, 1}, where a value of 1 represents that a queen is present in the corresponding square, and 0 represents that the square is empty. The implicit constraints represent that no two queens are present in the same row, same column or same diagonal, and the total number of queens placed is N (i.e., sum(X[i,j]) = N over all i, j).
Formulation 2: Here, we choose the set of variables to be X = {X_1, X_2, ..., X_N}, where each X_i represents the position of the queen in the i-th row. The domain D = {1, 2, ..., N}. Here, X_i = j means that the queen in the i-th row is present in the j-th column. The implicit constraints are:
1. For each pair i, j, where i != j, X_i != X_j						
2. For each pair i, j, where i != j, abs(X_i - X_j) != abs(i - j) 
The first and second constraints enforce that no two queens are in the same column, and no two queens are in the same diagonal, respectively. The constraints that no two queens are in the same row and that the total number of queens is N, are automatically enforced due to the way we have chosen X and D.

Map Coloring Problem
We are given a map of cities, in which each city may be adjacent to zero or more cities. We have to color the cities in such a way that no two adjacent cities have the same color. Here, the variable X_i represents the color assigned to city i, and the domain is the set of all allowed colors C_i, i.e., X = {X_1, X_2, ..., X_n} and D = {C_1, C_2, ..., C_k}. The implicit constraint is that no pair i, j exists such that i != j, cities i and j are adjacent, and X_i = X_j.

Constraint Graphs
A constraint graph represents variables and the constraints between them. Many general-purpose CSP algorithms use the constraint graphs to speed up the process of finding a solution. In a binary CSP, each constraint relates at most two variables. For example, the second formulation of the N-queens problem above is a binary CSP. Constraints in such problems can be represented using binary constraint graphs, in which each node represents a variable and each arc connecting two nodes represents a constraint between the two corresponding variables. In CSPs, where constraints which relate more than two variables are present, we need to use special (auxiliary) nodes in the constraint graph. Each of these special nodes represents some constraint, and it is connected to multiple variables ('normal' nodes) which are related by this particular constraint.

Varieties of CSPs
CSPs can be classified based on the nature of the domain of values that the variables can take.
Discrete Variables: In such CSPs, the domain consists of discrete values. It may be finite or countably infinite. An example of a finite domain CSP is the Boolean satisfiability problem, in which the task is to find an assignment of truth values to a set of variables which satisfy a given Boolean formula. Here, the domain only consists of two values, {TRUE, FALSE}. In CSPs with a finite domain, the total possible number of assignments to n variables is d^n, where d is the size of the domain. An example of a countably infinite domain CSP is job scheduling, where variables are start and end times of jobs and the domain is the set of all non-negative integers.
Continuous Variables: In such CSPs, the domain is a set of continuous values. The domain is uncountably infinite in this case. An example of a CSP with a continuous domain is diet planning, in which the variables are quantities (weights) of different kinds of food items, and appropriate quantities of each food item need to be taken subject to constraints on the total intake of calories, sugar, fat, etc. The domain is the set of all non-negative real numbers in this case.
Solving continuous variable CSPs is generally easier than solving discrete variable CSPs.

Varieties of Constraints
Unary Constraints: constraints which are applicable to a single variable only. For example, in the Pythagorean triplet CSP, a unary constraint on variable a can be a > 0. Such constraints are equivalent to reductions in the domain.
Binary Constraints: constraints which relate two variables. For example, in the N-queens problem, the constraints in the second formulation (given above) are binary constraints. They can be represented by binary constraint graphs.
Higher-order Constraints: constraints which relate more than two variables. For example, in the first formulation of N-queens problem (given above), the constraint that sum of all variables is equal to N is a higher-order constraint. Such constraints require the use of special nodes in the constraint graph.

Preferences (Soft Constraints)
These types of constraints involve associating costs with values in the domain or with different assignments of values to variables. This results in preferring some assignments over other assignments, and the goal is then to find an assignment which not only satisfies the constraints, but also has the minimum cost. Such type of problems are known as constrained optimization problems.

Standard Search Formulation of CSPs
The states represent partial assignments of values to variables. The initial state represents that no variable is assigned a value yet. The successor function assigns a value to an unassigned variable. The goal test is that the assignment represented by the current state is complete (assigns one value to each variable) and satisfies all the given constraints.

Uninformed Search Algorithms for CSPs

Naive BFS
Since each state has a huge number of potential successors (for instance, there are n*d possible successors of the initial state, where n = number of variables, d = size of domain), BFS would have to store all these states in the fringe. It is not even possible in case of countably infinite domains. Moreover, we know that solutions are present only in the last level (at depth n), so BFS would have to traverse the whole search tree before reaching a solution. So, naive BFS is very inefficient for solving CSPs.

Naive DFS
This algorithm would keep assigning values to unassigned variables one-by-one, until it reaches a leaf node in the search tree, which corresponds to a complete assignment. If it satisfies the constraints, this assignment is returned as the solution, else, the algorithm backtracks and tries other assignments. Since we get to the first solution in just n steps, this algorithm is better than naive BFS. 

Backtracking Search
Two important modifications to DFS lead to backtracking search. Firstly, the order in which variables are assigned values is irrelevant to the solution. So, at each level in the search tree, one variable should be fixed and only that variable should be assigned values at that level. Once an ordering of variables is fixed, the algorithm only needs to assign values to one variable at each step. Secondly, constraints should be checked at each step, instead of checking them only at the last level. In the map coloring problem, a possible assignment is {X_1 = C_1, X_2 = C_1, ..., X_n = C_1}. Assuming that city 1 and 2 are adjacent, it is known at the second step itself (when X_2 is assigned C_1) that the resulting assignment would not satisfy the constraints. So, there is no need to traverse all the way down and then backtracking. Instead, the algorithm should backtrack as soon as a partial assignment is obtained which doesn't satisfy the constraints. Thus, we are doing an 'incremental goal test' at each step, which requires some additional computation, but drastically reduces the search space. 
So, backtracking search works by first choosing some ordering of variables, and then at each step, it assigns a value to the variable corresponding to that level, such that this value doesn't violate the constraints when combined with the existing partial assignment.

Techniques for Improving Backtracking Search

Filtering: Forward Checking
In this technique, we backtrack as soon as we obtain a partial assignment which would inevitably lead to a constraint violating assignment in the future. The partial assignment may not directly violate the constraints, but it will definitely lead to a violating assignment. To check this, we keep track of the domains of unassigned variables, and at each step when some variable is assigned a value, the values in domains of unassigned variables which will definitely lead to violating assignments, are crossed off. If at any step, the domain of some unassigned variable becomes empty, then the resulting assignment will definitely lead to a violating assignment in the future, so we should backtrack at this step itself.

Filtering: Constraint Propagation
We can improve the previous technique by doing additional checks when we propagate information from assigned variables to unassigned variables. For example, in the map coloring problem, suppose we have three total colors in the domain and we have already assigned two different colors to city 1 and 2. Now, if both the cities 3 and 4 are adjacent to city 1 and 2, we only have one color left in each of the domains of city 3 and 4. Now, if city 3 and 4 are also adjacent to each other, then the current partial assignment would definitely lead to one of the domains becoming empty in future (for instance, if city 3 is assigned the last color in the next step, city 4's domain would become empty). So, using this check, we can backtrack at the step in which city 1 and 2 are assigned different colors itself, even before some domain becomes empty, which further reduces the search space (although it increases computation at each step).

Consistency of a Single Arc
An arc (directed edge) from a variable X (tail) to a variable Y (head) is said to be consistent iff for every value in the domain of the tail (X), there exists some value in the domain of head (Y), which can be assigned to Y without violating the constraints. In terms of consistency of arcs, forward checking involves checking the consistency of each arc from an unassigned variable to the variable which is assigned a value in the current step. If such an arc is found to be inconsistent, all the values in the domain of the tail which lead to inconsistency are removed from the domain.

Arc Consistency of an Entire CSP
Checking consistency of only those arcs which point to the newly assigned variable as mentioned above may not lead to constraint propagation. For example, in the map coloring example mentioned in the constraint propagation section above, when a color is assigned to city 2, the inconsistency of the arc from city 3 to 4 won't be discovered since only arcs pointing to city 2 are checked. In order to enforce constraint propagation, all the arcs must be checked (in both directions) at each step (there are a total of n*(n-1) arcs). In this example, when city 2 is assigned a color, we note that the arc from 3 to 4 becomes inconsistent, since no color can be assigned to city 4 once the last remaining color is assigned to city 3. So, we can backtrack at this step itself, thus enforcing constraint propagation. 
Note that once some value is removed from the domain of the tail X to enforce consistency, all the arcs from the neighbours of X to X need to be rechecked in order to enforce constraint propagation. This technique would lead to earlier detection of failures than forward checking, but at a greater computation cost at each step.

Limitations of Arc Consistency
Since checking consistency of arcs involves checking only two variables at a time, constraints involving more than two variables may not be captured. For example, if there are three cities X_1, X_2 and X_3, and each of them is adjacent to the other two, and if each of their domains contains only two colors C_1 and C_2, all the arcs are still consistent, but no assignment which does not violate the constraints exists in such a case. Therefore, in such situations, checking for arc consistency may not lead to detection of inevitable failures in the future. Another limitation is that it considerably increases the running time of backtracking search, which may be infeasible for problems involving large number of variables and large domains.

Ordering of Variables in Backtracking Search
The order in which variables are assigned values in backtracking search is an important factor which determines the efficiency of the algorithm. We discuss some ordering techniques next.

Ordering by Minimum Remaining Values
In this ordering, the next variable to be assigned a value is the one with the smallest domain (least remaining number of values in the domain). Since variables with the smallest domain have the greatest chance of ending up with an empty domain, we greedily assign a value to such a variable before other unassigned variables with larger domains, so that the chances of our partial assignment failing in the future are reduced. This is a benefit of this technique. It is also called 'most constrained variable' or 'fail-fast' ordering.

Ordering by Least Constraining Values
In this ordering, the value to be assigned to the next chosen variable is the one which shrinks the domains of other unassigned variables by the least amount. The advantage of this ordering is that the chances of the partial assignment failing in the future are reduced, since the least constraining value is chosen at each step. Again, it comes at the cost of increased computation at each step.

Local Search Techniques and Optimization
All the problems and algorithms so far invloved simulation by the agent, not actual movement of the agent. Once a solution is found by simulation, the agent can then carry out the plan (sequence of actions) to achieve its goal. Now, we turn to problems in which the agent has to decide an action which it actually executes after making the decision. The decision of which action to take is based on the local neighbourhood of the current state. The objective of the problems will be to optimize some objective function which associates a value with each state, i.e., to find a state with an optimal value of the function. The path taken to reach such a state is irrelevant. The advantages of local search is that it uses very little memory as compared to the previous planning problems, since it only needs to keep track of the current state, and not the whole path to reach this state. This makes it feasible to find reasonable solutions to problems which have a large or infinite (even continuous) state space. Since we are considering pure optimization problems, the cost of paths and goal test formulation don't make sense here. The sole purpose is to minimise/maximise the objective function.

Trivial Algorithms

Random Sampling: In this algorithm, states are randomly generated and the value of the objective function is checked for optimality. If this process is repeated infinitely many number of times, it is guaranteed to result in an optimal solution.

Random Walk: In this algorithm, a neighbouring state of the current state is chosen randomly, and it becomes the new state. This process is repeated until an optimal state is found. 

Both the above algorithms are asymptomatically complete, which means that if they are executed for an infinite time, they are guaranteed to result in an optimal solution.  

Hill-climbing Search (Greedy Local Search)
This algorithm works by always choosing the neighbouring state with the highest value of the objective function (assuming that the goal is to maximize the objective function). If all the neighbouring states have a lower value of the objective function than the current state, the current state is a local maxima and it is returned (the algorithm terminates). Therefore, the result of this algorithm is a local maxima, i.e., it may not (and usually does not) return the global maximum. Apart from this drawback, the other problems are that it only considers the immediate neighbours (successors) of the current state, and doesn't look ahead of them, and if multiple neighbouring states have the highest values, it randomly chooses one from them. This may lead to wrong decisions (in the long term) and thus, result in smaller values of local maxima. The final solution obtained by this algorithm depends on the start state.

Example: 8-Queens Problem
We can formulate the 8-queens problem (which was originally a CSP) as a constrained optimization problem, which can then be solved by local search algorithms. Each configuration of the queens on the chessboard corresponds to a state, and the start state is that all the queens are initially in the first column. The successor function moves one queen to any square in the same row. Here, we define a heuristic function which will serve as the objective function, which is required to be minimized. The heuristic h(n) for a node (state) n is defined as the number of pairs of queens which are in an attacking position in the state n. Minimizing h to zero would then be equivalent to solving the same problem posed as a CSP. The difference is that when posed as an optimization problem, each possible configuration of queens on the chessboard is a solution. But our goal is to find a complete or optimal solution, which is one that minimises the value of h to zero.
Using the hill climbing algorithm to solve this problem will result in a suboptimal solution most of the times, i.e., one in which some pairs of queens are still in an attacking position. This is because this algorithm usually results in a local optima (minima in this case).

Drawbacks of Hill-climbing
1. Local Optima: As stated previously, this algorithm usually gets stuck in a local maxima/minima, but we generally want to find a global maximum/minimum.
2. Plateaus/Shoulders: Plateaus are states where all the neighbouring states (successors) have the same value of the objective function. Since the algorithm randomly chooses the next state in such cases (and doesn't look ahead of its immediate neighbours), it may remain stuck on a plateau for a long time. For this reason, the algorithm is usually terminated when it encounters a plateau.
3. Diagonal Ridges: Suppose we have to solve a 2D optimization problem, and the successor function only allows moves (steps) along the x and the y axes. It may happen that the function is increasing in a diagonal direction, for example, along the y = x line, but it is decreasing in x and y directions. In order to move along the diagonal direction, the algorithm has to take a step in the x and then in the y direction (or vice-versa). But due to its greedy nature, it won't do so, and remain stuck in its current suboptimal position.

Escaping Shoulders
A shoulder is a plateau which is not a flat local maxima, i.e., there are possible uphill moves on some points of the boundary of the plateau. If the algorithm encounters a plateau state, i.e., if there are no uphill moves present, we can allow it to perform sideways moves, in a hope that it will eventually leave the shoulder. In order to avoid getting stuck in an infinite loop (especially when the plateau is a flat local maximum, in which case it will never escape), we impose an upper limit on the number of sideways moves allowed. The algorithm terminates if it could not escape the plateau even after doing the maximum allowed number of moves.
Compared to normal hill-climbing, allowing sideways moves increases the probability of finding an optimum solution (h = 0) in the 8-queens problem from 14% to 94%, but using a greater number of steps to reach the solution.

Tabu Search
The drawback in the above method for escaping shoulders is that the algorithm may end up looping over the same few states repeatedly, since it resolves ties randomly, and no previous states are remembered. A possible solution is to maintain a queue of some fixed length, also called a tabu list, storing the list of states visited in the past. Whenever a new state is encountered, it is pushed into the queue and the oldest state is popped from the queue. It is ensured that the new state is not already present in the queue, which prevents repetition of states upto a few number of steps. As the size of the queue is increased, the search would increasingly become more "non-redundant", which would increase the chances of escaping a shoulder within the allowed number of sideways moves.

Variations of Hill-climbing
We describe some variations of hill-climbing which are designed to solve the local maxima problem faced by hill-climbing. Greedily selecting the steepest uphill move at every step will always suffer from getting stuck in a local maxima, but is more efficient than random walk or random sampling, which are guaranteed to find a global maxima, but are very inefficient. Let's try to combine the two approaches.

Stochastic Hill-climbing
In this variation, instead of always choosing the best neighbour, a neighbouring uphill state is chosen randomly with some probability, which directly depends on the steepness of the uphill move. In practice, this algorithm converges slower than the greedy approach, but it is able to find better solutions in some scenarios.

Hill-climbing with Random Walk
In this approach, the best neighbouring state is chosen with some probability p < 1, otherwise some neighbour is chosen randomly from the remaining neighbouring states with a probability 1-p. The hope is that it would prevent the search getting stuck in a local maxima. This algorithm is asymptomatically complete, i.e., if it is repeated for an infinite number of steps, it guaranteed to find an optimum solution.

Hill-climbing with Random Restart
This is a widely used method which works on the principle that "if you don't succeed at first, try again". A starting state is randomly generated and if hill-climbing from this state results in a local maxima, a new starting state is randomly generated, and the process is repeated (either until the optimum solution is found, or for a fixed number of restarts). This algorithm is asymptomatically complete, as repeating this process infinite times is guaranteed to eventually result in a starting state that itself is the global maximum. This algorithm finds an optimum solution relatively quickly when the number of local maxima and plateaux is relatively less. But even in NP-hard problems, which typically have an exponential number of local maxima, a reasonably good solution can be found after a few number of restarts.
If the probability of finding a global maximum is p in a particular iteration of hill-climbing, the expected number of restarts required is 1/p, and the expected number of steps taken is a + (1/p - 1)*b, where a and b are the expected number of steps needed for reaching a global maximum and a local maximum in a single iteration respectively.

Hill-climbing with both Random Walk and Restart
In this approach, at each step, either the best neighbour is chosen, or some other neighbour is randomly chosen, or a starting state is randomly sampled and the search is restarted from this state. Since both the component approaches are asymptomatically complete, this algorithm is also asymptomatically complete. 

Hill-climbing with Exhaustive Search for Escaping Shoulders/Local Optima
This is a fusion of the hill-climbing approach with exhaustive search methods (dfs or bfs). When the current state is a local optima or a plateau, we start exhaustive search from this state (using dfs or bfs), until some state with greater value of objective function is achieved. This way, we can escape local optima and plateaus, by adopting middle ground between hill-climbing (local) and exhaustive (systematic) search. Of course, it comes at the cost of the high computation required for exhaustive search.

Simulated Annealing
Inspired by physics, this algorithm tries to prevent the search getting stuck in a local optima, by providing it a 'shake' whenever it gets stuck. Formally, at each step, one neighbouring state of the current state is randomly picked and the difference between the values of the objective function, say D, is computed. If the neighbouring state is better, i.e., D > 0, it is selected and the search transitions to this state. Otherwise, if D <= 0, this state is selected with a probability p = exp(D/T), where T is the intensity of the shake or 'temperature'. The probability of selection p exponentially decreases as the neighbouring state gets worse (D becomes more negative), i.e., locally bad moves have a lower probability of getting selected. Also, the temperature T is a decreasing function of time, i.e., it has greater values at the initial steps of the search and smaller values as the search progresses. This has the effect of allowing downhill moves with greater probability initially, but it gradually decreases with time. Thus, over time, is becomes less and less likely to make locally bad moves, but there is always a chance to get out of a local optima. In fact, given that the temperature is decreased very gradually, this algorithm is guaranteed to find a global optimum, so it is asymptomatically complete. However, it is very slow in practice. It is therefore useful in problems which strictly require a global optimum, such as designing VLSI layouts, where an optimum layout can drastically reduce the cost of mass production.    

Physical Interpretation of Simulated Annealing
Hill-climbing search to minimize the objective function can be viewed as a ball rolling downhill on a surface, which will usually get stuck in a valley before it reaches the deepest crevice, which is analogous to the search getting stuck in a local minima. To release the ball from the valley, we can provide a little 'shake' to the ball which is just hard enough so that it gets out of the local minima, but doesn't miss the global minimum. This 'shake' can also be viewed as a 'temperature' provided to a metal which causes vibrations in its particles. As the temperature is slowly decreased and the energy of the particles decreases, they become more and more stable, which is also expected to happen in the above mentioned approach. 

Local Beam Search
Instead of storing only one state in memory at a time, we can exploit the available memory (which is much larger in practice) by keeping track of k different states simultaneously at a time. All the successors of these k states are considered and if any one of them is the goal (global optimum), then it is returned and the algorithm terminates. Otherwise, the best k states is selected out of all the successors. This process is different than running k independent hill-climbing searches in parallel, since these k processes are dependent in this case. For instance, if the successors of one state are better than all the successors of the other k-1 states, the former will be chosen instead of the later. This effectively results in a communication between the processes which leads to continuation of better processes and halting of poor ones. The disadvantage is that the diversity may be reduced as all successors of a single state may get selected, and other diverse states may be rejected. In fact, quite often, all the k searches end up at the same local optima. The following modification resolves this problem.

Stochastic Beam Search
In this algorithm, k states are randomly selected from all the successors of the current k states, with a bias towards the better states, i.e., the probability of a successor getting selected is directly proportional to the value of its objective function. This approach closely resembles the process of natural selection, where offsprings (successors) of the states (organisms) have a greater tendency to survive (getting selected) if they are more fit (have a higher objective function).

Genetic Algorithms
These algorithms are motivated by the process of natural selection, evolution and sexual reproduction processes. The main idea is to generate the successor states (offsprings) by combining (crossing-over) the current states (parents). We start with k randomly generated states, called a population. Each state is represented by a string over some alphabet. For example, in the 8-queens problem, a state can be represented by a string of the digits 1-8 where the i-th digit represents the position of the queen in the i-th row. We also define a fitness function which associates a value to each state in the population, and the value is proportional to the quality of the state (better states have higher fitness values). For the 8-queens example, the number of non-attacking pairs of queens can be chosen as the fitness function (reverse of the heuristic function). The successor states are then generated using three processes, namely, random selection, crossover and random mutation.

Random Selection
This step involves random selection of k states from the population with replacement, with greater probabilities of selection for states with higher fitness. The selection probability of each state is calculated as the ratio of its fitness to the total fitness of all states in the population. This probability is analogous to the chances of survival of an organism based on its fitness for survival. After sampling k states, the resulting set of states has a higher overall fitness than the original population, and the remaining states in the original population 'die', since they are comparatively unfit.

Crossover
The sampled population is randomly partitioned into k/2 pairs, and each pair is then crossed over using some crossover scheme. For the 8-queens example, we can split the strings in a pair at a randomly chosen crossover point, and the suffixes of both the strings are swapped. Thus, each pair of parent states generates a pair of offspring states which results in a new set of k states (analogous to a new 'generation' of offsprings). These offspring states have combined properties of their parent states, which were able to survive due to high fitness, and therefore, are expected to be more fit (and thus, closer to the optimum solution). The effect of this step is that the search jumps to a completely different set of states in the search space, which cannot be achieved in a single step in local (or stochastic) beam search. Thus, it transcends the limits of local search, which only considers the immediate neighbourhood while making a move.

Random Mutation
After the new set of states is obtained, each state is randomly mutated independently with a small probability. For example, in the 8-queens example, zero or more digits may be randomly altered in the string corresponding to each state. This corresponds to randomly choosing queens in the crossed over states and changing their position randomly in the same row. This process is analogous to evolution in biology, which introduces small changes in organisms to better adapt them to their environment.

Advantages of Genetic Algorithm
This algorithm is a variant of stochastic beam search, which is analogous to asexual reproduction. The primary attraction of genetic algorithms is the crossover step, which closely resembles mating in organisms. The advantages are that this algorithm can find solutions which local search algorithms can't find, because they usually get stuck before reaching such a solution.

Disadvantages of Genetic Algorithm
This algorithm is mainly successful in problems where smaller configurations of good quality which are found by the search can be combined to get larger configurations of better quality than its components. Therefore, the success of this algorithm is considerably affected by the way in which the problem is framed. Since there are a huge number of tunable parameters in this algorithm, it is difficult to replicate the performance on other problems. Moreover, there is no evidence that these algorithms perform better than hill-climbing algorithm with random restarts.

Genetic Algorithm for Travelling Salesman Problem (TSP)

Representation: We can number each city in the problem and then represent each state (valid solution) as a permutation of these city numbers, which corresponds to a sequence in which the cities are traversed.

Mutation: We can randomly swap two cities in a given state, but a better approach would be to randomly choose two cities and swap them only if the total cost is reduced. Another approach can be to exhaustively search all the pairs and swap the one which results in the greatest reduction in the cost.

Crossover (greedy): We can start by randomly choosing a parent and pick its first city. Then we compare the next cities after this city in both the parents, and choose the closer one. If one of them was already selected before, choose the other one, and if both of them were already selected, choose some random non-selected city, and then repeat the process.  

Mixability
In the random selection step, it may happen that some state is selected more than once and it may not be the fittest state among the population. Since such a state may have greater number of copies in the sampled population than other states, it gets crossed over with different states and then, many states will have the characteristics of this state in the resulting offspring states. So, this particular state has greater mixability than other states. Thus, the fittest state in the initial population may not exhibit the greatest mixability if the sampling is done randomly.

Gradient Descent - Hill-climbing for Continuous State Spaces
Hill-climbing algorithm was applicable on problems with discrete search spaces, i.e., each state had a discrete (finite or countably infinite) set of successor states. In such problems, we were able to compare each of the successor states one-by-one and choose one of them according to some strategy. In the case of problems with continous state spaces, one approach is to discretize the state space and apply the hill-climbing algorithms. But, a better approach is to apply gradient descent (or ascent, as required by the problem). In order to calculate the gradient of a multivariable function at some point, the function must be continuous and differentiable at that point. For a given function f(x_1, x_2, ..., x_n), we compute the partial derivatives of f at a given point (say P = (k_1, k_2, ..., k_n)) with respect to each variable x_i, say d_i. Then, the direction of the vector D = (d_1, d_2, ..., d_n) represents the direction of steepest ascent with respect to the P, and its magnitude gives the slope of the ascent (D is called the gradient of f at P). Then, the update step (in case of gradient descent) is defined by P = P - l*D, where l is a scalar value which determines the size of the step taken opposite to the direction of the gradient. Repeating this process several times leads to a local minima of the function f (a global minima if f is a convex function).
Similarly to the simulated annealing method, we can gradually decrease the value of l, starting with a larger initial value.    

References:
[1] Lecture Slides
[2] http://ai.berkeley.edu/home.html 
[3] http://www.cse.iitd.ac.in/~mausam/courses/col333/autumn2019/ 
[4] http://www.cse.iitd.ac.in/~mausam/courses.html 
[5] Russell, Stuart J. (Stuart Jonathan). Artificial Intelligence : a Modern Approach. Upper Saddle River, N.J. :Prentice Hall, 2010
[6] https://en.wikipedia.org/wiki/Semilattice