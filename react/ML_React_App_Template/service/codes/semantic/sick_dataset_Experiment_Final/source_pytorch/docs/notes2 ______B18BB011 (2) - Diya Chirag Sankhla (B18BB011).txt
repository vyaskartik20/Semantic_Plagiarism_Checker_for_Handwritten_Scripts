Artificial intelligence 
Name- Diya Sankhla(B18BB011)
While solving 8 puzzle problem we saw how A* search with heuristic equal to the number of titles incorrectly placed from ideal location performed better compared to UCS when the number of steps to real goal states increases. Other than this we can define Manhattan distance as a heuristic in 8 puzzle problem, here manhattan distance will calculate the distance of title to its ideal location from the current state. This heuristic is admissible as we move to an ideal location by ignoring other tiles and it even gives the better result as the number of steps to reach the optimal path increases, the number of nodes to be expanded decreases compared to UCS and older heuristic but this increases computation, thus if we try to build heuristic value close to the actual cost, we need to do a trade-off between the number of steps to reach desired location and computation this is its advantage and disadvantage respectively. True cost as heuristic value is also admissible because the heuristic value is not greater than the actual cost. The Heuristic value equal to 0 is admissible as it is lower bound of the actual cost. The maximum of two admissible heuristics is also admissible. If h(A) is admissible and is greater than and equal to h(B) then h(B) is also admissible as h(A) is the lower bound of actual cost and h(B) is the lower bound of h(A).
Initially, when we used to perform tree search we used to redundantly visit explored nodes continuously. Repeating the same visited nodes increases computational exponentially, note that while searching Tree with BFS algorithm we need not bother about redundancy because while expanding horizontally we know that if we have explored that node earlier then there is no need to expand it again. To avoid such repetition we perform graph search and its basic ideology is to keep track of visited/expanded nodes by maintaining a set known as a “closed set”.Its general implementation is as follows: first, we start with the fringe containing the root node and an empty set to keep track of explored nodes. Then we will repeat the following steps: if the fringe is empty then give no solution, from fringe remove the node if that node contains a solution then return the solution, else put that node in the close-set if not present then expand the successor node and add it to the fringe.Set is maintained to store explored nodes over the list because searching across the set is very fast and efficient compared to the list by avoiding redundancy while searching. Since each node in the path towards the goal is expanded, it will ultimately lead us to a solution. It might not be the best one but it is complete. Optimality in Graph search will depend on the Algorithm we are using like UCS will provide us with an optimal solution even in graph search while A* Graph search fails to provide optimal even though admissibility is taken care of, to make it work heuristic need to be made consistent. Consistency implies that heuristic value between two nodes suppose A and B(h(A)-h(B)) should be less than equal to the true cost to travel node A to B simply saying it is considering arc cost. While admissibility was defined as the heuristic value of the current state less than equal to the actual shortest path cost to reach the goal node from the current state. Searching from node A(start node) to G(Goal node) via B(intermediate node) admissibility is defined for A to G while consistency is maintained for each path A to B and B to G(intermediate nodes are also considered). A consistent heuristic is always admissible. Consistency signifies that the value of f never decreases in a way towards goal, because f is the summation of g and h value, and g value will increase while approaching towards goal and h value has a constraint that it should not be greater than arc cost. While exploring tree search A* used to explore based on incremental f value. While in graph search, A* with consistent heuristic explores the path with the least cost/optimal solution before a suboptimal solution. This ensures that A* graph search is optimal with a consistent heuristic.
In search, problem planning and Identification can be broadly differentiated. In Planning, Agent performs the sequence of action to reach a goal while searching an optimal path, it is primarily associated with the planning of paths having different costs, and it is performed under the guidance of Heuristics like in A* search. While In identification, the agent is involved in identifying the goal not the path, the path is not a primary concern. Here agent's objective is to come up with an assignment to all the variables from a related domain with constraints been satisfied. Paths are of the same cost, and Constraint satisfaction problems are specially used for goal identification and there could be several goals. In standard search problem states are stored in any data structure, successor function could be any sort of action and cost associated, and goal test can be primary functions determining whether the goal is reached. While In constraint satisfaction problems, a set of variables(Xi, i is an index) are defined as states and values to the variables are taken from specific Domain(D), constraints on these variables are determined as goal tests. Examples: let’s consider the map coloring problem of Australia, here we need to color different states in such a way that no two adjacent states have the same color from the set of fixed colors. Formulating this problem as CSP: variables will be 7 different states of Australia, Domain is a set of colors like Red, green, blue, and constraints can be put in two ways, implicit constraint says no two adjacent cities should have the same color and explicit defines a subset of possible colors between adjacent cities like WA, NT can be assigned{ (Red, Green), (Green, Blue) …} respectively. The solution is to come up with an assignment to these variables with constraints not been violated. Example 2: N queen problem, here we aim to arrange N queens on an N*N board with no attacking configuration between two queens. Formulating this as CSP 1: Variables can be defined as the location of queens on the chessboard as X(ij), 4*4 chess boards will have 16 variables. The domain can be either 0(no queen present at a specific location) or 1(queen present at a specific location). Constraints will be, no two queens to be present on the same row and the same column, similarly, diagonals should not have more than 1 queen, and most important is that there should be N queens present on board. The solution will be arrangements satisfying these constraints. Another Formulation: Variable will be Q(k) indicating Q1 in row 1, Q2 in row 2 up to k rows. The domain will be a value from 1,2... N(columns location Queen can take in each row). The implicit constraint will be that no two queens to be in attacking positions eg consider pair of (Q1, Q2), the explicit constraint will be like a subset specifying location such a way that there should be a difference of at least one block like Q1 at column 1 then Q2 at column 3 or 4, similarly for other pairs. In such a way we can formulate constraints for N queens and come up with the desired arrangement. Representing CSP as a constraint graph can simplify the search. Variables can be represented as nodes and constraints as arcs/edges connecting nodes. Eg in the Constraint graph of the Australian Map, states are represented as nodes, and adjacent states are connected by edges with corresponding constraints. Note that here constraint is between two nodes therefore it is known as Binary CSP.Example of Cryptarithmetic as CSP: TWO + TWO =FOUR, assumption: the base is 10, all variables are different and ‘+’ is standard addition sign. Variables will be letters and carry. The domain will be base 10 {0,1,2…..9}, each variable will be denoted by one value. Constraint, all letter[T, W, O, F, U, R] will be different, and several constraints like O + O equal to R +10X1; X1 + W + W equal to U + 10X2, and so on; Note that here there are more than two variables in single constraint, therefore it's an example of Higher-Order constraint and in Constraint graph, auxiliary nodes can be used to connect several nodes. Example Sudoku: Variables will be the number of squares(9*9,81 variables). The domain will be {1,2….9}. Explicit Constraints- All nine digits to appear at most once in a row and a column and in a subset(3*3 block) all nine digits to be present. The implicit way we can check pairwise inequality between neighbors and assign numbers. 
CSP can also be applied to interpret the geometry of an object by analyzing the image and drawing its line diagram. Example cube with one-fourth part removed, here variables are each intersection, constraints are defined between adjacent variables and the solution will be its 3-D interpretation as a line diagram. Based on the types of variable CSP can be classified into two types Discrete CSP and Continuous CSP. In discrete CSP variables are discrete and they can take values from both finite(Like fixed set) as well as an infinite domain(any integer or string etc). While Continuous CSP has continuous values. Eg linear constraints can be solved using LP methods. Continuous Variable can be optimized easily compared to Discrete values. There are 3 types of constraints, Unary constraint - constraint on only one variable, example map coloring problem of Australia, WA not equal to green color, Domain for a particular variable is reduced by putting a restriction on it. Binary Constraint - constraints between two variables is a binary constraint, example the same map coloring, adjacent cities cannot be of the same color. Higher Constraints - Constraints with more than two variables and they are complex, for example, cryptarithmetic column constraint. In CSPs we come up with several solutions but suppose we want to minimize the cost of coloring in Map problems, we would prefer to maximize the use of the least-cost color in the solutions this is known as optimization of problem, here it is optimizing cost. Here constraints are satisfied as well as we preferred one color over others, this is how optimization is different from satisfaction.CSPs can be implemented to solve many real-world problem examples like Making timetable - arranging classes for every batch, Job scheduling, Transport scheduling - scheduling time and route for vehicles, Hardware arrangement -arranging all components of the electric circuit such that everything can be accommodated on small board, etc.CSPs can be formulated as a standard search problem, states can be defined as a variable with assigned values, initial or start state will have no variables assigned, the successor function will assign values to non assigned variables and the goal test is based on assigning values to all variables with constraints being taken care. Searching can be done through Naive approaches like BFS and DFS. Starting with BFS first initial state will have no assigned variables. The start state will have a higher branching factor and as we go down in the search tree branching factor decreases, the depth of the search tree is based on the number of variables. Then we will assign value to any variable and then will go level by level assigning values in BFS, after assigning all values, assignment satisfying constraint will be the solution. Unlike uninformed search here we need to reach the last level to get the solution so we need to build the whole tree which is memory consuming and it is disadvantageous. With DFS, the initial state will not contain any assigned variable, DFS will assign value to any one variable and will move to the next level, will choose one path and will assign value accordingly and after the assignment is completed and constraints are satisfied they will return solution, else will continue the search. While solving CSPs DFS is advantageous over BFS as we get a solution(without constraint violation) in one of the paths and then we can stop the further search. But these naive approaches are still searching a lot, and the search is complete when all variables are assigned and if they are not following constraints then there is a need to backtrack to meet constraints. This excess searching can be avoided by bringing some changes to DFS. First, by assigning a single variable at a time by ordering variables. Second, by considering constraints while assigning and then assigning non-conflicting values to an unassigned variable. Incorporating these two changes with DFS can be termed as Backtracking Search this is also an uninformed search algorithm as it builds upon DFS, it helps to reduce time and increase computational speed. Using the Backtracking search n Queen problem(n nearly equal to 25) can be solved in proper time. With constraints, variables and domain stated for the problem we begin backtracking search by picking up unassigned variable based on order and similarly assign value in a particular order if that assignment is not violating constraint then we return it and move to the next unassigned variable else we backtrack last assigned variable and continue our search, this is how Backtracking can be implemented.The backtracking search can be further improved by ordering variables as well as ordering values from the given domain to minimize backtracking. Earlier we used to backtrack when we find a violation of constraint, instead, if we look for failure in advance and try to avoid those possibilities then the useless search can be avoided. This is the job of filtering, it will keep track of domain available for unassigned variables and will neglect those options which will lead to violation of constraint(failure) in the future. Lets again take the example of the Map Colouring problem with forward check, its a type of filtering it will try to remove values(color) from the domain which can violate constraints in the future, there is a shrinking of the domain after each variable been assigned and it will check for the empty domain.There are seven states(variables) of Australia {WA: neighbour of (SA,NT),SA: neighbour of (NSW,NT,V,Q,WA),NT : neighbour of (WA,SA,Q),Q: neighbour of (NT,SA,NSW),NSW: neighbour of (SA,V,Q) and last T: has no neighbour}.The initial assignment is empty, then let’s suppose we started with WA(Domain={R, B, G}) suppose we assign red color to it. Then domain of its adjacent state NT and SA (Domain={B, G}) will shrink Now suppose the neighbor of NT and SA, Q(Domain={R, G, B}) is assigned Green color, then the domain of NT and SA will shrink further to Blue and for NSW(Domain={B, R}) and now suppose we assign Blue to state V(neighbor of NSW and SA) then the domain of SA will be empty and NSW(Domain={R}). The SA's domain is left with no assignment, here we have not violated constraint but we have come across the possibility of violation in the future, the forward check will stop the search when finding the empty domain of any variable and will backtrack to come with other possibilities, Note we are backtracking before the violation. The forward check has helped us to avoid redundant search by early detection but problem is that it only gives us some failure, not all failures. And this can be improved by putting constraint checks as well. This will increase computation somehow but will ultimately reduce search time. In the Earlier example only after assigning Red to WA and Green to Q domain of NT and SA was reduced to Blue and we ignored the constraint that NT and SA are neighbors and both of them cannot be Assigned Blue color and we moved further and assigned Green color to V which created the possibility of failure but if we have stopped earlier and taken care of constraint by backtracking to last assignment and coming up with other possibilities then this failure could be detected earlier. This is how constraint propagation tries to avoid the situation of the empty domain.
This will lead us to the consistency of arcs in the constraint graph. Consistency can be defined in such a manner suppose we have to two-node(variables) A, B connected by an edge(directed arc from A to B) in constraint graph, the arc can be called consistent iff for all the values for B(Tail), there are some values in A(Head) which cannot violate constraint between them. Simply saying we are removing values from the tail not head to make that arc consistent and after the deletion of the value we need to check neighboring arcs, in the worst case when all variables are neighbor of each other then all arcs(n*n pairs n is the number of variables) need to be rechecked for violation. We simply want to achieve consistency between all arcs of CSP to avoid constraint violation in the future even before forward check but there is a huge computation for every assignment of value. Arc consistency can give us one or more solution or might not give any solution because Arc consistency check for constraint between pairs not between multiple nodes, This is its limitation. Other than filtering we discussed ordering to improve Backtracking. Variable ordering is a greedy approach done based on the minimum remaining value. While doing the assignment variable with the least domain value is preferred over other variables to avoid the situation of empty domain because they are most likely to violate the constraint. This is also known as the most constrained variable. Ordering domain value can be done by picking that value for a particular node(if multiple values in the domain) which lead to the least shrinking of domain values for other variables and this reduces chances of the violation. Example of the map: if we assign Red to WA and Green to NT then Q can be assigned either Red or Blue.case 1: Assign Red to Q, then SA can be assigned Blue. But in case 2: if we assign Blue to Q then SA which is a neighbor of WA, Q, NT cannot be assigned with anything. The backtracking algorithm can be improved by combining Arc consistency, Ordering Value, and Ordering Variable for CSP.
As discussed so far in uninformed, informed search and CSP, agents try to create simulations of the situation and accordingly come up with a path to reach the goal. And these simulations require high memory to keep track of states and the search tree expands as the number of nodes increase. But in Local search, the agent will not simulate anything, it will look for its neighboring states and make moves accordingly, basically, an agent is searching locally and it requires less memory as it only keeps track of the current node and expands neighboring nodes. This advantage of less memory can be useful in searching even large state space. Local Search can be formulated as an optimization problem by developing a function called the objective function, which gives value to each state, and search algorithms aim to find the states with either minimum or maximum objective value. In Hill climbing search suppose we aim to find local maxima, the problem is stated as input, and states are well known to us then it can be implemented as follows: from the current start state, we will look for all neighboring states if the current state has a higher objective value than all neighboring states then we return that state as it is best among the neighbors, else we would move to a state with the highest objective value and update it as current state and will repeat this task. If we aim to find local minima then we need to reverse the inequalities of tasks within the loop. Values can be either heuristic or objective value and the search continues till it finds local maximum value. And if there are multiple successor nodes with the best possible value then anyone can be picked randomly. It is called hill climbing because we are climbing gradually towards maximum value with amnesia(no memory). The end state will depend on the starting state, in case of maximum objective search, it can reach global maxima, local maxima, or flat local maximum in which current and neighboring states have the same objective values. Hill climbing will be stuck after finding the local maximum even if there is the best value(global maximum) present because it's looking for neighbors and after reaching the top of the local maxima neighbors are at a low value so it will stick (max objective case). N-Queen Problem can even be formulated as a constraint optimization problem. States can be defined as an arrangement of N queens on board. The successor function can be defined as the movement of the Queen to a neighboring square in the same column(restricting movement in a row to avoid randomness of movement) and the Objective function can be defined as a number of pairs of Queen in attacking configuration and we try to minimize objective value to 0. Let's suppose we have 8 queens on the board and 2 are in attacking positions while others are arranged in the proper configuration. If we consider CSP then it won’t be a solution but according to Hill climbing it’s a solution that might not be optimal as 1 pair is in an attacking position but here Hill climbing will get stuck at local minima. All the configurations of 8 queens are a solution and the complete solution is the one with an arrangement of no two queens at attacking position. Hill climbing during N queen(N=8) problem gets stuck maximum time(86%) at the local minimum and only around 14% time will reach a complete solution but it is advantageous as the number steps to get success is pretty less like 3 or 4. Three drawbacks of Hill Climbing are that it mostly gets stuck after reaching local minima or maxima, second in Plateau type it gets stuck because of the same value in the long run, third it doesn't allow diagonal moves either we can move in x or y-direction. Sideway moves help to resolve issues with plateau seen earlier and even it increases chances of a win by 94% from 14% in 8 queen problems. The limit of Sideway moves can be fixed to avoid getting into infinite loops. Unlike the local search, it tries to keep track of many nodes other than the current node which ultimately increases the chances of getting a complete solution but there is a significant increase in the number of steps and this is its drawback. Tabu search can be done by adding states we visit into a Queue and removing the oldest one but we need to avoid loop between states, in such a way it tries to keep the memory for some states which can help us to solve while escaping Plateau. Another way to avoid plateau by performing BFS to explore all neighboring nodes when a plateau is reached and after finding better value we can continue Hill climbing search but it consumes a lot of time.The drawback of getting stuck in local minima or maxima while Hill Climbing can be avoided by allowing randomness in choosing a state or simply saying by introducing stochastic variation in Hill climbing. Like earlier, we used to choose the one with the best objective value based on the problem but in Random walk, the neighbor with the best value will be chosen with probability p, and with 1-p probability, a neighbor can be randomly chosen.This randomness can help us to escape local optimizations. While in random restart after completing the run we will again restart with other states as starting state and by trying again and again(for a fixed number of times or until it terminates) with several initial states we can reach a complete solution. Random walk and Random restart both individually give a complete solution, a combination of both of them with hill climbing will even ensure completeness of the solution. This combination can be implemented by beginning from the initial state, then either we choose neighbor randomly(1-p probability) or with the best value(p), and after completion of one run, we can repeat the same process from other states.
Simulated annealing is another local search algorithm inspired by the annealing of metals which brings particle to a stable state. An example we have the ball rolling from a hilltop in a range of hills and the ball aims to reach the global minima but it might get stuck in the local minima and it won’t be able to rise until some level of shake(energy) is not provided to get out from local minima and reach global minima here shake is been referred as the context of temperature, we go on reducing the temperature, which reduces randomness or level of shake to reach frozen(desired) state. Initial local minima require more shake as compared to the next local minima. In this example chances of escaping local minima increases with the introduction of shake. This was the first local search algorithm we discussed so far who can come out of this local optima. The disadvantage is that it is very slow as time passes because of a gradual decrease in Temperature. It’s highly used for VLSI arrangement and even in problems of traveling salesmen etc.
Local Beam search is another version of the Hill climbing search algorithm. Here we keep the memory of some k random states. Starting with k states, we will explore neighbors of all k states if any of the neighboring states is a goal then we stop search and return it else we choose the best k states based on objective function among them and repeat this. The advantage is that we can keep track of k states to get a complete solution. The disadvantage is that we end up choosing k states from all the neighbors and they might lead to the same optima, simply saying diversity is not maintained. This disadvantage can be resolved by bringing some randomness in selecting k states and then choosing the k successor which are good and this can maintain diversity. Stochastic beam search is homologous to natural selection. Consider k states as k different organism and each organism producing children's(successor), just like 1 amoeba dividing into two and so on and best of the fittest k successor will be selected(survive) else will be eliminated. This has driven our path towards a genetic algorithm that is somewhat inspired by the genetics of Homo Sapiens. Here pair of the organism(parent state) generates children's(successor) and each state is associated with a function describing its ability to survive. States with higher fitness values are better(high survival chances). For the 8 queen problem by constraint optimization, unlike the objective function, the fitness function can be described as how many queen pairs are in a non-attacking configuration. Simulations of evolution drive the generation of children states, first is a random selection of k states from the population of states(organisms) with better survival chances. Second is combining the characteristics of two states to come up with new states. The third is random characteristic changes in successor which is somewhat analogous to random mutations in an organism. In this context, fitness is based on the closeness of the current state with the goal state. Local Beam search involved single parent to generate child while in the genetic algorithm we begin with the crossover between two-parent state producing children. Let's formulate N Queen(N=8) problem as a Genetic algorithm. Here initial population or state configuration can be taken as a string of queens location, such that no two queens in the same column eg {16257483} mean queen 1 at row 1 and column 1, while queen 2 in column 2 and row 6 so on. Then we define fitness function as the number of queen pairs in a non-attacking configuration here max value could be 28 or the minimum value could be 0. One with the least fitness probability is eliminated and the others which are selected undergo crossover in pairs. We randomly describe partition in a string of state in a pair and we take one portion either left from one parent and right from other(or vice-versa) and combine it to generate new child with characteristic inherited from both parent states. Randomly changing the position of one queen in a state to arise with new characteristics eg from{32748552} to {32741552}. Crossover is advantageous as it will jump to a very far state, which is not easy reaching in a short span by other algorithms and it's better as we can easily skip local minima or maxima and this Algorithm is fast compared to Hill Climbing. However, this algorithm is highly uncertain because of randomness at each step, whether selection, crossover, or mutation, and replicating results to a problem is very tricky until all information is not provided.GA is not only about individual fitness, but it's also based on how much individuals blends or crossovers with others producing new characteristics. Example: Travelling salesman across 9 cities can be formulated as the genetic algorithm by taking input string as the order of cities been visited like[923517648] means first visited 9th city the 2 so on...Here Crossover can be done by picking up the first city of parent state and comparing with remaining cities of other parent state and choosing the closest city from the current city and if that chosen city is already present in a tour to visit all cities then we choose other available cities in such a way we repeat this process. Mutation can be introduced in three ways first either swapping the position of any two random cities, second this can be improved by swapping those cities which will account for the least distance to be covered, and third, by iterating over all the cities and then swapping pairs after evaluating it to avoid any randomness, this is a greedy exhaustive search. So far we have discussed optimization for Discrete states, while optimization for continuous function can be achieved by moving in a way towards gradient. Based on whether we want to minimize or maximize objective function Gradient descent or ascend is performed respectively.
In given continuous function of n variables(x1,x2...xn) we can begin by computing derivative for each xi with respect to all the variables, this gradient provides us tangent direction. If we aim to minimize objective function then we can descent the gradient and update variable xi as (x1-lambda*gradient), lambda is the number of steps to descent down or ascends. Larger lambda indicates a big step while smaller lambda indicates a small step to descend or ascend. We can choose either large value of lambda and decrease gradually just like simulated annealing, this ensures to take big steps to reach better minima or either we can choose small lambda and goes on increasing with a decrease of a gradient.These steps are repeated until the gradient turns out to be 0 if convex surface then gradient descent leads to global minima else local minima like hill-climbing is reached.Netwon Raphson method is another method for optimizing continuous function ,unlike gradient descent it consider both first and second derivative it tries to minimize polynomial surface by finding roots which provides better direction to move,that's why it perform better than descent gradient but calculation of second derivate increases computation ,therfore its not highly preferred.

References:
1.http://ai.berkeley.edu/home.html 
2.http://www.cse.iitd.ac.in/~mausam/courses/col333/autumn2016/lectures/04-lsearch.pdf
3.https://www.seas.upenn.edu/~cis391/Lectures/CSP.pdf



