-----------------------------------------------------------------------
Course Name: Artificial Intelligence
Course Code: CS323
Course Instructor: Dr. Yashaswi Verma
-----------------------------------------------------------------------
Submitted by -
Name: Devin Garg
Roll Number: B18CSE011
-----------------------------------------------------------------------
Class Notes-2
Dates Covered: 14 Sep. 2020 - 25 Sep. 2020
-----------------------------------------------------------------------

------------------------BEGIN------------------------------------------

Relaxing a problem - to obtain an admissible heuristic: Admissible heuristics have the condition that the heuristic value is greater than or equal to zero but less than the true cost of the state to the goal. Now, we want to use heuristics that are optimistic. This means that the heuristic should always state the h-value for a particular state to be not more than the true cost for that state otherwise it would have the problem of keeping a valid path's node on the fringe for too long. 

So, how does relaxing a problem get us an admissible heuristic?
We can see that if we are relaxing a problem, we are ridding us of some constraints that would otherwise have cost us more to satisfy. This in effect means that we are making it 'easy' to solve the problem which in turn means incurring less cost than the true cost and hence we can definitively say that such a relaxation would lead to an admissible heuristic. (As the h-value, in this case, would be upper-bounded by the true cost).

Now, coming to the trade-offs involved in the process. There are two bases on which we need to think - the amount of compute we are using and the number of tree nodes we will end up expanding.

Amount of compute - The trade-off here is with respect to how good of a heuristic we want. If we are bent at using a very good heuristic which would lead us almost directly to the goal state without exploring a lot of useless paths, we would need more amount of compute to obtain said heuristic. On the other hand, if we are fine with going through some not-so-useful paths, then a not-so-good heuristic that can be computed is less time would be fine.

Number of nodes expanded - The trade-off here is with respect to the exploration (in effect construction) we intend to do of the search tree. If we wish to get to the goal state with minimum expanded nodes, we would need a very good estimate of the true cost w.r.t. any node which is basically the heuristic value. Hence the two - amount of compute and number of nodes expanded are directly related by the inverse relation. The more the nodes expanded to get to the goal state, the lesser is the amount of compute needed to calculate the corresponding heuristic needed.

Consider the example of the 8-puzzle. In this, we have a 3x3 board in which 8 tiles are placed. This means there is one empty space that can move about because of any other tile taking its place. Given the tiles are in any haphazard manner, we wish to move the tiles in such a way, that ultimately we end up in a state where the empty space is at the (1,1) position, and all the other tiles are arranged in increasing order from left to right and top to bottom. To get from a random state to this final state using A* search, we need to come up with some admissible heuristic function. Now, as we have seen above, for obtaining admissible heuristic value we need to relax the problem. So, we relax this problem in the following way - we just think of the problem not in terms of sliding the tiles but simply removing a tile from the board and fixing it in its designated place according to the goal state. In doing so the heuristic function is basically the number of tiles that are not in their correct position. If we come to think about this heuristic, it is a very 'safe' bound, not very close to the actual cost. So, we try to improve upon it. Instead of simply removing the tile and placing it anywhere we want, which turned out to be too much of a relaxation, we get a more practical value of the heuristic, by relaxing the problem as follows - consider the manhattan distance of a tile from its current position to the position it should be, in the goal state. This would give a better estimate because here we have some semblance of how a tile would end up being at its target place. Any given tile would have to cover at least the above mentioned Manhatten distance, so, this is an admissible heuristic that is closer to the actual cost than the above attempt (of simply removing and fixing the tiles).

The 'better' heuristic and the concept of Dominance:
When it comes to comparing two heuristics, the concept of dominance gives a very good idea as to which is a 'better' heuristic. In this case, 'better' is being used to describe that heuristic that is closer to the true cost for every node. (Nevermind that it may require more computation). Let 'h_a' and 'h_b' be two admissible heuristics that are to be compared. If for all nodes in the search space, h_a's value for a node is greater than or equal to h_b's value for that node, then h_a is said to be a heuristic that dominates h_b. Dominating here is in the sense that the heuristic has a greater (or at least equal) value w.r.t. all nodes and hence is closer to the true cost (as both of them are admissible). Now, since there are multiple admissible heuristics possible for a given problem, we can use a 'semi-lattice' to represent all of them. This semi-lattice is a structure containing all the admissible heuristics under consideration. At the top is obviously the exact heuristic i.e. one whose value for any given node is equal to the true cost of that node from the goal. At the bottom is the function h = 0 which in effect reduces the corresponding A* search to Uniform Cost Search because effectively we don't provide any guidance to the search. In between these two top and bottom cases, lie all the other admissible heuristics. Note that if two heuristics don't satisfy the above-mentioned constraint of dominance, i.e. for some nodes h_a has a higher value and for other h_b has the higher then, two such heuristics are kept parallel to each other. Otherwise, the dominating heuristic is placed on top of the heuristic that it dominates. Ultimately, if there are two heuristics at the top of the semi-lattice, then we can take the maximum of the two h functions to get the effective heuristic value for any node. This would always be admissible because the values whose max is being taken are themselves within the bounds of admissibility. Also, as might be obvious already, a heuristic higher up in the semi-lattice provides a value closer to the actual cost and so might be more costly to compute. We can also say that the exact one at the top dominates every admissible heuristic for the problem whereas every admissible heuristic for the problem dominates the h=0 (trivial) heuristic function.

Graph Search:
The motivation behind looking into graph search is not having to expand the same node again and again when reaching to it via different paths. Going back to tree search, we know that whatever we add to the fringe is expanded by using some strategy or another. However, this also adds the negative functionality of having to expand the same node again, if we have it added in the tree at some other position. The approaches that we have looked at before, cannot really get rid of this problem and so we use graph search to escape it.

The basic idea behind graph search is to keep track of the nodes that we have expanded before and not to expand them again. What this means is, we basically follow whatever algorithm we have been using for tree search along with one added caveat. This caveat being that we maintain a "closed set" of expanded nodes, so whenever we come across a node, before expanding it we try to look for it in the set. If the node is not in the set, it means we haven't previously expanded it, so we expand the node and add it to the set. However, if the node is already in the set, it means we have expanded it before and we simply skip this node and not expand it again. A small implementational detail to note is that it is wise to maintain this set of expanded nodes as a set and not a list because the access time to look for a node in a list would be disastrous for large spaces and sets would be much for efficient. 

Properties of graph search -
1. Used to escape the redundant work of expanding nodes again that we already expanded before.
2. Completeness - Graph search is bound to be complete because we are simply preventing any repetitive work which won't stop us from exploring the search space as we would normally do to find a solution. This means that if there is a solution which we were able to find using a complete tree search strategy, we would definitely be able to get to it using the corresponding graph search. 
3. Optimality - This is a point where we need certain extra constraints because in general, graph search is not bound to be optimal. Say, for example, we perform A* graph search using some admissible heuristic, we can get stuck with a non-optimal solution. This would happen in a case, where we are following a non-optimal path say, D -> E -> G -> F where D is the start state and F is the end state. Consider the situation when we add F to the fringe. However, due to the heuristic values we end up expanding some other child of D, say L i.e. we expand in the manner - D -> L -> G. Now, we try to expand G via this second path which, say, gives us the optimal solution, but since G has already been expanded, it would be in the closed set and we would end up skipping it. This would cause us to try and expand F, which is the goal state so we stop. Finally, we end up with a non-optimal path. The problem that crept up which caused such a scenario is the inconsistency in the heuristic values. Somehow, the heuristic function either didn't decrease towards the goal as it was supposed to, or the decrease wasn't that much in magnitude that would support the consistency required to reach the optimal solution. Enter the concept of consistent heuristics. 

Consistent heuristics - To put it simply, heuristics that leave us with such f-values (i.e. g+h values) that never decrease along a path are called consistent heuristics. This is a tougher constraint than admissibility as would be evident shortly. Admissibility simply indicated that the heuristic value at every node should be within the bounds of [0, true cost]. However, it does not say anything about the relationship between nodes and just concerns itself with the relationship of a node to the goal. Consistency, on the other hand, ensures that the heuristic values for successive nodes along a path are agreeable w.r.t. with the edge costs between two nodes.
E.g. To understand it better, consider the example of two nodes, A and B. Let an arc go from A to B with cost 3. Let 'h' be the heuristic function in use. Say, h(A) = 7 and h(B) = 2 are the heuristic function values at A and B respectively. Now, formally, consistency states that "the heuristic arc cost <= the actual arc cost." Given the situation at hand, going from A to B, the true cost of the arc is 3. However, heuristically, there is a fall of 5 (h(A) - h(B)). However consistency enforces that h(B) >= h(A) - arc_cost(A to C). This means that the heuristic value at B has to be greater than 7-3 = 4. This in effect is enforcing the constraint that heuristically, the gap should be at most as big as the actual arc cost i.e. there shouldn't be a drastic fall in consecutive heuristic value if the actual arc cost itself is not varying that much.

Proof that A* graph search is optimal (when the heuristic used is consistent) - 
We already know that A* search, in general, expands nodes according to increasing forward estimates and backward costs i.e. f = g+h values. Now, with non-consistent heuristics, the only hurdle in the optimality was in the fact that we weren't controlling how we are reaching a node. With the consistency constraint applied, we can conclusively state that we would always reach a state optimally first, before expanding it because of reaching it via a sub-optimal path. (By optimal path here, we mean the optimality of the path by looking at both the forward estimates and backward costs i.e. the f-values.) This proves that with a consistent heuristic in hand, A* graph search is bound to be optimal.

Note, that consistency includes admissibility (this would be obvious when we consider the consistency condition for all the nodes one by one).

We discussed the optimality of A* graph search above, from which we can directly conclude that uniform cost (graph) search would be optimal as the heuristic h=0 is trivially consistent.

So far we have come across path-finding problems, but that is just one category of problems that are modeled as search problems. Another important category of problems is 'Identification Problems'. These problems are just concerned with 'identifying' the values that should be assigned to the involved variables to solve the problem. These kinds of problems are not generally interested in the path that we take to reach the solution i.e. the assignment. Also, many at times, if model them as search problems, we would have the same maximum depth as all the variables would have to be assigned some value from the possible values ("Domain") for the overall assignment to be called a solution. An important class of problems that come under 'Identification' is that of the constraint satisfaction problems, which we look at next.

Constraint Satisfaction Problems (CSPs):
As the name suggests, the problems under this class are represented by a set of variables, a domain (that contains the values that can be assigned to the variables) and some constraints. The goal for these problems is to assign some value from the domain set to all the variables in the variable set such that the assignments are in agreement with the constraints mentioned in the problem. 

Contrasting the goal test with path-finding problems:  
So far, in the case of path-finding problems, 'solving' a problem implied reaching the goal state and returning the appropriate path that was used to get to the goal state. In the case of CSPs, however, solving a problem involves running the goal test at any particular state. All the goal test does is to verify that none of the specified constraints have been violated. This means, at any given time, we have all the variables and we simply check the assigned values so as to ensure that they satisfy all the constraints and if so, we say that the goal state is reached.


Some examples to get an understanding of how to model a problem as a CSP:
1. The N-Queens problem - In this problem, we are given an NxN chessboard and we are supposed to place N queens on the board in such a manner that no two queens attack each other. Now this problem can be formulated as a CSP in multiple ways. One such way could be to use no knowledge of the problem whatsoever and simply consider each square on the chessboard as a variable that is either empty or filled. The constraints in which case would be the conditions that - firstly, there must be exactly N of the squares that are filled, secondly, all the non-attacking constraints must be satisfied. Now, this is a very crude formulation and we can do better by making use of some of the information that we have about the problem. For instance, we are aware that at least N queens have to be placed on the board, so we model the problem in the following way - Let their be N variables (one corresponding to every row) and all the variables pick a value from the domain {1,2,3,...,N} such that the value of i-th variable represents the column number in which a queen is present in the i-th row. This drastically reduces the number of variables and also now we only have to deal with the constraints of the non-attacking positions and the first constraint mentioned above is satisfied implicitly.
2. The Map Coloring problem - In this problem, we are given a map and a number of colors such that we have to assign a color to each territory on the map in such a manner that no two territories with a common border are assigned the same color. Again, we model this as a CSP - the variables being the territories, the domain being the set of colors out of which we have to assign one to each territory and the constraints being the condition that no two 'adjoining' (i.e. common-border) territories have the same color.

Representing constraints:
There are two primary ways in which we can model the constraints in a problem. These are 'implicit' and 'explicit'. Implicit constraining is when we apply the required constraints in such a way that we don't concern ourselves with the actual value being assigned to the variable as long as the constraint is satisfied. Explicit constraining is when we explicitly list out the possible assignments that satisfy the required constraints of the problem. For example, in the case of the map-coloring problem described previously, an implicit constraint could be:
territory_1's color != territory_2's color where territory_1 and territory_2 have common borders.
On the other hand, an explicit constraint would be:
(territory_1's color, territory_2's color) belongs to the set {(R,G), (G,B), (R,B)} where R=Red, G=Green, B=Blue.
Another type of constraint is the unary constraint that involves only one variable. 

Constraint Graphs:
The constraints in a given CSP can be modeled with the help of constraint graphs. In these graphs, two nodes are connected if there is a constraining condition between them where the nodes represent two variables. This is the case for binary constraints. At times, when a problem involves multiple variables constrained by a single constraint, we employ auxiliary vertices in the constraint graph and connect all the concerned variables to that vertex which represents said constraint.

Formulating CSPs as search problems:
For a given CSP we model it as a search problem in the following manner - any given state is just the assignments that have happened as of yet, the initial state is the one where no variable has been assigned a value, the goal state is one where all the variables have been assigned values in such a manner that none of the constraints are violated. This kind of a formulation enables us to think of solving CSPs using the search-based approaches that we have seen so far. The difference here would be that we won't care about the nodes we have traversed so far as all the information in a particular state will be all that we require. Typically, we will have levels in the search trees built using the above formulation. Levels in said tree would represent the number of assigned variables; this is because the successor function in such a problem is just the assignment of another unassigned variable. So, going down a level in such a tree basically means picking up an unassigned variable and assigning it some value. Therefore, in such a typical situation we will usually have all the states where no variable is left unassigned at the last level of the tree.

Solving Constraint Satisfaction Problems using the search formulation:
Given a CSP modeled as a search problem, the methods available to us are quite limited in terms of getting to a solution in a reasonable time. 
Let's look at the different possible approaches like we did in the case of path-finding searches - 

1. BFS - A simple breadth-first search would be brutally beaten by other search strategies when it comes to exploring the tree of a CSP. This is because as we have mentioned above, a CSP has the goal states at the deepest level and so there will be NO cases where BFS would perform well. It would always have to do exponential work (exponential in the number of variables) to even get to the last layer and begin any useful goal testing! This is why a BFS-based approach would fail and we will generally look at DFS-based algorithms for such problems. 

2. DFS - A simple DFS would do better than the naive BFS approach, but it would still do pretty badly in general. This is because, even though this approach would first go deepest to the last level and do a useful goal test and then try other paths, it wouldn't come to know a mistake that was made at an earlier level until it explores all the paths possible after the mistaken assignment. This is a lot of useless computation when we are actually aware that we will DEFINITELY not get any solution in a particular sub-tree but still we will explore it completely before moving on to the next possibility. Let's see how we can improve upon this approach.

3. Backtracking Search - Based on the previous two approaches, we come across two main drawbacks that we need to address. The above BFS and DFS approaches are naive but they give us two ideas as to how we can improve a general algorithm to be much faster without even utilizing any problem-specific details! (contrary to the previous path-finding problems where the heuristic we chose was completely problem-dependent) These two ideas are - first, the order in which we assign values to variables and second, the testing of constraints. In the case of CSPs, we are not really concerned with in what order we assign values to the variables and so we can fix any one ordering and just go with it, in the naive case or maybe improve upon which variable to assign next for an improved algorithm. Also, a problem that we came across in the naive DFS approach was that we were doing a lot of 'useless' computation even when we had violated some constraint(s) in the first few levels. So, instead of blindly going deeper, every time we come to a new level we pick a variable, assign it a value and then CHECK! We check whether any constraints have been violated as of yet. If not, we go deeper into the next level otherwise we simply drop that line of assignments and try other things with more potential and which are not doomed to failure. This approach is essentially an 'improved DFS'. 

Can we improve upon this backtracking approach?
YES. As mentioned previously, we can improve the way we pick the variable that will be assigned a value at any given level. We can improve the order in which we allot these values so as to eliminate doomed cases quicker. Therefore, we now look at filtering and ordering as the two major concepts that reduce 'useless/doomed' backtracking to a great extent.

Filtering - 
As the name suggests, filtering is an approach we follow in which instead of waiting around for a constraint violation later in our journey to the goal, we filter out the doomed-to-fail cases early on. This would, in essence, mean that we need to be careful about any constraint violations in the beginning and do an 'incremental constraint check' i.e. even before reaching to a state with some variables left unassigned. Note that, intuitively we can say that filtering helps us exclude doomed cases quicker by 'CONSTRAINT PROPAGATION' to some extent. Without filtering, we are aware of constraint violations only near the end when all the variables are assigned and it's already too late to escape the 'useless' computation. With filtering, however, we can propagate the effect of an assignment to the nearby (in some cases far-off) nodes and exclude all those paths, where we know we won't be successful. This propagation of constraints due to new assignments is what makes filtering improve the search strategy.
There are two main ways in which we can do this; these are - Forward Checking and Arc Consistency.

1. Forward Checking - It is a filtering procedure that helps us prevent nearby constraint violations early on. In some sense, we rule out the cases that are bound to fail, but only for the nearby nodes and it still does not propagate the effect of new assignments very far and we might end up doing some backtracking (although it would be much less than the naive approach).
E.g. Consider the example of Map-Coloring. In the corresponding constraint satisfaction graph for this problem, we would have nodes representing territories, and edges between these nodes are present if two territories share a border. In this problem, if we begin by assigning some color to a country, then with the help of forward-checking, we would remove all those values from the domains of nearby nodes that we know are bound to fail upon assignment. This means, that if some node is assigned Red, we look at the domains of the 'neighbor' nodes and remove 'Red' from them so that we don't go off on a wild goose chase. Hence, we are following 'domain-reduction' to reduce the number of times we are made to backtrack due to incorrect decisions made early on. Note that, when we employ forward checking into the backtracking approach, the point when we know we need to backtrack is the point when the domain any of the variables becomes empty, which would mean there is no satisfying assignment and we have failed on this path.
A point to note here is that we are not propagating the constraints very far using this approach. Also, we don't follow any specific order in which the variables are assigned values. As a result, we may end up at a situation wherein the beginning everything was going completely fine and we had no violations among the nearby nodes, but towards the end when we assign the last few variables which were at the core of the graph, the whole thing breaks down and there are no valid assignments available to them. This makes it clear that ordering is important, but before we get into that, let's look at whether we can improve the way we perform filtering. Here, we weren't able to look at how assignments are affecting constraints far-off, so can we filter in such a way that we have a better idea in advance about when we are doing such an assignment which will lead to a conflict near the end (i.e. final few assignments). YES - the second approach to filtering i.e. Arc Consistency improves this propagation to a great extent.

2. Arc Consistency - To put it simply, the idea of arc consistency is applied to the arcs+ (we'll see why we say + here) in a constraint graph and helps in propagating the effect of an assignment to even far off variables in the graph. This method may involve more compute but in the end it causes us to backtrack a lot less than the previous approaches! Let's first consider how we look at the arc consistency of any given arc. An arc in a constraint graph links, in a way, the domains of values that can possibly be assigned to two connected nodes. Every arc can be looked at from two perspectives - one as going to a node and another as leaving the node to go to the connected one. So, how do we formally define consistency for such an arc. If we say that an arc goes from X to Y, where X and Y represent two nodes in a constraint graph, we say that the arc is consistent if for every value in the domain of the tail variable - X, there is at least one possible value in the domain of the head variable - Y that can be assigned to it without violating any constraint between X and Y. So, what do we do if an arc is not consistent? We make it consistent! How do we do that? We go about reducing the domain of the 'tail variable' till the arc is consistent (because we have defined consistency in such a way i.e. w.r.t each tail variable). An interesting point here is that there need not be actual constraints between two variables for us to consider the arc consistency between them. We could say that X and Z are not connected in the constraint graph, but we look at the arc going from X to Z. This wouldn't do us any good because if there are no constraints between two variables, the value we assign to any one of them won't affect the value we assign to the other and so we won't make any changes to the domain of either of the variables. Just as a technicality, however, the arcs we are considering here can be between any two nodes in a constraint graph and are not just the edges of the graph.
What makes this approach complex? This approach can be a bit of a pain to implement complexity-wise. This is because, every time we check the consistency of an arc and it ends up not being consistent, we need to make some changes in the domain of the tail variable of the arc. This would invalidate any of the consistencies that depended on that particular value being present in the domain of that variable. To solve this we would have to push all the neighbors of said node into the arc consistency checking queue. So as we can see the complexity can be quite bad as a single deletion from a domain can cause multiple nodes to be added to the queue. Does this even converge? YES! We can always say that this approach would converge and lead us to an answer because even though we might add multiple nodes again and again onto the checking queue, every time we do that, we are reducing the domain of some node and since we are considering nodes with finite, discrete domains at some point of time, the said domain for some variable would come to an end which would cause us to backtrack or we would end up at a solution. 
Arc consistency is enforced usually after every assignment and detects failures well in time before forward checking would. The complexity of this approach for binary CSPs is polynomial in the number of variables and the size of the domain i.e. O(n^2*d^3) (or O(n^2*d^2) in an improved implementation). This can be seen easily as in the total arcs to check are O(n^2) and at most d times (in the worst case) we would put all back into the checking queue. And every check is bounded by O(d^2) time so O(n^2*d^3) in the naive implementation.

When is arc consistency not enough?
Enforcing the arc consistency of a constraint graph involves just looking at the nodes pair-wise and declaring consistency. This, however, will not be enough in case we have, say, triangulated constraints, which would be declared consistent by this approach but would mean that we are doomed. Consider the example of 3 nodes connected to each other like vertices of a triangle. Say, all 3 of them have 2 colors in their domains. When we check for arc consistency, we get the result that all 6 arcs (2 from each constraint arc) are consistent. This is correct in its own right but it doesn't serve our purpose here as we are aware that this is already a doomed situation but we are still given a green flag by the consistency check to go ahead with the next assignment. It is only after the next assignment that we would start backtracking.

We looked at improving the backtracking from one angle i.e. early failure detection by frequent constraint checks. Now, we will look at the second concept i.e. ordering.

Ordering - 
As the name suggests ordering techniques are used to determine the order in which we should pick the variable to assign a value to next. It also includes determining the order in which a value should be assigned to a variable given the many options in its (maybe restricted) domain. These two approaches are discussed here:

1. Minimum Remaining Values (MRV) - This is a technique used to determine which variable should be assigned a value next. We've seen previously that just any arbitrary order of assigning variables values, can doom the filtering processes to fail, so at every point, we pick that variable to assign next which has the smallest domain left. We derive the intuition for this from the map coloring problem. Consider the example where we start coloring from one of the outer territories and assign a color to the first one. Now, if we skip a few of the nearby territories and assign a color to a territory very far off from the one we just assigned, we would run the risk of having to backtrack to this state if we end up failing at a later stage which would be highly probable as less and less number of possibilities would be available to the inner nodes then. Instead, it would be preferable to keep our position strong from the start and assign values where we are more confident i.e. first to neighboring territories. Now, relating it with what we just said, the 'neighboring' territories would, in essence, be the ones with smaller domains as compared to the far-off territories which have no nearby areas colored. Another way to think of is this - we are incrementally increasing the constraints on the domains if we follow this approach which decreases the possibility of failure at a later stage. On the other hand, assigning values to variables in an arbitrary order might cause us to work with very loose constraints in the beginning and then ultimately leave us with extremely tight constraints for the last few assignments. This would be a very unfavorable situation and hence following this approach of assigning values to variables with the smallest domains first (i.e. most constrained variables first) i.e. in some sense incrementally constraining the overall assignments is a much better approach.

2. Least Constraining Value (LCV) - The second ordering approach that would help reduce the mistakes and hence the number of backtracks is the least constraining value. Now, we have already determined which variable to assign a value to, using the above MRV approach so now we need to determine which is the best order of choosing values to assign to variables from their domain. The order is the least constraining value first. This means out of the given values that can be assigned to any variable, we choose that value to assign first, which causes the least amount of reduction in domains further. Consider the example of the map coloring problem, by LCV we mean that, say, for any variable we have options to choose from Red, Green, and Orange. If we pick red and other nodes are colored in such a manner that we end up greatly reducing the domains of the other nearby variables. So, instead, we pick that color (say green) which causes the least of such reductions. This helps us in 'keeping more options open' for the nodes to come and so in a way gives us a better chance to not make an incorrect assignment in the beginning. In the beginning, this approach may seem counter-intuitive to what we've been doing in the first ordering technique above i.e. MRV, but it is not! How? See, for any given CSP, the condition that we need to satisfy is that all the variables must be assigned some value and not the other way around. That is to say, that it is not necessary for every value in the domain to be assigned to some variable. Since that is not our goal, we can be conservative in assigning values to variables, and hence this LCV technique works towards preventing incorrect choices in the very beginning!

A little about Constraint Satisfaction vs. Optimization Problems - Even though at first these two classes of problems may seem quite different, delving a little deeper into their formulations reveals that they belong to the same complexity class! In general, if we intend to convert a constraint satisfaction problem to an optimization one, what we need is an optimization function. A straightforward objective function here would be the number of constraints un-satisfied and we simply have to maximize this function in the equivalent optimization problem. On the other hand, if we want to model an optimization problem into a constraint satisfaction problem, all we need to find is an equivalent constraint for the objective function! This means, that in addition to any other constraints of the problem, we add a new constraint that says that the objective function is greater than or less than a particular value (whatever be the problem). Then we would reduce/increase this value accordingly to get arbitrarily close to the actual optimum value of the objective function by solving the equivalent constraint satisfaction problem. We have looked at constraint satisfaction problems in this last section, so now let's look at a set of algorithms that come under the broad head of 'Local search' and are used to solve optimization problems.

Local Search -
So far we've looked at various search algorithms that keep track of the nodes where we've been and some that even look ahead and prevent disastrous steps! What we will look at now are some local search algorithms. These algorithms are known to scale really well in certain applications by providing very reasonable solutions for even infinite search spaces. But where do we apply this local search? Local search is generally applicable to the following kind of problem formulation - An optimization problem in which we have a set of states such that each of the states is a solution and we have an objective function describing any particular state. Let's describe how such problems look like in general. For an optimization problem, as the name suggests, we have to maximize or minimize the value of an objective function. For every state, the objective function returns some value and the goal is to reach such a state that has the most optimal (i.e. maximum or minimum) value. Local searches are very memory efficient as they do not concern themselves with any of the previous states and keep track of just the current state. This adds to the point mentioned above, that in vast search spaces where building entire search trees wouldn't be feasible, these searches are able to perform reasonably well! Another consequence is that we don't get the path but the solution state when we solve these problems (in general). Also, as the name suggests, we move only to nearby neighborhood states whenever we take a step in any direction in case of local searches. Consider the example of the N-Queens problem - an objective function, in this case, could be the number of pairs of queens that are attacking each other and our goal would be to minimize this objective function.

Trivial Algorithms for Local Search -

1. Random Sampling - In this algorithm, we simply come up with any one out of all the states in possible, randomly. We keep on repeating this process, every time coming up with a new state and checking whether the objective function is optimized (according to our requirement) or not.

2. Random Walk - In this algorithm, we come up with some state randomly (as above). Then, we look at all the neighbors of this state and we pick a neighbor randomly and step into that state. Then we keep repeating the same process of moving into a random neighbor state again.
 
What is the termination point? 
Depending upon the problem, we need to set certain expectations after which we terminate the above search processes. Note that both of the above algorithms are asymptotically complete. This means that if we were to take infinite steps as directed by these algorithms, we would end up at the solution which is understandable by the way they are formulated. However, for a random walk, we need to ensure another condition, the neighborhoods of all the states should be connected in the sense that there shouldn't be multiple connected components in the state space. All the states should at least be reachable from any other states via the intermediate neighborhoods.

Can we improve? YES. By a small improvement in the random walk approach mentioned above, we can at times reach to a solution deterministically faster. This improvement gives rise to a solution called the Hill Climbing or Greedy Local Search algorithm.

Hill Climbing Search or Greedy Local Search - 
The slight improvement to random walk being moving in the optimal direction. At every state, instead of stepping in a random direction, this algorithm directs movement in the direction that is favorable to the problem goal by looking at the objective function value of all the (immediate) neighbor states and making an informed decision. Here, the termination point is said to be the state, when no neighboring states have an objective function value better than the current state we are in.
Optimality: Clearly, this algorithm can get stuck at a local optimum as the neighborhood condition would be satisfied there and the algorithm would terminate. Hence, greedy local search is not asymptotically complete (unlike the previously discussed random sampling and random walk).

Escaping the local optima:
Depending upon the landscape of search, there could be multiple reasons why we don't get to the global optimum while following hill-climbing search. We could get stuck at a local optimum such that all the neighbors have a worse objective function value; we could get stuck at a state where the nearby states have the same value on one side and worse on the other i.e. a shoulder in the landscape. Following are some techniques used to escape such local optima situations.

1. Sideways Movement - One of the scenarios mentioned above can be escaped by allowing limited sideways movement. This means that we could set up a limit, say, 'c' steps are allowed to be taken in a direction where though the objective function value isn't better than the current state but it isn't worse either. This is done in the hope of escaping the 'horizontal' landscape and to be able to head in the direction of the global optimum if possible. An important point to note here is the limit because, without such a limit, we could get stuck in an infinite loop and the search would never terminate.

2. Tabu Search - Building on the idea of sideways movement, tabu search is just sideways movement + a small caveat of a "tabu list" added to it. This list is used to maintain the last k-states visited and a constraint is added stating that we won't visit any of the states in the tabu list when given a choice to take a step. This fixed-length queue is maintained as one might expect i.e. as we get to a k+1-th node, we remove the oldest node and add the next visited node. Notice that this technique would become completely structured graph search as the size of the tabu list is increased because we won't ever come back to the same node.

3. Enforced Hill Climbing - This idea includes the concept of structured-memory based search to a greater extent in the sense that we employ BFS here. The idea being that we make use of the simple hill-climbing algorithm but as soon as we see that we are stuck at a local optimum (by looking at the objective function values of neighbors), we start a breadth-first search until we find a point that is better than our current state. As soon as such a state is found, we jump to that state and again perform the whole process starting with more hill climbing till the next time we are stuck. The termination condition is not entirely explicit but we could use such searches for a limited time and output the best (i.e. with the best objective function value) state found so far. 

So far we've discussed deterministic algorithms improving upon the greedy local search but we wish to get completeness into the picture. This is done with the help of stochastic hill-climbing algorithms which we discuss next.

Stochastic Hill Climbing - Introducing some probability into how we pick the next state would help in claiming asymptotic completeness, so we could merge the algorithms we've studied so far:

1. Hill-climbing with random walk - We define a parameter 'p', such that at each step we take the greedy hill-climbing step with probability p or a random walk step with probability 1-p. This parameter 'p' could be fixed or we could improve this using some intuition as we would see shortly.

2. Hill-climbing with random restarts - In a similar manner as above, at any step, we either take the greedy step in the best-seeming direction or we choose a random state and jump to it. There could be a number of variations here, including how many times we do the 'restarts' and for every restart how long do we keep on going with the greedy steps or do we continue till we hit a local optimum. 

3. A combination of 1 and 2 - Finally, we could consider an approach, where depending upon some probability parameters, we decide to do any of the following three: take a greedy step, take a random step to a neighbor (random walk) or take a random step anywhere (random restart). Depending upon the variation of random restart we decide to work with, we might have to keep track of the "best state" so far so as to return it when the time allotted to the search runs out.

4. Simulated Annealing - This algorithm is a more advanced implementation of random walk + greedy search. The basic idea behind this algorithm is as follows - at any given step, we pick any neighbor of the current state randomly and find the objective function value at that state. If the objective function changes favorably, we jump to that state. If the objective function changes unfavorably, we take a look at the amount by which the function changes and the probability with which we take this unfavorable step is proportional to this difference amount. So, if the move is disastrous (i.e. a large unfavorable change), the probability of taking that step will be very small. Now, the overall process of taking steps is also controlled by a factor that is referred to as "temperature". Temperature is varied as a function of time with the connotation that at a higher temperature, we would take an unfavorable step with a higher probability than at a lower temperature. Therefore, in the beginning, as the temperature is high, we are more likely to make random moves but not in later stages of the search process. Also, a temperature of zero implies no motion at all and we return the current state without further steps.

Further improvement in this class of algorithms i.e. local search is done by the following two final algorithms -

1. Local Beam Search - This search method is based on the understanding that local search i.e. keeping just the current state in the memory at any given instant is something that has been adopted due to a very serious memory-constraint setback and is not completely sought-after. Instead, what we could do is keep 'k' states in the memory at any given time. How do we go about doing this? First, we select 'k' random states, then at every step we simply look at all the successors of these 'k' states. Among these successors, if the problem is a goal-based one and we find that the goal condition is met, we return that successor. If not, we simply pick the k best successors collectively from all the successors under consideration i.e. we could get multiple successors in the next step from a single parent state. But, as can be seen, this will lead to some trouble. The trouble being that we are picking the k best successors from the entire pool and it is possible that after a few iterations we would simply end up with searches happening over the same local hill (in the extreme case) or in a few local hills. This brings the idea of stochasticity to be implemented into this search which would prevent such an aggregation to a great extent most of the time. 

2. Stochastic local beam search - this is an improved version of the local beam search in which instead of directly picking the 'k' best next states, we sample k states from the entire pool of successors with higher probability given to the 'good' states. This would ensure to some extent that aggregation, as used to happen in local beam search, doesn't happen any more.

A biological analogy commonly used here is that of natural selection. At every iteration, there is a high probability of 'healthy' (or 'good' w.r.t. the objective function) states to be picked up. However, drawing an even closer analogy to the generation propagation routine, we can come up with the genetic algorithms that are proposed to work well as the stochasticity is improved.

3. Genetic Algorithms - These are a more advanced form of stochastic local beam search. Here, instead of simply sampling based on a probability distribution to introduce stochasticity we perform a certain set of steps to ensure to some extent propagation of 'healthy' states and creation of 'healthier' states by escaping the local optima at any stage. The general process followed is this - first, we represent a state as a string over some finite alphabet. We start by sampling randomly k states. Now, we make use of the 'fitness function' (which is just another name for interpreting objective function's value being 'better') to determine which of these states that will be given a higher probability while sampling from our set of k states. This sampling is being done to select which states should be 'parents' to the next generation. The fitness function involvement to some extent ensures that 'healthier' states propagate their features. We then cross over, at random points, these sampled states and create the next generation (some strings). This next generation may also have some random mutation in addition to being a random combination of the parents. This random cross over enables distant jumps and not just local steps.

Certain limitations of genetic algorithms - There are multiple parameters that need to be tuned to get the best performance out of this genetic algorithm. These include - number of parents to be sampled at every step, the distribution to be used to determine at what point crossover needs to happen, etc. In addition to this, there has been no empirical convincing evidence that such genetic algorithms would work better than the comparatively much simpler hill-climbing search with random restarts.

Having completed the discussion of local search algorithms in discrete space, let's briefly take a look at what shape, these take in the continuous space.

Gradient Descent - Local search algorithms in the continuous domain are widely being used under the title of gradient descent. Here, we might have some complex multi-dimensional function to optimize over a set of continuous variables, and to do that, we use gradient descent which is, to put it simply, a type of local search in which the objective function is optimized by going in the direction of maximum descent/ascent. This direction is made available with the help of partial derivatives w.r.t. the variables. The rate at which this movement takes place is determined to some extent as a hyperparameter which can be selected in multiple ways or can be made to vary according to the problem requirement or expected landscape of optimization. E.g. we could keep on increasing the rate until the function is being optimized or we could take very large steps initially and then decrease the step size as we move along the optimization process. 
-------------------------END-------------------------------------------

References: Class discussion of CS323 - Artificial Intelligence.
The slides & video lectures & hand-outs of CS188 at UC Berkeley (Fall '18) (Source: Course Website)
Video lectures of Introduction to Artificial Intelligence by Prof. Mausam. (Source: NPTEL)
Declaration: I have not taken any help from any other source(s). The above notes are based on my understanding of the content.

--------------------------------------------------------------------------