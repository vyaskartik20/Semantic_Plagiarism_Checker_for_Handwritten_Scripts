B18CSE014
Ekagra Das

Resources Used :- Only Video lectures with Timestamps at relevant places.

///Lecture 09-1///
Doubts regarding Notes submission were addressed in this lecture.

///Lecture 09-2///
[00:40]
Recap of 8 Puzzle-1
In cost, whatever approximations is being made , they are going to be somewhat less, there will always be a lower bound to what is actually being done.
First we come up with a relaxed version, of this particular problem. Here we are relaxing in terms of the moment of the square blocks, and define our costs, which are exact for this relaxed problem. These cost denote a lower bound on how much cost, we may get while solving the actual non relaxed problem. Aslong as it is a lower bound in relaxation, the requirement for the Heuristic function to be admissible will be satisfiable. As we try to come up with even better Heuristic, it will require more computations, and the search space will reduce, but it will require more work in computing them, hence a trade off occurs.
As we increase the depth of how far, the start state is from the goal, the difference between the steps in UCS(higher) vs Heuristic function(lower), increases drastically.

[07:40]
8 PUZZLE-2
Any tile could slide in any direction in the previous puzzle whilst ignoring other tiles position.
Here we will take the Manhattan Distance as our Heuristic Function, ie take the summation of distance of all tiles from current vs goal state.
Comparing it to previous Heuristic, we will be performing more computations, but will get a better approx.
In previous puzzle relaxed version, we could simply move one tile from one place to another counting as 1 step, In this relaxed version we arent allowed to move directly, but only walk to goal whilst ignoring other tiles(hence more computations, but better approx towards cost).
In TILES(higher) vs MANHATTAN(lower), as steps increase difference increases between these relaxed versions, so both are admissible, but there is a tradeoff between them.

[19:16]
8 PUZZLE-3
Actual cost used as heuristic function.
It will be admissible, because the approx cost never exceeds the true cost.
The values that Heuristic function returns is the actual cost ie F=G+H, and H=true forward cost. In computation, it may not help us, but when it comes to the nodes that we expand, it will get us with the least steps, thereby giving us the optimal solution. We will go in a single path, not exploring anything.
The thing wrong with this is that there is too much computation, which may not always be feasible.
In A*, as the heuristic is closer to real cost, we will expand less nodes but do more work per node for computation of the heuristic itself.

[26:18]
SEMI-LATTICE OF HEURISTICS
Trivial Heuristics, Dominance
Zero is always an admissible heuristic, we can always use it to become equivalent to UCS.
Consider Ha and Hc heuristics.
Ha dominates Hc if for all n: Ha(n)>=Hc(n)
If Ha is admissible, Hc is also admissible, as its a lower bound of Ha, and Ha is a lower bound of true cost.
Also we may not be able to compare two heuristics like Hc and Hb, however we can see some ordering between them.

///Lecture 10-1///
[00:40]
GRAPH SEARCH (State space graph)
In tree search, it happened that we did redundant work, by revisiting a node and start searching from it, so to avoid that, we do state graph search.
In BFS, we can see that several nodes are repeating, so we may say that since we have already seen this node, we won't further expand that subtree. So simple solution is to keep a track of all the nodes visited in past. To implement it we basically create a closed set, where once after we expand a node, we put it in that closed set. And before expanding a node, we just check if its already in the closed set or not, if it is , we don't expand it.
Note - Store the closed set as a set, and not as a list. (Searches in set is quicker)
Since we are ensuring each node is expanded, we can ensure that we will get a solution for completeness.(may or may not be optimal)
Optimality ?
In UCS we are getting the optimal solution for the example, but A* is not, here in this graph search, because we are not searching the full tree by putting constraint in terms of what nodes we will be expanding. Also remember that UCS may end up searching the full tree in the worst case.

[18:40] 
CONSISTENCY of Heuristics
In Admissibility, Heuristic Costs <= Actual Costs to goal
In Consistency, we take into consideration the arc costs, ie Heuristic arc cost <= actual cost for each arc
Here an example A,h=4 ----(1)-----> C,h=1
(h(A)-h(C)=3) <= (1=cost(A to C)), which is false, hence its not consistent. If A,h=2, it would have satisfied the criteria.
Consequences of Consistency -
When we traverse the path, the total value ie G+H will never decrease.
And A* graph search will be optimal.

///Lecture 10-2///
[08:30]
OPTIMALITY OF A* GRAPH SEARCH
In case of tree search, we have seen that the nodes that we expand, are prioritized based on the F value(expanding in increasing value of F).
In case of graph search, we can say that whatever node we expand, if there is another path to reach that node, of less cost, then we will expand that other path first. Basically the path which has less F value, will be taken out of the fringe, before the other path. 
This way we can ensure that we can always get the optimal solution with A* graph search given that the Heuristic function is consistent.

[12:30]
Optimality SUMMARY -
In case of Tree Search - A* optimal if heuristic admissible, and UCS is a special case where h=0.
In case of Graph Search - A* optimal if the heuristic is consistent and UCS optimal as h=0 is consistent.
[Consistency -> Admissibility]
Most of the times a natural admissible heuristic, turns out to be consistent if its a relaxed problem.
[15:10]
A* Summary -
Both backward costs and estimate of forward costs.
Optimal when admissible or consistent.
[16:43]
Graph Search PSEUDO CODE -
Maintain a closed set in addition to fringe, steps all similar to tree search, except whenever we take out a node from the fringe, we first check if it is there or not in the closed set. If it is not there, we add it and expand it, to avoid any redundant searches.

///Lecture 11-1///
[10:50]
Constraint Satisfaction Problems -
Assumptions - Single Agent, Deterministic Actions, Fully Observed State, Discrete State Space
Planning Task - 
Interested in identifying the path to the goal whilst minding the costs and depth of the path. And in IS algo, we make use of Heuristic functions which gives us estimate of how far we are from goal state.
Identification Task - 
Here we dont care about what path we choose and we may have multiple goals here. Depending on the algorithm being used, all paths maybe at the same depth, but how we reach maybe different. And Constraint Satisfaction Problems fall under this category of Identification task.

[17:11]
Constraint Satisfaction Problem (CSPs) -
In Standard Search Problems, we saw that we define a state and it can be coded using data structures(depends on what we are storing). And one of the states is a Goal state/test. Successor function will depend on what algo we will be using.
In CSPs, we are given a set of variables that defines our state. These variables can take values from a fixed domain and each variable may have different domain. In the end we would want to assign values to each of these variables from their respective domains so that the given set of constraints get satisfied for all of them. Eg- Map coloring problem.
Its kind of a representation language. We will see various algos and depending on various task we'll see which can work where.

[20:55]
CSP Examples - Map of Australia.
We assign a color to each state such that no two neighbouring states are of the same color. (Given a set of colors to pick from)
Variables are WA, NT, Q, NSW, V, SA, T (states).
Domains are Red,Green,Blue (colors)
Constraint is - 
No two neighbouring states of same color. (Implicit)
Neighbouring pair colors belongs to (Red,Green),(Red,Blue),(Green,Blue) (Explicit)
Solution - Some assignment of these variables such that all constraints are satisfied.
CSP Examples - N-Queens
NxN chess board, we have to place N queens such that no two queens are attacking each other.
Formulation 1:
Variables - (location on chessboard) - Xij
Domains - {0,1} 0 meaning that there is not a queen in that location and 1 meaning there is a queen on that location.
Constraints -
For all i,j,k
Xij,Xik belongs to {(0,0),(0,1),(1,0)}
Xij,Xkj belongs to {(0,0),(0,1),(1,0)}
Xij,Xi+k,j+k belongs to {(0,0),(0,1),(1,0)}
Xij,Xi+k,j-k belongs to {(0,0),(0,1),(1,0)} 
Also Summation of Xij = N

///Lecture 11-2///
[02:27]
CSP Examples - N-Queens
Formulation 2:
Variables - Qk(row), 1 for each queen
Domains - 1,2,3,...N
Constraints -
(Implicit) For all i,j Non-threatening positions (Qi,Qj)
(Explicit) Q1,Q2 belongs to (1,3),(1,4) ...

[06:50]
Constraint Graphs -
Mapping problem of Australia -
Denoting each state as a node, and edge represents that they are neighbouring states. So for each constraint, we have a corresponding edge in the graph. Also we can note that state T is independent as it didnt have any neighbours, hence no edges. 
In this case, since each constraint is between a pair of states, it is called a Binary Constraint Graph. And the corresponding CSP is called Binary CSP, and each constraint contains atmost 2 variables.

[09:50]
Example of N-Queens - 
Each variable(queen) from A .... E, can take values from 1 ... 5.
So we plot a graph with nodes representing A .... E, with the edge in between representing the constraint that the Queen on the edge can belong to either of the two connected nodes. 
Example - Cryptarithmetic
	 TWO
	+TWO
	----
	FOUR
Variables - T W O F U R X1 X2 X3
Domains - 0,1,2,...,9
Constraints - 
AllDifferentVariables(F,T,U,W,R,O)
O+O=R+10*X1
Assuming the calculation in Base 10 and + denoted standard addition.
To represent this using a constraint graph.
Unlike previous example, here there are more than 2 variables in a single constraint and we would like to connect all of them to denote this.
Consider O R and X1, we cant have a single edge connecting three nodes, so we come up with a auxillary node and connect all three of them to it to denote that its a single constraint containing three variables. So this becomes our constraint graph for the given problem with constraints that contain two or more variables. These are also called Factor graphs in some context.

[29:50]
Example - Sudoku
Variables - 81 variables(each square)
Domain - 1,2,...,9
Constraint - 9 way all difference for each column and row and region OR We can have many pairwise inequality constraints.

///Lecture 12-1///
[01:45]
Example - The Waltz Algorithm
CSP in doing interpretation of a geometric object
Assume there is a robot looking at an image C and based on the image that it sees, it comes up with a line diagram of it. Based on this, the objective is to interpret the geometry of the object. We then have to see which intersection is an outwards or inwards and so on. Such restrictions become part of our constraints for the CSP and based on these variables and constraints, our objective is to come up with different possible 3D shapes or interpretations.
This is one of the first example of using CSP for solving a practically appealing task.

[06:55]
VARIETIES of CSPs -
We have seen Binary Constraints where there are atmost 2 variables, then Constraints where there can be any number of variables.
There can be broadly 2 types of CSPs in terms of type of variables -
Discrete Variables - 
Here we can have both finite as well as infinite domains.
Eg- We consider a fixed set - finite domain
Eg- Any possible integer/string - infinite domain
Discrete optimization is somewhat difficult compared to continuous optimization.
Continuous Variables -
Eg- Timestamp
Eg- If there are linear constraints, of sort of ax+by+c, then these are solved quickly as there are standard methods for this.

[09:22]
VARIETIES of Constraints -
There are atleast 3 types.
Unary Constraints - When a variable cannot take a particular value from the domain and this becomes equivalent to reducing the domain for that particular variable.
Binary Constraints - 2 variables
Higher order constraints - More than 2 variables
We have discussed that by solving CSPs, our objective is to get to the goal and the path is not of our concern. And usually there are multiple solutions available of a task.
In the Map coloring problem, we can come up with multiple assignments.
It may happen that among the various solutions possible, we may be looking for certain types of solutions. For eg while coloring the map, one color maybe costlier than the other, and we might want the cheapest way. In such cases we may give more preference to the cheaper color.
So that becomes the constraint satisfaction aswell as constraint optimization.

[14:14]
REAL WORLD CSPs -
When looking at CSPs we will consider only constraint-satisfaction task for now.
Some real world examples are course allocation/time table task, hardware configuration(limited space on board/circuit to maximize some value, and satisfy constraints), transportation scheduling(buses/trains/flights), factory scheduling etc.

[17:20]
SOLVING CSPs -
Standard Search Formulation -
We will pose CSPs as a search task.
Initially, we are given an empty assignment(empty state), where none of the variables are assigned any value from the domain.
As we move in our search space, we continue to assign values to variables that are unassigned and in the end, we will have some assignment of all the variables and if that satisfies our constraints, it will be our solution. Here solution is the goal, not the path. So the goal test is- assignment should be complete(none should be unassigned), and it should satisfy all the constraints.

[BFS] in Map Coloring -
Start State - All variables are unassigned
Next level - Assign one value to each of the 7 variables(states), also there may be multiple assignments, so around 21 children of the given start state. (branching factor is 21)
Then in next step, we pick 1 of the branches, then assign, then again go to the next level. Since the branching factor is huge, the tree will grow alot. And the depth of this tree will be the number of variables. By the time we reach a solution(maybe not acceptable) we would've grown a big tree.
So the disadvantage here is that we have to build the whole tree and we know that the solution is at the last level. In Uninformed search, we get the solution before hitting the last level, however here its not happening.

[24:40]
[DFS] in Map Coloring -
We start with empty assignment, then at next level, we assign 1 color to the unassigned variable. Now we wont fill all the nodes that would be at that level, but instead goto next level and assign a color to an unassigned variable. In DFS we can get 1 solution in 7 steps which is advantageous. 
So we will keep DFS as a baseline algo for solving CSPs and see how to improve upon it, to make it significantly fast.

///Lecture 12-2///
[16:20]
Backtracking Search - (Simple modifications in DFS)
Its an uninformed search algorithm because the base algo is DFS and we are not making using of any extra information about the goal other than the constraints.
First modification - Assign an order to the variables. 
Consider WA and NT, we can see that ordering doesn't matter. So we fix one order and based on that we can pick variables at each step.
Second modification - We will not check constraints when we reach complete assignment, but we do it at every assignment. If it turns invalid we stop at that point and move back.
With these modifications, we can solve N-Queens problem for bigger N, and in reasonable time.

[20:55]
Backtracking Example -
We start with an empty assignment, now that we have ordered the variables.
Choose first variable, then at level 1, we assign values to that variable, and instantly check whether we are violating any constraints.
Now at next level, choose second variable, assign values and check if constraints being violated. And we keep on going following the pseudo code.

[24:28]
Backtracking Search Pseudocode -
Initially we are given the unassigned variables, domains and constraints. We order the variables.
First step, we select the first unassigned variable based on the order. Then we also order the values in domain.
Then we pick each of those values in that order and assign that value to the variable that we chose here. We check if its satisfying constraints. If yes, then we continue with the next variable, otherwise we stop and go back to the previous assignment.

///Lecture 13-1///
[01:38]
Improving Backtracking -
We saw that DFS was inefficient(getting a complete assignment and then checking constraint satisfaction), because of which alot of useless work was done. So we modified it by assigning one variable at a time, and checked constraint satisfaction at each assignment.
Further improvements - Order variables in a way to minimize backtracking, and also try to order values in the available domain for each variable.
Filtering - Trying to omit possibilities that arent going to be useful.(Stop in advance)
Another improvement is using the structure of the problem, by using the constraint graph seen earlier.

[06:00]
FILTERING - Forward Checking
In filtering, we keep track of the unassigned variables, and for each of those variables, we continuously shrink the domain, ie consider only those options that wont lead to failure for sure.
Example-
We red to WA, then remove red from available domain of both NT and SA.
Next step we choose Q and assign green to it, then remove green from domain of both NSW and NT and SA.
Then we pick V, and assign blue, then remove blue from domain of SA and NSW.
Now we see that SA doesn't have any value available for it in the domain. So we stop at this point and backtrack.

[21:38]
Filtering - Constraint PROPAGATION
In this we can see that, we are going to backtrack even before we get a node that has empty domain, basically based on the available domain, we say that some of the constraints will become invalid in future.
We know that NT and SA can't both be blue, so we basically reason from one constraint to another.

///Lecture 13-2///
Queries regarding assignment/notes/project were addressed.

///Lecture 14-1///
[1:22]
In the constraint graph, there would be an edge between WA and NT. So we need to consider such an arc between the neighbours. The statement says that for every X in the tail(NT Domain) there is some Y in the head which could be assigned without violating the constraint. So there are 3 values available in domain of NT and 1 value in the domain of WA. So if we assign blue to NT, we are not violating any constraint. Now if we assign green to NT, again no constraint violated. However if we assign red, constraint will be violated.
So we remove red from domain of NT to make the arc consistent.
Now consider WA and Q, here we can assign any of these colors to Q, we wont violate any constraint, so this arc is consistent.
So effectively, we are removing values from the domains of the tail nodes which would violate any of the constraints, so that in the future, we dont get any constraint violations.
[06:00]
Forward checking will not tell us whether a constraint will not tell us that a constraint will get violated in the future, but if we check for consistency of all the arcs, we will see that there is an arc that is not consistent. Consider the arc from NT to SA, we can only assign blue, but not to NT, hence its an inconsistent arc, which will violate a constraint in the future, hence we backtrack.
The arc V to NSW is consistent, as any value can be assigned without violating a constraint in the future. However NSW to SA is inconsistent as if we assign blue to NSW, then SA can't use it and violate it.
[13:40]
So finally Arc consistency will help us detect any failures, before forward checking does. And the downside of it is, that it leads to alot of computation.
Runtime Complexity is O(n^2d^2)

[19:00]
After Arc Consistency is enforced -
We can either have one solution left, or multiple solutions left. And no solutions left when there are constraints that are controlling multiple nodes.
[22:30]
Ordering - 
Minimum Remaining Values (MRV)(Variable ordering) - 
Whenever we are going to assign, we choose the variable whose domain is the smallest. We choose the node the min over the max so that in future, we dont face constraint violations as that min node may get a 0 size domain. And this min variable is also called the most constrained variable.
Least Constraining Value (Value ordering) -
We pick a value such that the domain of other variables shrink by the minimum amount. Reason for least than the most is same as above.

///Lecture 14-2///
[00:20]
LOCAL SEARCH AND OPTIMIZATION 
Re-look at N Queens problem - 
We based this as a constraint satisfaction problem, interested in solution and path taken was unimportant. Posing this as a search task, we found that it is possible to come up with a path that would take us to a valid solution.
In case of Local Search, path may not be unimportant, but where it takes us to is important.
In context of CSPs we discussed constraint satisfaction and constraint optimization. Eg in Map coloring, if we assigned different cost to each color, we would want to use cheap colors more.
In many problems, it is possible to map a goal satisfaction problem into an optimization problem.

The search methods uptil now were simulated by the agents, which required alot of memory. But in local search, since Agent is not going to simulate, it needs to keep track of just the current state. That way the memory required is low. And similar to earlier methods, we wont care about path.
Advantage here is, we can find solutions even when search space is huge.
We can pose local search problems as optimization problems. Just like Heuristic function, we can make a function called Objective function, whose value should be max or min depending on problem.

[10:30]
TRIVIAL Algorithms -
If we randomly generate, we can hope that we can pick the best state, ie the state which will have the maximum or min value wrt the objective value and the algo will return it.
In Random Walk - we start from a state, and randomly move to a neighbouring state, and at some point we hope that we reach the state with the best value for the objective funciton.

[12:30]
HILL CLIUMBING (GREEDY LOCAL SEARCH) - max version
It will return a state that is a local maximum. Maximum would be the state with the max value of the objective function.
Given the variables/nodes/states, we start with the state, then look at the neighbouring state, and if any of these neighbouring states has a value higher than the current state, then we goto that next state and repeat, else we say that the current state is best locally.
If we want to minimize, we just reverse the inequality. 
This is analogous to climbing a mountain in a thick fog with amnesia, mountain cause its always an upward direction, and thick fog because we cant see beyond the neighbouring states, and with amnesia because we arent remembering anything other than the current state.

[21:00]
Example - N Queens 
Earlier we saw that its a constraint satisfaction problem.
Now lets pose it as a constraint optimization problem - For each config, how many pairs are attacking, and we have to minimize this value.

[24:10]
State Space here?
Each state corresponds to some configuration of all the N Queens.
The successor function would be defined in a way, to move a single Queen to another square in the same column. Restricting queens to move only in a vertical direction or along the columns to reduce randomness. And objective function is number of pairs of queens attacking each other.

///Lecture 15-1///
Hill Climbing on 8 queens is good as for a state space of 17 million states, it takes only 4 steps on average to succeed and 3 on average to get stuck.
[04:45]
Hill Climbing Drawbacks -
The solution that we reach depends on start state, and most of th time we get stuck in local optima. In plateaus, the value of objective function wont increase for quite long and we get stuck for several steps. Sometimes we have to make a step in diagonal directions, then this successor function is not possible.

[07:44]
Escaping Shoulders : Sideways Move
Sideways move help us in coming out of plateaus. This increases the chances of solving of 8 queens problem from 14% to 94%, however the number of steps increases significantly.

[15:00]
Stochastic Hill Climbing -
Since standard hill climbing gets stuck in local optima, to avoid that we introduce randomness in the approach, by introducing random walk, and random restarts. This ensures completeness.
Hill Climbing with random restarts is the best possible local search algorithm.
HC with Random Walk - In greedy, we choose the neighbour with larger value, whereas in random, we move to a random neighbour.
HC with both Random walk and Random restart - We start from a state and identify the best neighbouring state, we choose it with a probability p and a random neighbour with probability 1-p. We continue to do this and eventually stop at some point. In next iteration, we randomly start from some point and repeat the provess. They are individually asymptotically complete. Their combination will also take us to a complete solution. 

///Lecture 15-2///
[16:00]
Simulated Annealing -
Idea here is, suppose we are standing on a hilly surface and would want to goto a minimum point. We can drop a ball from the maxima which will fall to the local minima, then it may get a jump, and eventually settle at the global minima.
Its also related to the physical process of annealing. When we heat some metal, the particles gain some energy, and as the temperature goes down, the randomness in the particles decreases, and eventually settle in a stable state. 
Algorithmically its similar to hill climbing, but we dont always take the best move.
We are allowed to goto the state which has a higher objective function value, when we want lower value. Initially its more probable to get stuck in local minimas, so the amount of shake, we provide to this rolling agent is more. Gradually, as it goes down the surface, we can reduce the amount of shake (temp goes down).
Advantage of this algo is that it is guaranteed to take us to the global minima. However the disadvantage is that in practice it becomes very slow.
Examples where this is used - 
VLSI Layout - arrange some circuit on a board in an optimal manner, since its costly as once we fix the arrangement on the board, we produce thousands of lakhs of such boards.

[25:00]
Local Beam Search - Variant of Hill Climbing
If we are allowed to make use of some memory, we can start from K randomly selected states, then identify their successors.
If lets say 1 state has 4 successors, now out of these 4, it is possible that more than 1 successor has a better objective function value than current state. So for each of those K states, we identify the successors, and among those successors, there are more than K successors whose values are increasing. But we pick only the K best ones.
Again we can introduce randomness here, like instead of picking K best successors, we can choose them probablistically, by being biased towards the better ones.

///Lecture 16-1///
[03:30]
Note - We do not run K searches in parallel, because then each search would've run differently and in the best case we would get K different peaks. In this case, we mix all the successors at each step and then pick from them.
Suppose we consider each of these K states as an organism. So we have a population of K organisms, at a given timestep, each organism can produce few more organisms, and out of all those organisms, the ones which are fit survive, and the rest die.(Amoeba)

[06:50]
Genetic Algorithms -
Closely related to how genetics of humans work.
We assume that there are a pair of states, called as parents, and from them we get one or more successors. And with each state, we associate a fitness function, which tells us how probable a state is to survive. The more the fitness, the better chances of survival.
We also have additional notions, which are random selections, crossover and random mutation. Say we have a population of K organisms/states, and based on their fitness, we can randomly select pairs and then we take characteristics from each organism in that pair, and come up with a successor. Then there is some random change in the characteristic of individual successors

[09:50]
8-Queens using Genetic Algo-
Fitness function is the number of pairs of queens not attacking each other.
[22:37]
We see that the motivation comes from Stochastic Beam Search algo, and the advantages here are - from a given state, we can explore and reach those states which are not in the neighbourhood.
These algos are motived from biology. The terms neural and genetic are metaphors.
The disadvantage is that there are lots of points of uncertainities, because of which it becomes difficult to get the same results when we run an experiment multiple times. (reproducibility during research is difficult)

[26:47]
Travelling Salesman Problem -
Assign an index to each of the cities.
We have to decide a fitness function, then a crossover criteria, then how to perform the mutation.
Initially to start, we come up with a set of K random states. Each state tells us the order in which these cities are visited.
If we have to perform a mutation, we cannot simply pick an index and check it, because our objective is to visit all the cities. So instead of taking 1 index, we take 2 indices and swap them only when the total distance we cover reduces.
Something non random, is iterate all the pairs, and swap the best pair.

///Lecture 16-2///
[08:33]
TSP Continued-
Greedy Crossover - Out of the two parents, we randomly select one parent, take the first city from that parent, then we identify what are the cities that can be reached from the first chosen city in a single pop. Then we pick the closest one. It may not provide optimal, but one of the feasible ones.
While choosing the second city, if one of them is already visited, simply skip it and choose the second one. If both have been covered, then we randomly pick another city and repeat the process. 

[10:30]
The Advantage in Genetic Algo - Can explore the neighbours that are not close to a given state(cause of Crossover - jumps bigger than 1 hop)
Disadvantages - Alot of randomness involved, so reproducibility is tricky.
Initially, our objective is to evolve the population, such that after every iteration, the overall fitness of the population should increase. But instead of considering the fitness of each individual seperately, the next generation that we get will have characteristics of that individual, which is not only fit, but also mixes with other individuals more. 
 
[15:26]
When we discussed Hill climbing, it was for discrete tasks.
Gradient descent is similar to Hill Climbing, but on a continuous base.
The procedure is, we have a function of N variables. And this function is differentiable, then its a continuous function. Its derivative will tell us that if we compute the partial derivative of this wrt each of the variables, it will tell us the direction of the tangent. And based on the direction of tangent, we can decide in which direction to move. Then repeat
So if we are to minimize the objective function, we can move in the downward direction of the tangent. So this equation will correspond to the gradient descent. 
In case of continuous functions, remember that one requirement is to decide how big our step is. 
How to choose Lambda? - Start with a small value and keep on increasing it aslong as we see that the function value is decreasing, or start with a large value then decrease it successively. Then the motivation comes from the Simulated Annealing method, because there we saw that initially, it maybe useful to make bigger steps, so that we dont get stuck in local minimas. So initially, if Lambda is high, we make bigger moves. And gradually if we reduce it, as we move, the size of the steps will go down and we reach a good local minima atleast.

[29:05]
Newton Raphson Method -
Here instead of using first derivative, we use both first and second derivative, because it can approximate a polynomial/quadratic surface at each point for better information about each direction we move. However its not suitable cause of high computation, and X in high dimensional space.
























