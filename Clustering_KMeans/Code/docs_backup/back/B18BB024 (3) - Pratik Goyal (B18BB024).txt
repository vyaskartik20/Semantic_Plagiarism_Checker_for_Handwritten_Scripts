In the mid-1950s John McCarthy, the father of AI coined the term “Artificial Intelligence” and he defined it as “the science and engineering of making intelligent machines” [1]. Today even after 70 years the fundamental meaning has not changed a bit and we define it as “the study and design of intelligent/rational agents”. Rational agent here refers to the entities (agents) which can perceive an environment and can select an action (from a set of possible options) which could maximize its chances of success [2]. In a nutshell, AI is the science of making machines which can think and act rationally. 
AI has been used in a variety of fields. AI-based machines have beaten humans in chess (deep blue) and jeopardy (watson). It is widely used in self driving cars, speech recognition, image recognition, Natural language processing, and in the field of robotics. Deep blue was an AI based chess playing computer and about which Drew McDermott once said: “Saying Deep Blue doesn’t really think about chess is like saying an airplane doesn’t really fly because it doesn’t flap its wings” [6]. 
These intelligent agents can be categorised into two types viz: reflex agents and planning agents. Reflex agents choose the best action based on their current percept. They ignore the future consequences of their actions. Hence, they are generally not expected to be rational. Still, they may act rationally for some environments (just by luck) e.g.: There is only one action available to the agent for every transition, in this case the reflex agent will act rationally. On the other hand, planning agents plan their actions, before proceeding to the goal i.e. they come up with a sequence of actions needed to achieve the desired goal [4]. To do so, they must have a model that could determine the consequences of their actions. i.e. a model which could output the future possible actions corresponding to a current action.
Let’s define some of the key terms used in AI viz: state space, start state, goal state, successor function, solution. A state space is nothing but the set of snapshots of all possible scenarios our environment could be in. Start state is that snapshot from where our agent will start its journey and the goal state is the final state that our agent wants to achieve. A successor function outputs a set of states reachable from a chosen state. Generally, there is a cost function which calculates the cost of every transition. Solution is the sequence of actions which the agent took in order to transition from start space to the goal state. A solution could be either complete or optimal. In complete solution we are only concerned about finding the goal while in optimal solution we want to find our goal in the best possible way. E.g. finding the solution with the least cost.
As described earlier, our intelligent agents will select the most optimal action from a given set of actions. To do so, it first needs to search (among all the actions) and find that optimal action. Formally, Search is the process of navigating from a start state to the goal state by transitioning through intermediate states [3]. To plan our path, we first need a well-organized structure representing state transitions. We use state space graphs and search trees for that. State space graph is a representation of a search problem where all the states our environment could be in, are represented as nodes and all the nodes are unique. Each transition is represented as arcs between the current state and the future state (after the action). The arcs may have a cost associated with it. Generally, a state space graph could be very big (in terms of memory) and are rarely fully build. Search tree follows a tree data structure where the start state is the root node and successors are represented as children. We can see that in search tree, nodes might not be unique. Further, we don’t generally construct a full tree as the size could be very big and we don’t really need to do that. Most of the times we construct a partial tree and expand it, till we reach the goal. 
We now have our search tree and we need to search and find our goal state. We need to keep this in mind that we should expand as few nodes as possible, any unnecessary expansion will just increase the time and space taken. The fundamental strategy is simple. We will maintain a fringe of partial plans (initialized with the root node i.e. start state) and then choose a plan from the fringe (according to some sets of rules) if the fringe is empty, we will return failure. Else if the leaf node of our partial plan contains the goal state, we will output the plan and return. Otherwise, we will expand the leaf node and add the resulting nodes/plans in the fringe. Note here that after choosing a plan we are removing it from the fringe. There are many search algorithms and they all follow this fundamental idea. However, they are differentiated by the rules they follow while choosing a plan from the fringe.
Depth first search (DFS) is a search algorithm which follows LIFO stack implementation of fringe. As the implementation follows LIFO rule, we will be expanding some left prefix of the tree. And only after exploring a whole left prefix, we will go to the next subsequent one. In the worst case, our goal could be at the rightmost bottom and then we will need to explore the whole tree to get to the goal. If b is the branching factor and m is the maximum depth. It may take O(b^m) time and the space complexity would be O(b*m). DFS guarantees a complete solution (given that it exists and cases like infinite cycles are handled). However, the solution might not be an optimal one as the algorithm will give the leftmost solution ignoring the cost. One benefit of DFS is that it is space efficient.
Breadth first search (BFS) is a search algorithm which follows FIFO queue implementation of fringe. Because of queue implementation it expands the shallowest nodes first and then goes to the next deeper levels. Let’s say the branching factor is b, the maximum depth is m and we are at some level s. And we define the cost as the depth from the root node. Then there would be b^s nodes at that level (s). Therefore, our fringe will need to store O(b^s) nodes, and searching among them will also take O(b^s) time. So, one thing is clear that BFS is not memory efficient. Just like DFS if a solution exists BFS will find it, but the catch here is that the above found solution would be the optimal one (least depth) as FIFO rule forces our algorithm to find the shallowest solution. 
We can conclude that if the goal state is somewhere deep down in the left part of the tree DFS will perform better but if the goal state resides in the right part and in a shallow region BFS will outperform DFS.
Let’s assume a case where we know that our goal state is somewhere in the right part and in a shallow region. Using DFS here won’t be a good idea as it will first explore the left prefix of the tree but we also can not use BFS because we are bounded by memory constraints. Therefore, we need to find a way so that we could take advantage of memory efficiency of DFS and FIFO implementation of BFS. It is done using iterative deepening. We basically run DFS for a certain depth limit (assume 1). If we do not find a solution, we increase the depth limit by 1 and then run DFS for this new depth limit (assume 2, 3, 4, …). We repeat this process till we find a solution. Basically, we are applying DFS iteratively under the constraints of depth limit and each time penetrating an extra level. This will take more time in comparison to a normal BFS but it will give better memory efficiency.
BFS can only find an optimal solution if the cost is defined in terms of depth. Otherwise it does not guarantee an optimal solution (least cost path). We can say that BFS or DFS are not cost sensitive. Uniform cost search (UCS) is a cost sensitive algorithm in which fringe is implemented as priority queue where priority is defined as the cumulative cost. And every time we choose the least cost state and expand it. This cost sensitive strategy leads us to the least cost solution. The solution will be not only complete (given that all costs are positive and finite) but also an optimal one. Let’s assume our solution costs C* and the arcs cost at least e, then the effective depth would be around C*/e. If our branching factor is b then it will take O(b^(C*/e)) time and the memory taken would also be the same, i.e. O(b^(C*/e)). The time and space complexities look familiar to BFS. Further, we could improve the efficiency of UCS if we could provide it a sense of direction as in general it is exploring options in every direction and it is blind to the goal. 
Note here that UCS follows a priority queue fringe rule. If we define this priority appropriately, UCS can easily be converted into DFS or BFS. We are using queue or stack just to get rid of log(n) overhead as insertion and deletion in priority queue takes log(n) time. Further, all of them are uninformed search, i.e. they are blind to the goal state while searching. They just know what the goal state is but apart from that they know nothing.
Informed search has some knowledge about the goal state e.g.: how far they are from the goal state. To incorporate additional information regarding our goal state we define a heuristic function. This function estimates how close a state is to the goal. For example, we can take Manhattan distance or Euclidean distance as the heuristics for Pacman problem or city and the shortest path kind of problem. For pancake problem heuristic could be the number of the largest pancake out of place. These heuristic functions are particularly designed for a problem. Meaning, a proper heuristic for a problem might not be a good heuristic for another problem. 
So now we have a function which could potentially guide us towards the goal. We are not blind and directionless. A simple method to find the solution would be using a greedy search. A greedy search algorithm will greedily choose and expand states which are closest to the goal state thinking the closest states will most likely take it to the goal. But, in most cases this greedy approach will take us to the wrong goal and if turning back is not allowed, we will be stuck. For example, lets suppose A is our goal state and B is the closest to A. Now assume greedy approach takes us to B thinking B is the best choice. But there is no path connecting A and B. In this case we will be stuck at B if going back is not allowed. Even if we are not stuck a worst case could be like a badly guided DFS in which we may end up exploring all the paths. Therefore, greedy search is neither complete (going back is not allowed otherwise it will be complete) nor optimal. 
Greedy search is bad and it is not even complete but one best thing about greedy is that it is not blind. UCS has everything but the sense of direction. If we combine both of them then we can have a UCS which is not directionless. This combined algorithm is A* search. It has a path cost or backward cost g(n) borrowed from UCS. And a goal proximity or forward cost h(n) taken from greedy search. 
So, the strategy of A* is the same as UCS. But the priority definition would change to f(n) = g(n) + h(n). One thing to note here is that we should terminate and come out with a plan only when we dequeue a goal not while enqueueing a goal. Otherwise we may miss some better possible plans. This A* solution seems optimal but when actual bad goal cost is less than estimated good goal cost it will not be optimal. In order to avoid that case, we need to ensure that the estimates are always less than actual costs.
We cannot play with cost function. The only thing in our hand is that we can change the heuristics (f(n) = g(n) + h(n)) and choose a better function which satisfies the above criteria. We call it admissible heuristics. Formally defining if our heuristic function follows 0 <= h(n) <= h*(n), where, h*(n) is the true cost to a nearest goal, then it would be an admissible heuristic. The problem with inadmissible (pessimistic) heuristics is that it may trap some better plans in the fringe and output a non-optimal result. On the other hand, in admissible (optimistic) heuristics the bad plans can never exceed the true costs. So, it will always output the optimal result. 
Our A* search will always give optimal solution if this admissible heuristic used. The proof is: let’s assume two nodes corresponding to the goal state viz: A (optimal goal) and B (non-optimal goal). And assume that B is in the fringe, an ancestor n of A is also in the fringe. Now, if both A and B are in the fringe then obviously the priority queue will output A (optimal solution). Therefore, we need to prove that n will expand to A before B expands. Now, f(n) <= f(A) because the heuristic is admissible. And also, f(A) < f(B) because A is the optimal solution. Hence, f(n) <= f(A) < f(B) therefore, n will expand before B. 
When we observe the expansion contours of UCS and A*. We see that UCS will expand equally in all directions as it is blind towards the goal. While, A* expands manly towards the goal as it knows where the goal resides. However, it does not follow a direct straight path to the goal to ensure optimality. The A* search algorithm is used in a variety of problems usually as a pathfinding models, such as video games, language analysis, speech recognition, routing and resource planning [5].
We just assumed that we got an admissible heuristic and it solved all our problems. But, creating an admissible heuristic is itself a tedious task. One way is that we relax the problem i.e. We reduce or remove some of the constraints of the problem and then solve the relaxed version and try to find an admissible heuristic function. For example: For a city and shortest path problem we may relax the problem and assume that a person can fly and take a straight-line path directly to the goal state. Now the heuristic of this relaxed version (straight line distance) might give us an admissible heuristic. For an 8-puzzle problem there is a constraint that we must interchange a tile with a blank. If we relax the problem then we can assume that any tile could be interchanged with any other tile (or blank). So, this relaxed version will give the heuristic = number of tiles misplaced which could be an admissible heuristic. 


Reference:
1. http://www-formal.stanford.edu/jmc/whatisai.pdf
2. https://www.sciencedaily.com/terms/artificial_intelligence.htm
3. https://towardsdatascience.com/ai-search-algorithms-every-data-scientist-should-know-ed0968a43a7a
4. https://www.cpp.edu/~ftang/courses/CS420/notes/planning.pdf
5. https://en.wikipedia.org/wiki/A*_search_algorithm#Applications
6. https://www.nyu.edu/gsas/dept/philo/courses/mindsandmachines/Papers/mcdermott.html
7. http://ai.berkeley.edu/
