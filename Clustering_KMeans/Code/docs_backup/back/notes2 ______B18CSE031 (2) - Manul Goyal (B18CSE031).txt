Artificial Intelligence - Lecture Notes (Lecture 9 to 16)
Manul Goyal (B18CSE031)
Instructor: Dr. Yashaswi Verma

Tradeoff between Quality and Computation of a Heuristic
The more closely a heuristic function estimates the actual forward cost, the better it becomes. A good quality heuristic function, which approximates the actual cost closely, results in a reduced search space while searching for the solution. In other words, lesser nodes need to be expanded to reach a solution when a better heuristic is used, because it guides the search towards a solution in a more effective way, wasting lesser time in exploring nodes which do not lead to a solution. But, better heuristics usually require more computation per node. Thus, there is a tradeoff between the quality of the heuristic and the amount of computation it requires. This tradeoff is clearly evident in the two extreme cases, one in which we define the heuristic to be zero for every node, and the second in which the heuristic is equal to the actual forward cost for each node (called the exact heuristic). Both these heuristics are admissible since they are a lower bound for the actual forward cost, and therefore both of them lead to optimal solutions when used with the A* search. In the first case, no computation is required for the heuristic, since it is constant, but it is a useless heuristic since an A* search using it is equivalent to a uniform cost search, which is an uninformed search. In the second case, the heuristic is equal to the actual forward cost, which means that at the start state, the value of the heuristic is equal to the actual cost of the optimal solution. This heuristic would guide the search straight to the optimal solution, without expanding any extra nodes (which do not lead to an optimal solution) in between. Thus, it greatly reduces the search space, but requires a significant amount of computation per node. So, an appropriate heuristic, somewhere betweeen these two extremes, should be chosen keeping this tradeoff in mind.

Dominance of Heuristics
A heuristic h_1 is said to be dominant over another heuristic h_2, denoted by h_1 >= h_2, if for each node n, h_1(n) >= h_2(n). In this case, if h_1 is an admissible heuristic, h_2 is also an admissible heuristic, since h_2 is a lower bound of h_1 and h_1 is a lower bound of the true cost, which implies that h_2 is a lower bound of the true cost as well. 

Semi-lattice of Heuristics
The set of all admissible heuristics forms a semi-lattice (a partially ordered set). The ordering between heuristics is defined in the above section (dominance of heuristics). Not all pairs of heuristics can be compared, only some specific ones, which satisfy the definition of dominance stated above. Hence, it is a partially ordered set. The lowest and greatest elements of this set are the zero heuristic (which is always zero) and the exact heuristic (which equals the true cost) respectively. Every admissible heuristic is dominant over the zero heuristic and is dominated by the exact heuristic. These two heuristics are also called the trivial heuristics, and every admissible heuristic lies between them. Also, each set of admissible heuristics, {h_1, h_2, ..., h_k}, has a least upper bound, H = max(h_1, h_2, ..., h_k). For any given node n, H(n) = max(h_1(n), h_2(n), ..., h_k(n)). H is also admissible, since h_i(n) cannot exceed the true cost for any node n and any (admissible) heuristic h_i. Thus, the maximum of admissible heuristics is also admissible. 

Graph Search
It is a type of informed search, which searches the state space graph, ensuring that no state is expanded more than once. In all the search algorithms we have seen so far, the search tree was being searched, and states which had already been expanded may get expanded again, leading to redundant computations. In a search tree, multiple nodes may correspond to the same state, since there can be multiple paths to reach the same state (and there is a separate node corresponding to each such path in the search tree). Therefore, expanding more than one node corresponding to the same state leads to redundancy in tree search algorithms. In fact, expanding repeated states may lead to exponentially more work, especially in the case of breadth-first search. This can be illustrated using a path graph, in which there are two directed edges between each pair of adjacent nodes, both from the previous node to the next node. In such a graph, there are 2^(n-1) paths possible to reach the n-th node, and bfs would expand 2^(n-1)-1 nodes before finding the n-th node (while dfs would find it after expanding just n-1 nodes). In order to remove this redundant work, graph search algorithm maintains a closed set of expanded states, S, which stores all the states that have already been expanded. The algorithm proceeds like normal tree search, but every time a node is taken out of the fringe, it is first checked whether this node corresponds to a state which is already present in set S. If it is already present in S, then this node is ignored, and the next node is popped from the fringe. Otherwise, the state corresponding to this node is added to S, and then the node is expanded. This ensures that no state is expanded twice. This closed set can be implemented as a set rather than as a list, since sets can be searched more efficiently than lists.

Completeness and Optimality of Graph Search
Given adequate time, graph search would eventually expand every state (once). So, if there is a solution, the states leading to that solution will be expanded at some step, and therefore, the solution will definitely be found. Thus, graph search is complete. Uniform cost search (UCS) carried out using graph search will always give an optimal solution, because UCS maintains the invariant that every node which is not yet expanded will have a greater (or equal) cumulative cost than the node which is chosen from the fringe for expansion at any given step. So, the first time a state is expanded in UCS, it is guaranteed that the corresponding node has the least cumulative cost among all other nodes representing the same state, so the state need not be expanded again. On the other hand, A* search carried out using graph search may not give an optimal solution even when an admissible heuristic is used. It may happen that some state, say X, can be reached via two different paths, and the algorithm expanded the node corresponding to the more costly path that leads to X, because some ancestor of X on the less costly path had a heuristic value which overestimated the arc cost. This overestimation by the heuristic on the less costly path led the algorithm to expand X along the more costly path, and the state X won't be expanded again along the optimal path since each state is expanded at most once in graph search. In this case, the search would result in a suboptimal solution. To overcome this problem, we need to impose tighter constraints on the heuristic function, which is explored next.

Consistency of Heuristics
A heuristic h is said to be consistent if for every arc in the state space graph, from a state X to a state Y, h(X) - h(Y) <= arc cost from X to Y. The cost of the arc from X to Y just represents the actual decrease in the forward cost (cost to the nearest goal) when one transitions from X to Y. h(X) - h(Y) represents the heuristic 'arc' cost, i.e., the estimated decrease in the forward cost during this transition. A consistent heuristic, therefore, is such that the heuristic arc cost always underestimates the actual arc cost for every arc. A consistent heuristic is always admissible, but the converse does not hold. Assume that a sequence of states on the path from a state X_0 to the nearest goal state X_n be X_0, X_1, X_2, ..., X_n. If h is a consistent heuristic, then h(X_i) - h(X_(i+1)) <= cost of arc from X_i to X_(i+1), for all i. Summing on both sides, we get h(X_0) <= actual path cost from X_0 to X_n (since h(X_n) = 0 as it is a goal state). This implies that h is admissible. Recall that function f is defined for each node n as f(n) = g(n) + h(n), where g(n) is the backward cost or cumulative cost of n and h(n) is the value of the heuristic for n, which is an estimate of the forward cost to the nearest goal state from n. The consistency of h implies that the value of function f never decreases along a path leading to a goal state. Since h(X) - h(Y) <= arc_cost(X to Y), h(X) <= arc_cost(X to Y) + h(Y). Adding g(X) to both sides gives f(X) <= f(Y) (because g(X) + arc_cost(X to Y) = g(Y)). The result of this property of f is that the invariant which holds in UCS for cumulative cost (mentioned in previous section), also holds for f in A* search. Thus, A* graph search always gives an optimal solution when used with a consistent heuristic. Recall that A* tree search only requires an admissible heuristic for optimality, but, A* graph search requires a tighter constraint on the heuristic, namely, consistency. Also, UCS is optimal when carried out using graph search, because it is a special case of A* search with heuristic function as 0, which is a consistent heuristic. Generally, admissible heuristics which naturally arise from relaxed versions of problems tend to be consistent.

Proof of Optimality of A* Graph Search using a Consistent Heuristic
A consistent heuristic guarantees that the value of function f never decreases along a path leading towards a goal. Consequently, whenever a node with the smallest value of f is taken out of the fringe for expansion, all nodes with smaller values of f have already been expanded, and all the nodes remaining for expansion have greater or equal values of f than the one which is being taken out. Therefore, the first time a node corresponding to some state is popped from the fringe, it has the least possible f value among all nodes corresponding to the same state, so it is not needed to expand the same state via a different node (i.e., along a different path) in the future. Finally, when a goal node is popped from the fringe, it has the minimum f value (which, for a goal node, is equal to the cumulative backward cost) among all the goal nodes, thus giving us an optimal solution.

Planning and Identification Tasks
Planning tasks: These are the type of problems we have seen so far. Here, the objective is to come up with a plan, which is a sequence of actions, to reach a goal state starting at some initial state. This translates to finding paths in a state space graph, and each path is associated with some cost, which may be required to be minimized. Heuristic functions are used for informed searches, which guide the search towards a goal, instead of blindly searching the state space as in uninformed search. 
Identification tasks: These are problems in which the goal is to find an assignment of values to a given set of variables, from a given domain of values. The assignment may need to satisfy some given constraints (imposed by the problem). The path itself is not important in such problems, the resulting assignment produced is important. In fact, in many formulations of such problems, all paths have the same depth. Constraint satisfaction problems come under this category.

Constraint Satisfaction Problems (CSPs)
Given a set of variables and a domain of values (or different domains for each variable), the problem is to find an assignment of values to variables, which satisfy a set of given constraints. CSPs are a special subset of search problems, where the states represent assignment of values (from the domain(s)) to variables, and the goal test represents a set of constraints on the assignment, where each constraint specifies the allowed combinations of values for a set of variables. Examples of CSPs are the map coloring problem and the N-queens problem. Let's take a simple CSP example, which is searching for a Pythagorean triplet. Here, the set of variables is X = {a, b, c} and the domain D is the set of all natural numbers, i.e., D = {1, 2, 3, ...}. There is only one constraint in this problem, which can be represented implicitly or explicitly. The constraint can be represented implicitly as a^2 + b^2 = c^2, and explicitly as the set of assignments satisfying the constraint, i.e., (a, b, c) = {(3, 4, 5), (8, 15, 17), ...}. A solution is an assignment of values from D to all variables in X which satisfies the given constraint, for example, {a=7, b=24, c=25} is a solution to this CSP. Next, we present two other examples, the N-queens problem and the map coloring problem.

N-Queens Problem
We are given an N by N chessboard and we have to put N queens on the chessboard such that no two queens are in an attacking position. We will show two formulations of this problem as a CSP.
Formulation 1: Here, we choose the set of variables to be X = {X[i,j]}, where each X[i,j] corresponds to the square at the i-th row and the j-th column on the chessboard. The domain D = {0, 1}, where a value of 1 represents that a queen is present in the corresponding square, and 0 represents that the square is empty. The implicit constraints represent that no two queens are present in the same row, same column or same diagonal, and the total number of queens placed is N (i.e., sum(X[i,j]) = N over all i, j).
Formulation 2: Here, we choose the set of variables to be X = {X_1, X_2, ..., X_N}, where each X_i represents the position of the queen in the i-th row. The domain D = {1, 2, ..., N}. Here, X_i = j means that the queen in the i-th row is present in the j-th column. The implicit constraints are:
1. For each pair i, j, where i != j, X_i != X_j						
2. For each pair i, j, where i != j, abs(X_i - X_j) != abs(i - j) 
The first and second constraints enforce that no two queens are in the same column, and no two queens are in the same diagonal, respectively. The constraints that no two queens are in the same row and that the total number of queens is N, are automatically enforced due to the way we have chosen X and D.

Map Coloring Problem
We are given a map of cities, in which each city may be adjacent to zero or more cities. We have to color the cities in such a way that no two adjacent cities have the same color. Here, the variable X_i represents the color assigned to city i, and the domain is the set of all allowed colors C_i, i.e., X = {X_1, X_2, ..., X_n} and D = {C_1, C_2, ..., C_k}. The implicit constraint is that no pair i, j exists such that i != j, cities i and j are adjacent, and X_i = X_j.

Constraint Graphs
A constraint graph represents variables and the constraints between them. Many general-purpose CSP algorithms use the constraint graphs to speed up the process of finding a solution. In a binary CSP, each constraint relates at most two variables. For example, the second formulation of the N-queens problem above is a binary CSP. Constraints in such problems can be represented using binary constraint graphs, in which each node represents a variable and each arc connecting two nodes represents a constraint between the two corresponding variables. In CSPs, where constraints which relate more than two variables are present, we need to use special (auxiliary) nodes in the constraint graph. Each of these special nodes represents some constraint, and it is connected to multiple variables ('normal' nodes) which are related by this particular constraint.

Varieties of CSPs
CSPs can be classified based on the nature of the domain of values that the variables can take.
Discrete Variables: In such CSPs, the domain consists of discrete values. It may be finite or countably infinite. An example of a finite domain CSP is the Boolean satisfiability problem, in which the task is to find an assignment of truth values to a set of variables which satisfy a given Boolean formula. Here, the domain only consists of two values, {TRUE, FALSE}. In CSPs with a finite domain, the total possible number of assignments to n variables is d^n, where d is the size of the domain. An example of a countably infinite domain CSP is job scheduling, where variables are start and end times of jobs and the domain is the set of all non-negative integers.
Continuous Variables: In such CSPs, the domain is a set of continuous values. The domain is uncountably infinite in this case. An example of a CSP with a continuous domain is diet planning, in which the variables are quantities (weights) of different kinds of food items, and appropriate quantities of each food item need to be taken subject to constraints on the total intake of calories, sugar, fat, etc. The domain is the set of all non-negative real numbers in this case.
Solving continuous variable CSPs is generally easier than solving discrete variable CSPs.

Varieties of Constraints
Unary Constraints: constraints which are applicable to a single variable only. For example, in the Pythagorean triplet CSP, a unary constraint on variable a can be a > 0. Such constraints are equivalent to reductions in the domain.
Binary Constraints: constraints which relate two variables. For example, in the N-queens problem, the constraints in the second formulation (given above) are binary constraints. They can be represented by binary constraint graphs.
Higher-order Constraints: constraints which relate more than two variables. For example, in the first formulation of N-queens problem (given above), the constraint that sum of all variables is equal to N is a higher-order constraint. Such constraints require the use of special nodes in the constraint graph.

Preferences (Soft Constraints)
These types of constraints involve associating costs with values in the domain or with different assignments of values to variables. This results in preferring some assignments over other assignments, and the goal is then to find an assignment which not only satisfies the constraints, but also has the minimum cost. Such type of problems are known as constrained optimization problems.

Standard Search Formulation of CSPs
The states represent partial assignments of values to variables. The initial state represents that no variable is assigned a value yet. The successor function assigns a value to an unassigned variable. The goal test is that the assignment represented by the current state is complete (assigns one value to each variable) and satisfies all the given constraints.

Uninformed Search Algorithms for CSPs

Naive BFS
Since each state has a huge number of potential successors (for instance, there are n*d possible successors of the initial state, where n = number of variables, d = size of domain), BFS would have to store all these states in the fringe. It is not even possible in case of countably infinite domains. Moreover, we know that solutions are present only in the last level (at depth n), so BFS would have to traverse the whole search tree before reaching a solution. So, naive BFS is very inefficient for solving CSPs.

Naive DFS
This algorithm would keep assigning values to unassigned variables one-by-one, until it reaches a leaf node in the search tree, which corresponds to a complete assignment. If it satisfies the constraints, this assignment is returned as the solution, else, the algorithm backtracks and tries other assignments. Since we get to the first solution in just n steps, this algorithm is better than naive BFS. 

Backtracking Search
Two important modifications to DFS lead to backtracking search. Firstly, the order in which variables are assigned values is irrelevant to the solution. So, at each level in the search tree, one variable should be fixed and only that variable should be assigned values at that level. Once an ordering of variables is fixed, the algorithm only needs to assign values to one variable at each step. Secondly, constraints should be checked at each step, instead of checking them only at the last level. In the map coloring problem, a possible assignment is {X_1 = C_1, X_2 = C_1, ..., X_n = C_1}. Assuming that city 1 and 2 are adjacent, it is known at the second step itself (when X_2 is assigned C_1) that the resulting assignment would not satisfy the constraints. So, there is no need to traverse all the way down and then backtracking. Instead, the algorithm should backtrack as soon as a partial assignment is obtained which doesn't satisfy the constraints. Thus, we are doing an 'incremental goal test' at each step, which requires some additional computation, but drastically reduces the search space. 
So, backtracking search works by first choosing some ordering of variables, and then at each step, it assigns a value to the variable corresponding to that level, such that this value doesn't violate the constraints when combined with the existing partial assignment.

Techniques for Improving Backtracking Search

Filtering: Forward Checking
In this technique, we backtrack as soon as we obtain a partial assignment which would inevitably lead to a constraint violating assignment in the future. The partial assignment may not directly violate the constraints, but it will definitely lead to a violating assignment. To check this, we keep track of the domains of unassigned variables, and at each step when some variable is assigned a value, the values in domains of unassigned variables which will definitely lead to violating assignments, are crossed off. If at any step, the domain of some unassigned variable becomes empty, then the resulting assignment will definitely lead to a violating assignment in the future, so we should backtrack at this step itself.

Filtering: Constraint Propagation
We can improve the previous technique by doing additional checks when we propagate information from assigned variables to unassigned variables. For example, in the map coloring problem, suppose we have three total colors in the domain and we have already assigned two different colors to city 1 and 2. Now, if both the cities 3 and 4 are adjacent to city 1 and 2, we only have one color left in each of the domains of city 3 and 4. Now, if city 3 and 4 are also adjacent to each other, then the current partial assignment would definitely lead to one of the domains becoming empty in future (for instance, if city 3 is assigned the last color in the next step, city 4's domain would become empty). So, using this check, we can backtrack at the step in which city 1 and 2 are assigned different colors itself, even before some domain becomes empty, which further reduces the search space (although it increases computation at each step).

Consistency of a Single Arc
An arc (directed edge) from a variable X (tail) to a variable Y (head) is said to be consistent iff for every value in the domain of the tail (X), there exists some value in the domain of head (Y), which can be assigned to Y without violating the constraints. In terms of consistency of arcs, forward checking involves checking the consistency of each arc from an unassigned variable to the variable which is assigned a value in the current step. If such an arc is found to be inconsistent, all the values in the domain of the tail which lead to inconsistency are removed from the domain.

Arc Consistency of an Entire CSP
Checking consistency of only those arcs which point to the newly assigned variable as mentioned above may not lead to constraint propagation. For example, in the map coloring example mentioned in the constraint propagation section above, when a color is assigned to city 2, the inconsistency of the arc from city 3 to 4 won't be discovered since only arcs pointing to city 2 are checked. In order to enforce constraint propagation, all the arcs must be checked (in both directions) at each step (there are a total of n*(n-1) arcs). In this example, when city 2 is assigned a color, we note that the arc from 3 to 4 becomes inconsistent, since no color can be assigned to city 4 once the last remaining color is assigned to city 3. So, we can backtrack at this step itself, thus enforcing constraint propagation. 
Note that once some value is removed from the domain of the tail X to enforce consistency, all the arcs from the neighbours of X to X need to be rechecked in order to enforce constraint propagation. This technique would lead to earlier detection of failures than forward checking, but at a greater computation cost at each step.

Limitations of Arc Consistency
Since checking consistency of arcs involves checking only two variables at a time, constraints involving more than two variables may not be captured. For example, if there are three cities X_1, X_2 and X_3, and each of them is adjacent to the other two, and if each of their domains contains only two colors C_1 and C_2, all the arcs are still consistent, but no assignment which does not violate the constraints exists in such a case. Therefore, in such situations, checking for arc consistency may not lead to detection of inevitable failures in the future. Another limitation is that it considerably increases the running time of backtracking search, which may be infeasible for problems involving large number of variables and large domains.

Ordering of Variables in Backtracking Search
The order in which variables are assigned values in backtracking search is an important factor which determines the efficiency of the algorithm. We discuss some ordering techniques next.

Ordering by Minimum Remaining Values
In this ordering, the next variable to be assigned a value is the one with the smallest domain (least remaining number of values in the domain). Since variables with the smallest domain have the greatest chance of ending up with an empty domain, we greedily assign a value to such a variable before other unassigned variables with larger domains, so that the chances of our partial assignment failing in the future are reduced. This is a benefit of this technique. It is also called 'most constrained variable' or 'fail-fast' ordering.

Ordering by Least Constraining Values
In this ordering, the value to be assigned to the next chosen variable is the one which shrinks the domains of other unassigned variables by the least amount. The advantage of this ordering is that the chances of the partial assignment failing in the future are reduced, since the least constraining value is chosen at each step. Again, it comes at the cost of increased computation at each step.

Local Search Techniques and Optimization
All the problems and algorithms so far invloved simulation by the agent, not actual movement of the agent. Once a solution is found by simulation, the agent can then carry out the plan (sequence of actions) to achieve its goal. Now, we turn to problems in which the agent has to decide an action which it actually executes after making the decision. The decision of which action to take is based on the local neighbourhood of the current state. The objective of the problems will be to optimize some objective function which associates a value with each state, i.e., to find a state with an optimal value of the function. The path taken to reach such a state is irrelevant. The advantages of local search is that it uses very little memory as compared to the previous planning problems, since it only needs to keep track of the current state, and not the whole path to reach this state. This makes it feasible to find reasonable solutions to problems which have a large or infinite (even continuous) state space. Since we are considering pure optimization problems, the cost of paths and goal test formulation don't make sense here. The sole purpose is to minimise/maximise the objective function.

Trivial Algorithms

Random Sampling: In this algorithm, states are randomly generated and the value of the objective function is checked for optimality. If this process is repeated infinitely many number of times, it is guaranteed to result in an optimal solution.

Random Walk: In this algorithm, a neighbouring state of the current state is chosen randomly, and it becomes the new state. This process is repeated until an optimal state is found. 

Both the above algorithms are asymptomatically complete, which means that if they are executed for an infinite time, they are guaranteed to result in an optimal solution.  

Hill-climbing Search (Greedy Local Search)
This algorithm works by always choosing the neighbouring state with the highest value of the objective function (assuming that the goal is to maximize the objective function). If all the neighbouring states have a lower value of the objective function than the current state, the current state is a local maxima and it is returned (the algorithm terminates). Therefore, the result of this algorithm is a local maxima, i.e., it may not (and usually does not) return the global maximum. Apart from this drawback, the other problems are that it only considers the immediate neighbours (successors) of the current state, and doesn't look ahead of them, and if multiple neighbouring states have the highest values, it randomly chooses one from them. This may lead to wrong decisions (in the long term) and thus, result in smaller values of local maxima. The final solution obtained by this algorithm depends on the start state.

Example: 8-Queens Problem
We can formulate the 8-queens problem (which was originally a CSP) as a constrained optimization problem, which can then be solved by local search algorithms. Each configuration of the queens on the chessboard corresponds to a state, and the start state is that all the queens are initially in the first column. The successor function moves one queen to any square in the same row. Here, we define a heuristic function which will serve as the objective function, which is required to be minimized. The heuristic h(n) for a node (state) n is defined as the number of pairs of queens which are in an attacking position in the state n. Minimizing h to zero would then be equivalent to solving the same problem posed as a CSP. The difference is that when posed as an optimization problem, each possible configuration of queens on the chessboard is a solution. But our goal is to find a complete or optimal solution, which is one that minimises the value of h to zero.
Using the hill climbing algorithm to solve this problem will result in a suboptimal solution most of the times, i.e., one in which some pairs of queens are still in an attacking position. This is because this algorithm usually results in a local optima (minima in this case).

Drawbacks of Hill-climbing
1. Local Optima: As stated previously, this algorithm usually gets stuck in a local maxima/minima, but we generally want to find a global maximum/minimum.
2. Plateaus/Shoulders: Plateaus are states where all the neighbouring states (successors) have the same value of the objective function. Since the algorithm randomly chooses the next state in such cases (and doesn't look ahead of its immediate neighbours), it may remain stuck on a plateau for a long time. For this reason, the algorithm is usually terminated when it encounters a plateau.
3. Diagonal Ridges: Suppose we have to solve a 2D optimization problem, and the successor function only allows moves (steps) along the x and the y axes. It may happen that the function is increasing in a diagonal direction, for example, along the y = x line, but it is decreasing in x and y directions. In order to move along the diagonal direction, the algorithm has to take a step in the x and then in the y direction (or vice-versa). But due to its greedy nature, it won't do so, and remain stuck in its current suboptimal position.

Escaping Shoulders
A shoulder is a plateau which is not a flat local maxima, i.e., there are possible uphill moves on some points of the boundary of the plateau. If the algorithm encounters a plateau state, i.e., if there are no uphill moves present, we can allow it to perform sideways moves, in a hope that it will eventually leave the shoulder. In order to avoid getting stuck in an infinite loop (especially when the plateau is a flat local maximum, in which case it will never escape), we impose an upper limit on the number of sideways moves allowed. The algorithm terminates if it could not escape the plateau even after doing the maximum allowed number of moves.
Compared to normal hill-climbing, allowing sideways moves increases the probability of finding an optimum solution (h = 0) in the 8-queens problem from 14% to 94%, but using a greater number of steps to reach the solution.

Tabu Search
The drawback in the above method for escaping shoulders is that the algorithm may end up looping over the same few states repeatedly, since it resolves ties randomly, and no previous states are remembered. A possible solution is to maintain a queue of some fixed length, also called a tabu list, storing the list of states visited in the past. Whenever a new state is encountered, it is pushed into the queue and the oldest state is popped from the queue. It is ensured that the new state is not already present in the queue, which prevents repetition of states upto a few number of steps. As the size of the queue is increased, the search would increasingly become more "non-redundant", which would increase the chances of escaping a shoulder within the allowed number of sideways moves.

Variations of Hill-climbing
We describe some variations of hill-climbing which are designed to solve the local maxima problem faced by hill-climbing. Greedily selecting the steepest uphill move at every step will always suffer from getting stuck in a local maxima, but is more efficient than random walk or random sampling, which are guaranteed to find a global maxima, but are very inefficient. Let's try to combine the two approaches.

Stochastic Hill-climbing
In this variation, instead of always choosing the best neighbour, a neighbouring uphill state is chosen randomly with some probability, which directly depends on the steepness of the uphill move. In practice, this algorithm converges slower than the greedy approach, but it is able to find better solutions in some scenarios.

Hill-climbing with Random Walk
In this approach, the best neighbouring state is chosen with some probability p < 1, otherwise some neighbour is chosen randomly from the remaining neighbouring states with a probability 1-p. The hope is that it would prevent the search getting stuck in a local maxima. This algorithm is asymptomatically complete, i.e., if it is repeated for an infinite number of steps, it guaranteed to find an optimum solution.

Hill-climbing with Random Restart
This is a widely used method which works on the principle that "if you don't succeed at first, try again". A starting state is randomly generated and if hill-climbing from this state results in a local maxima, a new starting state is randomly generated, and the process is repeated (either until the optimum solution is found, or for a fixed number of restarts). This algorithm is asymptomatically complete, as repeating this process infinite times is guaranteed to eventually result in a starting state that itself is the global maximum. This algorithm finds an optimum solution relatively quickly when the number of local maxima and plateaux is relatively less. But even in NP-hard problems, which typically have an exponential number of local maxima, a reasonably good solution can be found after a few number of restarts.
If the probability of finding a global maximum is p in a particular iteration of hill-climbing, the expected number of restarts required is 1/p, and the expected number of steps taken is a + (1/p - 1)*b, where a and b are the expected number of steps needed for reaching a global maximum and a local maximum in a single iteration respectively.

Hill-climbing with both Random Walk and Restart
In this approach, at each step, either the best neighbour is chosen, or some other neighbour is randomly chosen, or a starting state is randomly sampled and the search is restarted from this state. Since both the component approaches are asymptomatically complete, this algorithm is also asymptomatically complete. 

Hill-climbing with Exhaustive Search for Escaping Shoulders/Local Optima
This is a fusion of the hill-climbing approach with exhaustive search methods (dfs or bfs). When the current state is a local optima or a plateau, we start exhaustive search from this state (using dfs or bfs), until some state with greater value of objective function is achieved. This way, we can escape local optima and plateaus, by adopting middle ground between hill-climbing (local) and exhaustive (systematic) search. Of course, it comes at the cost of the high computation required for exhaustive search.

Simulated Annealing
Inspired by physics, this algorithm tries to prevent the search getting stuck in a local optima, by providing it a 'shake' whenever it gets stuck. Formally, at each step, one neighbouring state of the current state is randomly picked and the difference between the values of the objective function, say D, is computed. If the neighbouring state is better, i.e., D > 0, it is selected and the search transitions to this state. Otherwise, if D <= 0, this state is selected with a probability p = exp(D/T), where T is the intensity of the shake or 'temperature'. The probability of selection p exponentially decreases as the neighbouring state gets worse (D becomes more negative), i.e., locally bad moves have a lower probability of getting selected. Also, the temperature T is a decreasing function of time, i.e., it has greater values at the initial steps of the search and smaller values as the search progresses. This has the effect of allowing downhill moves with greater probability initially, but it gradually decreases with time. Thus, over time, is becomes less and less likely to make locally bad moves, but there is always a chance to get out of a local optima. In fact, given that the temperature is decreased very gradually, this algorithm is guaranteed to find a global optimum, so it is asymptomatically complete. However, it is very slow in practice. It is therefore useful in problems which strictly require a global optimum, such as designing VLSI layouts, where an optimum layout can drastically reduce the cost of mass production.    

Physical Interpretation of Simulated Annealing
Hill-climbing search to minimize the objective function can be viewed as a ball rolling downhill on a surface, which will usually get stuck in a valley before it reaches the deepest crevice, which is analogous to the search getting stuck in a local minima. To release the ball from the valley, we can provide a little 'shake' to the ball which is just hard enough so that it gets out of the local minima, but doesn't miss the global minimum. This 'shake' can also be viewed as a 'temperature' provided to a metal which causes vibrations in its particles. As the temperature is slowly decreased and the energy of the particles decreases, they become more and more stable, which is also expected to happen in the above mentioned approach. 

Local Beam Search
Instead of storing only one state in memory at a time, we can exploit the available memory (which is much larger in practice) by keeping track of k different states simultaneously at a time. All the successors of these k states are considered and if any one of them is the goal (global optimum), then it is returned and the algorithm terminates. Otherwise, the best k states is selected out of all the successors. This process is different than running k independent hill-climbing searches in parallel, since these k processes are dependent in this case. For instance, if the successors of one state are better than all the successors of the other k-1 states, the former will be chosen instead of the later. This effectively results in a communication between the processes which leads to continuation of better processes and halting of poor ones. The disadvantage is that the diversity may be reduced as all successors of a single state may get selected, and other diverse states may be rejected. In fact, quite often, all the k searches end up at the same local optima. The following modification resolves this problem.

Stochastic Beam Search
In this algorithm, k states are randomly selected from all the successors of the current k states, with a bias towards the better states, i.e., the probability of a successor getting selected is directly proportional to the value of its objective function. This approach closely resembles the process of natural selection, where offsprings (successors) of the states (organisms) have a greater tendency to survive (getting selected) if they are more fit (have a higher objective function).

Genetic Algorithms
These algorithms are motivated by the process of natural selection, evolution and sexual reproduction processes. The main idea is to generate the successor states (offsprings) by combining (crossing-over) the current states (parents). We start with k randomly generated states, called a population. Each state is represented by a string over some alphabet. For example, in the 8-queens problem, a state can be represented by a string of the digits 1-8 where the i-th digit represents the position of the queen in the i-th row. We also define a fitness function which associates a value to each state in the population, and the value is proportional to the quality of the state (better states have higher fitness values). For the 8-queens example, the number of non-attacking pairs of queens can be chosen as the fitness function (reverse of the heuristic function). The successor states are then generated using three processes, namely, random selection, crossover and random mutation.

Random Selection
This step involves random selection of k states from the population with replacement, with greater probabilities of selection for states with higher fitness. The selection probability of each state is calculated as the ratio of its fitness to the total fitness of all states in the population. This probability is analogous to the chances of survival of an organism based on its fitness for survival. After sampling k states, the resulting set of states has a higher overall fitness than the original population, and the remaining states in the original population 'die', since they are comparatively unfit.

Crossover
The sampled population is randomly partitioned into k/2 pairs, and each pair is then crossed over using some crossover scheme. For the 8-queens example, we can split the strings in a pair at a randomly chosen crossover point, and the suffixes of both the strings are swapped. Thus, each pair of parent states generates a pair of offspring states which results in a new set of k states (analogous to a new 'generation' of offsprings). These offspring states have combined properties of their parent states, which were able to survive due to high fitness, and therefore, are expected to be more fit (and thus, closer to the optimum solution). The effect of this step is that the search jumps to a completely different set of states in the search space, which cannot be achieved in a single step in local (or stochastic) beam search. Thus, it transcends the limits of local search, which only considers the immediate neighbourhood while making a move.

Random Mutation
After the new set of states is obtained, each state is randomly mutated independently with a small probability. For example, in the 8-queens example, zero or more digits may be randomly altered in the string corresponding to each state. This corresponds to randomly choosing queens in the crossed over states and changing their position randomly in the same row. This process is analogous to evolution in biology, which introduces small changes in organisms to better adapt them to their environment.

Advantages of Genetic Algorithm
This algorithm is a variant of stochastic beam search, which is analogous to asexual reproduction. The primary attraction of genetic algorithms is the crossover step, which closely resembles mating in organisms. The advantages are that this algorithm can find solutions which local search algorithms can't find, because they usually get stuck before reaching such a solution.

Disadvantages of Genetic Algorithm
This algorithm is mainly successful in problems where smaller configurations of good quality which are found by the search can be combined to get larger configurations of better quality than its components. Therefore, the success of this algorithm is considerably affected by the way in which the problem is framed. Since there are a huge number of tunable parameters in this algorithm, it is difficult to replicate the performance on other problems. Moreover, there is no evidence that these algorithms perform better than hill-climbing algorithm with random restarts.

Genetic Algorithm for Travelling Salesman Problem (TSP)

Representation: We can number each city in the problem and then represent each state (valid solution) as a permutation of these city numbers, which corresponds to a sequence in which the cities are traversed.

Mutation: We can randomly swap two cities in a given state, but a better approach would be to randomly choose two cities and swap them only if the total cost is reduced. Another approach can be to exhaustively search all the pairs and swap the one which results in the greatest reduction in the cost.

Crossover (greedy): We can start by randomly choosing a parent and pick its first city. Then we compare the next cities after this city in both the parents, and choose the closer one. If one of them was already selected before, choose the other one, and if both of them were already selected, choose some random non-selected city, and then repeat the process.  

Mixability
In the random selection step, it may happen that some state is selected more than once and it may not be the fittest state among the population. Since such a state may have greater number of copies in the sampled population than other states, it gets crossed over with different states and then, many states will have the characteristics of this state in the resulting offspring states. So, this particular state has greater mixability than other states. Thus, the fittest state in the initial population may not exhibit the greatest mixability if the sampling is done randomly.

Gradient Descent - Hill-climbing for Continuous State Spaces
Hill-climbing algorithm was applicable on problems with discrete search spaces, i.e., each state had a discrete (finite or countably infinite) set of successor states. In such problems, we were able to compare each of the successor states one-by-one and choose one of them according to some strategy. In the case of problems with continous state spaces, one approach is to discretize the state space and apply the hill-climbing algorithms. But, a better approach is to apply gradient descent (or ascent, as required by the problem). In order to calculate the gradient of a multivariable function at some point, the function must be continuous and differentiable at that point. For a given function f(x_1, x_2, ..., x_n), we compute the partial derivatives of f at a given point (say P = (k_1, k_2, ..., k_n)) with respect to each variable x_i, say d_i. Then, the direction of the vector D = (d_1, d_2, ..., d_n) represents the direction of steepest ascent with respect to the P, and its magnitude gives the slope of the ascent (D is called the gradient of f at P). Then, the update step (in case of gradient descent) is defined by P = P - l*D, where l is a scalar value which determines the size of the step taken opposite to the direction of the gradient. Repeating this process several times leads to a local minima of the function f (a global minima if f is a convex function).
Similarly to the simulated annealing method, we can gradually decrease the value of l, starting with a larger initial value.    

References:
[1] Lecture Slides
[2] http://ai.berkeley.edu/home.html 
[3] http://www.cse.iitd.ac.in/~mausam/courses/col333/autumn2019/ 
[4] http://www.cse.iitd.ac.in/~mausam/courses.html 
[5] Russell, Stuart J. (Stuart Jonathan). Artificial Intelligence : a Modern Approach. Upper Saddle River, N.J. :Prentice Hall, 2010
[6] https://en.wikipedia.org/wiki/Semilattice