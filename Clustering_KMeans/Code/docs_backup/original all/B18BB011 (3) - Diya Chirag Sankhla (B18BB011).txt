Weekly Notes 1
Name- Diya Sankhla(B18BB011)
Artificial Intelligence has become one of the favorite storylines for many Hollywood movies and even though it is imaginary and we don’t find it happening in the real world, still people find it very fascinating as we see machines which has thinking powers like Humans, have reasoning and problems solving skills which can make our life better. Broadly speaking Artificial intelligence is intended to make smart and powerful machines that possess intelligence which means thinking power in similar ways like human brains and behave accordingly in a productive manner than humans in some tasks. Computational complexities arise while making decisions Reasonably and sensibly. Arriving at a Rational decision simply involves making choices after reviewing full available information, even minute details, and evaluating all options to choose the best possible alternative to achieve the goals with the highest associated utility. Rational choices aim at maximizing the utility. Inspiration to make intelligent and smart machines that can think and behave rationally like humans are somehow motivated by the functioning of the Human Brain. Neurons in Humans' brains are good at learning and remembering from past experiences but are not always perfect, this helps in making Rational choices. Similarly, Intelligent machines should be designed in a way such that they have a memory to remember information and to create computational simulations.AI can perform several things like working as a bot, good at facial features recognition, speech recognition, and can even perform some household chores, but it still has some limitations like driving in crowded areas or talking successfully with another person for an hour or more[1]. When we are talking about AI, we are defining an Agent who can perform such tasks. An agent can be defined as an artificially intelligent machine, robot, or a computer who can perceive information or signal from an environment and can act accordingly[1]. The selection of rational actions is determined by three factors first, what an agent perceives from the environment, features of the environment, and availability of possible actions. And AI Aims in designing such a Rational agent. How agent plans are classified into two types Reflex Agent and Planning Agent. Reflex agents are the ones who perceive Information from the environment and act based on the current scenario. They don't think about the future consequences of their actions, they just behave[1]. We can't assure that they have a memory to store past decisions. The reflex agent being rational will ultimately depend on the environment and actions taken. While on the other hand Planning agents do not randomly choose any path, they understand the consequences of their actions. They are somewhat aware of their working environment and try to keep track of their path.
Example -Let’s take a simple Pac-man game without a ghost. Here agent aims to eat all the dots. So the agent will choose arbitrarily the path and will start eating dots encountered. But it might be possible that the chosen path at the end has a loop(P shaped path), so the agent will start eating dots on the straight path but after completion of a loop, it will be stuck because no dot left on the traveled path. As a result, it won’t be able to eat all dots in the environment. In this example the agent behaved in a reflex manner, he had not considered the future consequences that he might get stuck after taking this path. While on the other hand planning agent would have chosen a path, considering future outcomes.
Optimal planning implies choosing the best possible path to achieve the goal. While Complete Planning focuses on achieving the goal, not on the path followed to reach the goal therefore the chosen path might not be the best possible path. Example - suppose we have three coordinate locations on a plane A, B, C forming a triangular shape, reaching point C from A can be optimally achieved by taking Euclidean Distance between A and C, instead of taking a path from A to B then B to C.Note that Optimal planning is always complete planning[1].Ideally, we should aim in achieving optimal solutions, and this is where the complexity arises. Planning normally executes the whole plan at a time, while replanning comes with many different sub-plans executed one after another, and such an agent which plans continuously is called a Replanning Agent[1].
To behave rationally agents need to perform some tasks using search algorithms. While Formulating Search Problem different parameters need to be considered, first is state space, which depicts all the available possibilities or states in an environment. The start state is referred to as the initial or starting state and the goal state is a final state which an agent aims to achieve, it could be many. A goal test is some type of function that tells us whether or not the state reached is a goal state[2]. The successor function accounts for actions and the cost associated with the actions, It describes how agents act in given state space. The solution is described as step by step actions taken to achieve the Goal from the start state[2]. While doing search tasks we first take approximations to model the environment. Let’s consider an example of traveling in Romania and here we aim to travel from Arad to Bucharest with the shortest path. In the map all the state space is Cities, while the start state is Arad, Goal Test is Bucharest, and the successor function will be defined as roads which are connecting adjacent cities and cost as distance covered, the solution can be traveling sequentially from Arad-Sibiu-Rimnicu Vâlcea -Pitesti-Bucharest since this path has the lowest Distance[3]. Every Search problem we are trying to approach is determined in the context of the World State. And the World State keeps all possible and minute details of the environment while the search state only considers information needed for planning the path[1]. Let’s take an example based on Pac-Man. Let’s assume the world state consists of possible positions of Pac-Man agent, counts of power dots and normal dots, three ghosts, and their position and agent’s possible actions. And here we have two problems, one which aims in the planning path and another which aims to eat all the dots. While in the first problem, states are location coordinates(x,y), actions can be taken in NSEW directions, the successor function will monitor and update the location as we need to trace our path. The goal test is the end location(x,y). While in the second problem states additionally consists of booleans representing the presence or absence of dots. Actions will be the same as NSEW, the successor function will also include the process of updating boolean to keep track of dots eaten and the dots remaining. Here the Goal test will change and it will represent all eaten dots, while the state could be anything. So we can say that the goal test is not always the same as a goal state[1].
The search problem can be approached either by creating a state-space graph or a Search tree. In the state-space graph, all possible states are represented by nodes, and arrows connecting nodes represent the resulting state as a consequence of actions. Each state will occur only once while constructing a state-space graph, nodes might have multiple edges and they can be approached from several directions.Building a full state space graph is not always feasible and requires large memory for its storage but it can also give us vast information.The search tree begins from the root node which is the initial node and their branches denote child nodes which can be further be explored. Unlike the graph, there might be an iteration of some states. While searching the tree, the path taken to reach the Goal state can be easily traced out. In graphs, if loops are not taken care then it will ultimately lead to an infinite search tree and we won’t be able to reach our goal, so cycles in the graph should be avoided with some constraints. Because of restraint on memory and computational complexity we might never explore a complete search tree. While searching across the Tree we need to maintain a fringe, which will keep a track of nodes that were seen but not yet expanded. So basically fringe consists of partial or sub plans. General Search Tree algorithms will be in such a manner, first, we will start with a frontier containing the root node. Second, we will repeat the following steps, if the frontier does not contain any node then return no solutions. Choose the node from the frontier based on the strategy. If that node contains a goal state then return solution, else expand the node and add it to the frontier. And what sub-plan to be picked from the fringe will completely depend on the Algorithm to be used[1]. Eg if we are implementing a Uniform cost search then the path with the least cumulative cost will be explored first from the fringe.While in the DFS path which will lead to the deepest nodes will be explored first.The tree search algorithm has been broadly classified into two sub-parts Uninformed and Informed Search Algorithm. Let’s first begin with the Uninformed Search Algorithm. The depth-first search is an Uninformed search Algorithm that will begin from the root node and will expand the deepest node in the tree[4].In DFS implementation Last in First Out(LIFO)  principle is followed, which is standard stack. So basically DFS will pick one path and explore until dead-end and then it will backtrack and explore other nodes. If a solution exists and loops are taken care then DFS will always lead to a solution, therefore it is complete. We are not always sure to get an optimal solution as we don’t perform an exhaustive search. If on the first path only we found the goal then the search will stop and it won’t be looking for other solutions that might be optimal. If we define m as max depth and b as branching factor then the Time complexity for search will be O(b^m).DFS has a very high time complexity because it will search for the deepest node in the search tree. The space complexity of DFS will be O(bm), because we don’t need to store every node, but we need to keep track of memory. Breadth-first search is another Blind search algorithm that will always expand the shallowest node simply means it will expand nodes at the same level and then goes to the next level. For maintaining a fringe queue can be used, which follows the First in first out principle. If s is the depth of solution then Time complexity is defined as O(b^s) and space complexity will also remain the same as O(b^s) as here we need to keep track of every expanded node. Yes BFS is complete if the solution will exist and a number of levels will be finite( s will be finite).BFS can only be optimal in a scenario where the cost associated with an action is 1 while moving from one node to another node because later on, the cost will increase as it is cumulative. In average conditions, DFS has better space complexity than BFS.BFS will outperform DFS in a scenario where the branching factor would be less and the goal is near the root node so that it can be easily accessed.DFS can outperform BFS in a scenario that has a high branching factor and solutions would exist for an example in the left-most part of the tree at depth, and if the suppose solution exists at the depth at the rightmost branch then it won't be feasible to use a Depth-first search.Iterative deepening is another search algorithm and it is a combination of both DFS and BFS. It takes advantage of Low space complexity of DFS and BFS ability to find a shallow solution. First, we fix the level of depth (like suppose initially we fix depth to 1 )which we need to iterate, then we will search for a solution at a fixed depth. And if a solution is not found then we will move to the next level of depth and repeat the same process.It seems highly superfluous but still, it is better than DFS, as it finds a solution at the upper level instead of searching one path till the dead-end. This algorithm would work best and redundancy would be reduced when the goal is in the upper right portion of the tree.Another Uninformed search algorithm is Uniform Cost Search which paves the path based on the least cumulative cost. To maintain a fringe, the priority queue can be used, which will prioritize the least cost. While searching it will first explore the node with the least cumulative cost. Yes UCS is both complete(if finite steps and positive cost) and as well as optimal as it considers the least cost at each path.Normally UCS doesn’t explore the complete tree, but if  suppose cost turns out to be negative then we need to explore every node to find the optimal solution.UCS will have the same time and space complexity as DFS which is O(b^number of required steps).Example- Let's take a weighted tree.The first fringe will store the root node since the cumulative cost is 0.Then the Root node will be taken out from the fringe and all successor of the root node will be placed into fringe and the one with the least cumulative cost will be taken out from the fringe and explored further. In such a manner we will explore the current chosen node until a goal is reached with the least cost.As we have seen during searching, the agent tries to simulate all the possibilities to reach the goal and eventually chooses one plan.The accuracy of the model will determine the effectiveness of the search.Example- Google maps are simulations of real-world scenarios.And they sometimes show several paths to reach a particular destination.And suppose if they don’t specify that one of the paths is from someone’s personal property which is not right and here our search algorithm will fail.The issue with a model can affect search results.
One of the foremost limitations of Uninformed Search Algorithms is that their implementation is slow, as they don’t have prior information regarding the goal's location, so they search blindly in all possible directions. And this issue can be resolved by implementing informed search algorithms. Informed searches use a special function called the Heuristic function, it estimates how close or how far an agent is from the goal with respect to a particular state. It gives an estimate not actual value. For each problem, we need to design a unique Heuristic. The heuristic may not provide us an optimal solution but guides an agent towards a goal instead of searching in all directions. The heuristic value of the goal state is always zero. Greedy Search Algorithm is an Informed search algorithm that expands the nodes which look closer to the goal under the guidance of the Heuristic function. The greedy search algorithm is prioritized with the least Heuristic value. If finite nodes then the Greedy Algorithm is complete, but not assured to find an optimal solution. Example- let's take a weighted tree, with the heuristic value defined at each node.First, the initial state is explored and all sub plans are placed into the fringe. The one with the least heuristic value will be extracted from the fringe, not the one with the least cost and similarly, we would explore that current state until it reaches zero heuristic value that is the goal state. Greedy Search Algorithm turns out to be worse in a scenario where we reach the states near the goal, but since there is no path connecting that goal we might end up searching more number of nodes, because of badly guided heuristic function. This can be resolved by putting some constraints on heuristic functions, so to avoid misguidance. The disadvantage of Greedy Search is that it will always choose the path closer towards the goal from its current state and will not understand that moving slightly away from the goal can even give the best solution. So we can interpret that agents implementing greedy search somehow behave reflexively.On average Greedy Search, Algorithm performs better than DFS and BFS, since it has some guidance as compared to searching blindly in all directions.And the performance of the Greedy Search Algorithm will depend on the heuristic function as well as the working environment. At the end, we need an optimal solution with the least amount of exploration. And one such algorithm is A* search.A* search is the combination of both Uniform Cost Search and Greedy Algorithm.UCS has slow implementation but will ultimately reach an optimal solution.On other hand Greedy Search guides towards goals, with the faster implementation.
Example - suppose we have a weighted directed graph with the heuristic value associated with each node and cost associated with paths between two nodes. Earlier while implementing greedy search we only accounted for associated heuristic value, not the cost associated with paths. But here we will combine the strategy of UCS accounting for cumulative associated cost with the path or backward cost which will denote the actual path covered till now as g(n) and Greedy Algorithm strategy of prioritizing forward cost, which indicates a closeness to the goal as h(n). From here we can formulate a search tree where each node is associated with g  as cumulative cost value to reach a particular node and h as heuristic value.A* prioritizes based on the value of g(n) + h(n) . Here g(n) is the cumulative cost, and h(n) value is of a particular node. The main question arises that when we should stop searching, at the time of enqueueing or dequeue of the plan containing goal. While enqueuing the plan we are not considering the minimum cost, and we cannot assure that the chosen plan is optimal. But at the time of dequeue, we would have many sub plans in the fringe and the optimal one can be chosen accordingly. Therefore at the time of dequeue, we can ensure an optimal solution. All the Algorithms discussed so far only differ based on how they are prioritized.DFS is prioritized based on finding the deepest node in the frontier, BFS on finding the shallowest node at a level. While UCS is based on cumulative backward cost, in greedy search heuristic function is considered and in A* search, we use summation of both backward and forward cost.
Before commenting on the optimality of A* search, let’s consider an example. Suppose we have a directed graph defining heuristic value at each node. Two possible plans are connecting the start state and destination, one with the least cumulative cost but with very high heuristic value for the intermediate node even greater than the actual path cost. While others have higher cumulative value compared to path 1. In this, A* is expected to pick the optimal path, but it has selected the path with a higher cumulative cost.It went wrong because the heuristic function of an intermediate node of path 1 has a very high value than the actual path cost,so basically, the heuristic function started over-analyzing which led to a bad solution.A* can work optimally by putting some constraints on the heuristic function in such a way that the value of the Heuristic function at the particular node should be less than or equal to the actual cost required to reach the goal from that state.Such Heuristic functions which underestimate the actual distance to be covered is known as Admissible Heuristic. Inadmissible Heuristic overestimates the actual distance to be covered to reach the goal.If we have multiple nodes to reach the goal then the heuristic function h(n) is admissible if its value is less than or equal to the actual cost of the shortest path and greater or equal to zero.
Let’s try to prove the optimality of A* search when we might have multiple goals. Suppose we have a tree with two Goal nodes A and B.A is considered optimal while B is considered sub-optimal and h can be defined as an admissible heuristic function. Let n be the predecessor node of optimal goal node A. Here we need to prove that optimal node(A) will be considered first while dequeuing the fringe. Let’s presume B is already in the fringe and will proceed further where f(n) is the combination of both g(n) and h(n) , since h is an admissible, value of g(A) is greater than or equal to f(n) and at node A h value is 0 ,so f(A) is equal to g(A).So from here, we conclude that  f(A) is greater than or equal to f(n). The value of g(A) is greater than g(B) because B is a suboptimal node and h value at both goal states will be 0. Therefore we can say f(A) is also greater than f(B).From the above to proven point we can claim that node n is explored before node B.This proves that all the predecessors of the A will be expanded before B.Hence optimality of A* search is proved when h is admissible.
On Comparison with UCS ,A* is expected to search less for goals because UCS rely only on the backward cumulative cost and will explore uniformly in all directions but A* considers additional heuristic functions to avoid blind search which ultimately increases the computational cost of each node , and can be optimal if Admissible Heuristic function is used. Better Heuristic Functions which can provide higher accuracy in estimations are more complex and possess higher computational cost for each node.UCS will explore almost in all direction but end up finding the optimal solution, while greedy search explores nodes towards the direction of goal , but might not promise optimal solution but in the case of A*, it is expected to explore less than UCS but greater than Greedy search and will ultimately reach the optimal solution.
A* has several applications like designing video games eg-Pac Man, used in the analysis of language-searching complete sentences from the information of two or more words,recognition of speech and translation of some text based on available literature etc.
While creating Admissible Heuristic functions we somehow try to relax the problem by removing some constraints,so we can come up with different action plans.Eg supposes we have a map connecting two cities with several intermediate cities. Instead of traveling between different cities to reach the goal via road,we can try taking straight-line path to the goal directly from the start state via flight.In such scenario, we cover less distance compared to actual path distance, and this can be termed as an admissible heuristic. Even though Inadmissible heuristics might not provide optimal solution, we still use in certain problems where we need to find the solution quickly and where the solution is important not optimality.
Example- Suppose we have a 3*3 puzzle with 8 titles numbered 1 to 8 and one empty block.We aim to achieve a state where titles are arranged sequentially(1,2,3….) in a horizontal manner with an empty state at beginning.Here total states will be 9 factorial. Successors can be 2,3,4 in a particular state and actions will be sliding of nodes into empty space according to the strategy. And we can define less cost to the node nearer to the goal.We can define the heuristic function as a number of titles incorrectly placed from its ideal location. And this Heuristic can be admissible if we relax some constraints on tile movement.We can comment that average node expansion in the case of UCS will increase exponentially(because of blind search) when we are 4,8,12…. step away from ideal location compared to A* search(because of Heuristic Function). 


References:
1. http://ai.berkeley.edu/lecture_slides.html
2. https://www.geeksforgeeks.org/search-algorithms-in-ai
3. http://www.cse.iitd.ac.in/~mausam/courses/col333/autumn2019/lectures/02-search.pdf
4. http://how2examples.com/artificial-intelligence/tree-search#:~:text=Tree%20search%20algorithms%20attempt%20to,nodes%20in%20a%20systematic%20way.