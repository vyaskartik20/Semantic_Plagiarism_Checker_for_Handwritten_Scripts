B18CSE014
Ekagra Das
ONLY the lecture recordings was used for reference for these notes, with timestamps referring to relevant places in the recordings.

Lecture-01-2:
[00:00] What is unusual about futuristic Sci-Fi movies?
They are fictional, far from reality, the presence of self conscious AI in them.
Though we stil like them cause we want it to happen someday, and we usually try to escape from reality.
The main objective of AI is to make machines that can think like humans, act like humans and even be better than humans.
[05:46] In this course, being rational would be in the context of achieving a goal, and how it will affect the environment and accounting for relevant obstructions in the process. In the second half of the course, we will see how we can express it in some sort of value, ie in terms of utility. Utility here is a measure of the outcome.
[13:10] Imperfection of the brain - Though the human mind can make rational decisions, it isn't perfect. We can't break it down in terms of algorithms, but can only take inspiration from how it works, to create such algorithms, just how we took inspiration from birds to create airplanes.
[15:17] In the present times, basically AI can perform technical/objective tasks which can be performed using a set of instructions, but not tasks that makes use of emotions, or information that is very locally available.
[19:08] Some examples where machines are good - Speech related tasks like audio recognition or Text-to-speech synthesis, Language and image processing - tasks based on dialogues, or object and face recognition.
And coming to Robotics, physical tasks like walking, driving, painting and such are possible though we have to remember that reality is more difficult to control than in simulations.
[21:30] AI evolves with the help of logic and methods like Deduction, Constraint satisfaction and Satisfiability solvers.
And how does applied AI make decisions? By scheduling, route planning, medical diagnosis, web search engines, spam classifiers, automated help desks, fraud detection etc.
[22:54] What is a Rational Agent? It's an entity that perceives and acts, to maximize its expected utility.

Lecture-02-1
[05:00] Agents that Plan - Should be careful about their capabilities and how their surroundings will act in response to their actions.
Reflex Agents - These agents act based on what they see right now and they don't consider the consequences of their actions. They may or may not have 'memory' ie the ability to store data, like a past state.
[09:00] In Pac-Man, the ideal objective is to eat all the points as quickly as possible. So we start randomly and eat the dots, and keep on exploring the environment. Or we can explore the environment as much as we can and on the basis of that plan the path.
So basically these agents try to understand the consequences of the resulting actions, and decide and act on their path.
[13:20] Optimal vs Complete planning - Optimal is the best plan, and complete will make sure you reach the goal although it may not be the best plan, like in Pac-Man, going directly to end location, may fulfill the goal, but not collect the max possible dots, hence it's not the best.
[19:17] Search Problem - We define it using a state space(possible states), a successor function(contains primarily the actions and costs, talks about how the agent moves in a given state space), a start state and a goal test(when all the dots have been eaten - multiple states possible in this context).

Lecture-02-2
[01:11] In Search Problems, we try to model out the environment to discretize different values, to plan and test upon.
Example of Travelling in Romania - For the shortest path, the solution is simply the smallest path between start state and goal test.
[05:32] Pac-Man has two problems in it, one is a Pathing problem and the other is trying to Eat all the dots problem. Both the problems, will have different State space, Actions, Successor function and Goal test.
Pathing Problem - States:(x,y) coordinate, Actions:NSEW, Successor function:Update location, Goal Test: x,y coordinates = END
Eating all the Dots Problem - States:(x,y) coordinate and a dot boolean, Actions:NSEW, Successor function: Update location and the dot boolean, Goal Test: Dots are all false.
We can see that the Goal Test may not always correspond to a state or a condition, hence its a test and around it, the solution and objective revolves.
[13:22] To find the total number of World States in the given example - (Agent Positions) x (2(if dot eaten or not)^Food count) x (Ghost positions^2(if ghost there or not)) x DirectionTheAgentIsFacing(4 in this case-NSEW)
In the pathing problem, we are only concerned with the position of the agent, hence the total states in that context is 120, that is the total number of positions of the agent.
In the dot eating problem, we are concerned with the position of agent aswell as if the dots are eaten or not, hence the total number number of states is TotalNumberOfPositionsofAgent x 2(if eaten or not)^Food count.
[26:11] Problem - Eating all dots + keeping the ghosts scared - The state space will have to keep in account, the total agent positions, for each dot if its eaten or not, for each power pellet if its eaten or not, and the remaining scared time of the ghosts.
[29:00] State space graph - We can basically represent the whole state space in the form of a graph, where each node(state) may branch into further nodes(states), cause of the actions they perform(NSEW), and each state will occur only once in the graph. Upon considering all the possible states we will finally get a graph, but it will require a lot of memory which isn't feasible.

Lecture-03-1
[03:11] A Search Tree is different from a Search graph, it may not contain all the states, but it contains the paths for a sequence of states, where each child node corresponds to the succeeding state from a node. The starting state is the root node here and usually, we don't build the whole tree, but only search upto a few levels, cause it's not feasible memory-wise.
[04:35] In State space graphs, we show all the possible states and also show the transitions between them and in Search tree, we show the paths, between any two particular states and repitions are possible, and with this we can trace the path easily between two states. In State space graph, if we have to check for a path, we have to put conditions so that we don't get stuck in loops, whereas in the tree, we just have to check that we don't repeat any vertix.
[09:08] When we search with a Search tree, we can see the locations where we can go from our starting location/node, and these locations will become the child nodes of this starting root node. And we further expand these nodes, and which node is to be expanded is decided by the algorithm that we use. We also keep track of possible paths to consider, using a fringe, which is a queue containing the child nodes, from which the next node to be expanded is chosen by the algorithm, and the already expanded parent node is moved from the fringe node to an older queue containing all the expanded parent nodes, and is replaced by its child nodes in the fringe. Basically a fringe is a queue that is visible to the algorithm, but not yet expanded.
[15:00] General Tree Search(PseudoCode) - Its basically a function which we perform with the knowledge of what the problem is, and what algorithm/strategy we will be using. It will either return us the path or tell us that no such path that satisfy it exist. We start it by using the initial state of the problem, and then perform a loop where we check for available candidates(nodes) to expand, if none exist, then no solution. After that we choose a leaf node from the Fringe according to our algorithm/strategy. If we reach the goal test(state), we return the solution, else we further expand the node and add the resulting child nodes to the tree, and then repeat the loop.
[21:30] Depth-First-Search - We perform DFS on the tree, and the path that we come up with is indeed a solution, hence its a complete path. But we can't comment if its the optimal path or not.

Lecture-03-2
[00:39] Search Algorithm Properties - For a tree of 'b' as branching factor and 'm' as the maximum depth
In DFS
Time complexity will be O(b^m), which will be the worst case scenario (1,b^2,b^3,...,b^m).
Space complexity will be O(bm) because in worst case scenario, the longest path will go upto 'm', and for each node that we expand, it will have 'b-1' sibling nodes that are yet to be explored, which we have to keep an account of, so that the algorithm can know what nodes are next to check or been checked, hence O(m+(b-1)m) = O(bm).
[22:36]In BFS
The Time complexity will be O(b^s) where 's' is the level of the goal.
The Space complexity will be O(b^s) because while scanning a specific level we have to keep all the nodes in our memory to expand later.
And its a complete search since s must be finite, for the solution to exist.
And it's optimal in the scenario when the costs are 1 and by cost it means the cost of moving from one node to the next node.
[25:46]When to prefer DFS or BFS? BFS when the depth is shallow, and DFS when the depth of goal is deep.

Lecutre-04-1
[00:00]General tree structure - First child, next sibling links
[08:59]DFS step by step -
Start from root node, then look at children, and we have to keep track of these children. So we push S(parent node)->d, s->e, s->p in the fringe stack.
Say we pick d from this stack, so we take out s->d from the fringe stack. Now from D, we can goto b,c,e, so all these 3 parts we need to push in the stack, so now our stack will contain 3 more entries, ie s->d->b, s->d->c, s->d->e. From this we pick s->d->b, which we will say is on the top of the stack.
Since b has 1 child node a, we push s->d->b->a in this stack. Now we are done with this part, but still goal not reached, but since there are more options left to explore, we take out the next option s-d-c, and similarly, s-d-c-a. Now since we still havent reached the goal, we pop out the next option from our fringe stack. 
Now we have 2 children from e, so we need to push both of them to the fringe stack, ie s-d-e-h and s-d-e-r, in our stack. Now we pop the first one giving us two options. Now we put both of them too in the stack, and pop the first one and finally get s-e-h-p-q but still goal not reached. Similarly we keep expanding based on the options in our fringe.
Eventually we get s-d-e-r-f-g. 
When we get this, we put it into the fringe stack, but DONT STOP here. 
We only stop, when we push on the fringe and the last node of that corresponds to the goal.
[22:39]Iterative Deepening - Basically we use the DFS's space advantage alongwith BFS's time advantage.
We run DFS with depth limit from 1, and increase it 1 by 1. And it is pretty useful because most of the solutions are usually found in the shallow levels.

Lecture-04-2
[00:00] Cost Sensitive Search - 
BFS doesn't find the least cost path, but the shortest path in number of actions.
Uniform Cost Search - In this we keep a track of the cost too.
We first start with the choice between s-d (3), s-e (9), s-p (1). We choose s-p cause of the least cost, then we further check s-p-q giving value 16 in total.
Then check between s-d (3), s-e (9), s-p-q (16). We choose s-d (3).
Then between s-d-b (4), s-d-c (11), s-d-e (5), s-e(9), s-p-q (16), we choose s-d-b(4).
After choosing it, we go to the next node from b, ie a giving s-d-b-a(6).
Now in the fringe we choose from sdba 6, sdc 11, sdeh 13, sder 7, se 9, spq 16.
We choose the least one that is, sdba 6, and realize that a is not our goal, so we drop it.
Then we check the next least, sder 7, we get sderf 8, and upon expanding that since its still the least, we get sderfc 11 and sderfg 10.
We find that sderfg 10, has G that is our goal, but we will NOT STOP here, and continue to explore. 
After this we check and find that se 9, is the least one, giving two more options seh 17, ser 11. Now after checking all options in the fringe, we find that sderfg 10, is the least cost, so we take it out of the fringe and it becomes our solution.
Basically we don't immediately stop after encountering G, cause there is a possibility of another path having lesser value we can encounter later upon exploring.

Lecture-05-1
[00:47] Uniform Cost Search Properties -
Let cumulitive cost of optimal solution is C*, and the minimum cost of each arc is let E. Then at each step we would be reducing the cost by a factor of E and can say that we'll require C*/E steps or depth in the search tree.
Time complexity is O(b^(C*/E)) same as in BFS.
Space complexity is O(b^(C*/E)) same as in BFS.
For it to be complete search, the solution should have a finite cost and the minimum arc cost should be positive.
This algorithm looks at all possible paths, and since we explored all possible paths we can be sure it's optimal.
[14:20] The solution that we get is optimal as we explore on the basis of increasing costs. But the disadvantage is that we explore in every direction. Like if we are at a position and we have to reach another position, we simulate all possible moves in all directions to move from current position to goal, and we don't use any possible information we have about the goal in doing this. Hence this blind search is an uninformed search algorithm.
[17:10] The One Queue - In all these searches we use some form of a queue. Whenever we expand a subplan, we get new subplans and put them into a fringe. And how we operate on this queue, depends on the algorithm we are using.
In case of DFS, we use the LIFO method and in BFS, we use FIFO. In practice, we implement a generalized version of priority queue and we assign priority on the base of the algorithm being used. In case of UCS, priority corresponds to cumulitive cost. In case of BFS and DFS, we assign a timestamp with each subplan. In DFS - LIFO, pick the subplan of the least timestamp value and for BFS, the maximum timestamp value.
[20:00] The agents, create a virtual model of the world, and run their simulations and tests in it, instead of the actual world. The search operations occur in these models, and are as useful as the more detailed the model is. Like for example, in google maps, it may contain a path traversing through private property which leads to trespassing, if we use them. But the agent may choose this path as it doesnt have necessary information and things may go wrong.

Lecture-05-2
[07:10] Pancake problem - Can only flip a specific number of pancakes from the top, not from middle.
We can consider each configuration as a state in the search graph, and start from the start state, and each flip will act as an action that changes the state.
And for cost, it can simply be the number of pancakes flipped in that specific action.
We can see this problem, like N integers, and we have to arrange these in ascending order by flipping them. Another thing to note is that the same configuration/state can be repeated by flipping the same way.
[19:30] The Tree Search function to be used here is the same as Lecture 03-1 General Tree search function.
The Videos of Demo Counters UCS shows how searching in all directions is a waste, instead of searching in a specific direction with the knowledge of location of the goal.

Lecture-06-1
[10:00] Informed Search -
Heuristic Function - Takes a state as an input and returns a value that shows how far we are from a goal/goal state in Manhattan distance, Euclidean distance etc. Unlike the generic search tree/ search graph used to search , in Heuristic approach, a single Heuristic may not work every time.
[18:39] In the map, the Heuristic value provides guidance towards the goal, it may not provide the optimal path, but just shows a value of how close you are to the goal, to help us choose which place to go first based on which one has a lower Heuristic value, although it's not optimal.
In the Pancake problem, the size of the largest pancake that is out of place could be the Heuristic value.
[25:20] Greedy Search - We try to get the best that we can get out of a current state, but we may not get the optimal solution in the long run. We keep choosing the child node, with the least Heuristic value greedily, till we reach the Goal state.

Lecture-06-2
[02:00] The strategy in Greedy Search is to keep expanding the node that we think is closest to the goal state with the help of the Euristic Value.
Although most of the times we can notice that the best first greedy step, may lead us to the wrong path/goal.
The Worst Case scenario, will be a badly guided path, where we may end up doing more work than in BFS or DFS
In the PacMan small maze, the agent, will choose the direction with the least Euristic value, then as it reaches a dead end, it will go for the second least possible value path, when that also reaches a dead end, it will double backtrack and go for the third least possible value path, and eventually reach the goal state. This is an example of a Reflex agent.
[21:30] A* Search - Combining UCS and Greedy
A function g(n) would be showing the Uniform cost on the basis of path cost.
A function h(n) would be showing the Greedy value on the basis of proximity to goal.
A* Search orders by the sum of g(n)+h(n)=f(n)

Lecture-07-1
[03:40] When should we stop A*? When we dequeue a goal.
In the example, the starting position is S, and we have to decide between A and B to goto, we choose B, cause it has a lower h value and both have same g value, so B's h+g value is lower hence we chose it. After that we goto G from B, hence A-B-G, making up 5 cost value.
Then we dequeue the goal and stop. This however isn't optimal as SAG has 4 cost value.
[14:05] In this value, at S, we have two choices between SA and SG, SA has a cost value of 1+6=7, and SG has a cost value of 5+0=5, so we traverse via SG, then dequeue the goal and end it. But this isn't optimal as SAG, has a total cost of 4. Hence A* is not optimal in this case.
In both the examples the Actual 'bad' goal cost < estimated 'good' goal cost.
[24:00] There are two types of Heuristics, Inadmissible(pessimistic) and Admissible(optimistic).
Admissible heuristics never overestimate the cost of reaching the goal and Inadmissible heuristics try to break the optimality, and traps good plans in the fringe.
Admissible heuristic - 0<= h(n) <= h*(n), 
Where h*(n) is the true cost to a nearest goal.

Lecture-07-2
In the Pancake problem, the Heuristic value, is the number of the largest pancake, which is still out of their place.
We'll the call the Heuristic function as admissible Heuristic if the Heuristic value is less than or equal to the true minimum cost.
[21:00] Optimality of A* Tree Search -
Let a tree, where A is the optimal goal node and B is the suboptimal goal node, and h is an admissible Heuristic.
Since B is suboptimal the cost of RootNode-B is more than RootNode-A.
In order to claim that A* search is optimal, we have to prove that plan leading to A will come out of the fringe before the plan leading to B
Proof - 
Let path leading to B be in the fringe
Let Some ancestor N, of A is in the fringe aswell (A could be too)
The claim is that N will be expanded, before we expand B.
(1) F(N) is less or equal to F(A)
That is F(N) = G(N) + H(N) [definition of the F cost], and F(N)<=G(A) [ because H is admissible Heuristic ]
And G(n)+H(n) <=G(A), and since H=0, cause it's at the goal
G(A)=F(A)
(2) F(A) < F(B)
We know that G(A) < G(B), cause B is a suboptimal goal.
And F(A) < F(B), since Euristic value H=0 at the goal.
[27:00](3) N expands before B - because of (1) and (2), and when N and B are both in the fringe and we compare their F() values, the N will have less value and will come out of the fringe before B.
From here we can deduce that path to A will expand before path to B, hence we can conclude that the A* search here is optimal.

Lecture-08-1
[00:45] Properties of A* Search -
We compare UCS and A*, we find that A* is expected to search less as compared to UCS.
In case of UCS we don't make use of a Heuristic function and simply rely on search, and in A* we make use of Heuristic function, and if the Heuristic is admissible, it will show Optimality, however it increases computation because we are doing additional compution in respect to a given node.
[06:30]UCS vs A* Countours -
UCS expands, in all the directions equally, whereas A* only expands towards the goal, ensuring optimality.
A Comparison between Greedy-UCS-A* in PacMan - we see that in Greedy, the agent rushes in the direction with the least distance, and the solution isn't optimal, whereas in UCS, it searches almost everything, leading to a nonoptimal solution and in A* it doesn't search everything as in UCS, but not tunnel vision as much as in Greedy, and give us an optimal solution.
[13:40] A* Applications - 
Plenty of applications, like in games, pathing, routing, planning, motion planning, language analysis, machine translation and speech recognition.
[22:00] Creating Admissible Heuristics -
Let's say the Euristic that is in use is very fast, but is inadmissible, and the admissible Euristic performs alot of computation and is slow. Inadmissible Euristic is mostly useful when we want a solution regardless if its optimal or not, but we want it fast. 
Admissible heuristics, are often solutions to 'relaxed' problems. These are where new actions are available, and we drop constraints.

Lecture-08-2
[05:20] Example - 8 Puzzle -
In this 3x3 board, there is a Start State, the action is simply moving a the empty block NSEW, the goal state, and each possible configuration belongs in the state space.
For the given Start state, we can see that there are 4 successor states of it, but for a different configuration we can see that it can have 2 successor states.
A reasonable Euristic for this puzzle, would be the total number of tiles that are misplaced in each configuration from the goal state. This is a relaxed problem, as we check the Heuristic value without taking the constraints in consideration. Hence it's an admissible Heuristic.









