																				Artificial Intelligence Bi-weekly( sept 1 to sept-11 )
	Now-a-days we see AI in every aspect of our life. Human beings constantly discover ways to make life easier and the introduction of AI is not different. We see how movies depict scenes which as of now,seem unreal and unachievable, but can be achieved to some extent. A broad objective of AI is to make machines think, act, do things better than humans making life easier.
    Rational thinking is key in AI systems. A rational agent typically maximizes its expected utility, given its current knowledge. By utility we mean usefulness of the consequences of its actions. The utility function is defined by the programmer arbitrarily, but should be a function of "performance", which are the directly measurable consequences, such as winning or losing money. An agent might be rational within its own problem area, but finding the rational decision for arbitrarily complex problems is not practically possible.
    The objective of any AI system or agents is to maximize expected utility. When people talk about intelligent machines they try to take inspiration from the human brain which might be work or not work. What we know by learning about the brain is memory and simulation are key to decision making.
    "Brains are to intelligence as wings are to flight", which means we didn’t reverse engineer birds to create airplanes instead we studied the mechanics of flight and used this study to build wings and eventually were able to build airplanes. Likewise, we don’t need to reverse engineer the brain to create AI. We just need to understand the mechanics of intelligence and then we can build much faster and more powerful AIs.
    The mere ability to mimic human behavior is considered as AI. Intelligence can be defined as a general mental ability for reasoning, problem-solving, and learning. So what can AI agents do as of today? They can win a chess game against a grandmaster, they can scan through millions of documents and almost accurately separate them based on spam or promotion or importance, they can drive safely on a less crowded road. One example is at facebook where two bots were made to settle a deal with each other, but where the true concern appears is how the AIs deviated from the task in an unexpected way. The language they spoke was diverged from human readable form but they did finish the task. So AI can be unpredictable at times. Machines are quite good at some tasks like in Speech technologies,language processing techniques etc..
    An agent can be either a machine, robot or any non-living thing.Agents can be Rational, Planning.The simple reflex agents don’t specifically search for the best possible solution, as they are programmed to perform a particular action for a particular state. On the contrary, the artificially intelligent agents that work towards a goal, identify the action or series of actions that lead to the goal. The series of actions that lead to the goal becomes the solution for the given problem. Here, the agent has to consider the impact of the action on the future states. Such agents search through all the possible solutions to find the best possible solution for the given problem. In planning we have optimal and complete planning where optimal means best possible solution to reach goal and complete means just reach the goal but find a solution.So every optimal solution is a complete solution but not vice versa.	
	Search in AI is the process of going from a starting state to a goal state(goal test) by navigating through intermediate states. Almost any AI problem can be defined in these terms. State — A potential outcome of a problem. Transition — The act of moving between states. A solution to this search problem is a sequence of actions(a plan) which transform the start state to goal state or goal test.
	When we work with a search problem we make approximations on the environment and come up with a model. A search state keeps only the details for planning.For example, if we take a problem of eating all dots in a pac-man game our states are {(x,y),dot boolean}, Actions are move up,down,left and right, Successor would be next location and possibly a dot boolean and goal test is all dots should be false.A world state keeps track of every last detail of the environment.
    These search problems can be thought of as state space graphs where every node represents a state and edges represent actions and an important detail is that in graphs each state occurs only once.
    One other way to represent these are search trees.In this every state may occur multiple times.The state state is root node. Children correspond to successors. Each node in the search tree is an entire path in the state space graph. There might be a chance that we explore the same paths many times so we restrict till how much depth we are exploring. We expand potential plans and also maintain a fringe, which tells us what are the unexplored nodes.
    There are two kinds of search, based on if they use information which is ahead or not and they are Uninformed and Informed search..............
    Uninformed search does not use any domain knowledge. This means that it does not use any information that helps it reach the goal, like closeness or location of the goal. The strategies or algorithms, using this form of search, ignore where they are going until they find a goal and report success. Let's first look at a few of them.
    Depth First Search (DFS) strategy is to expand a deepest node first and fringe is a LIFO stack. This search always gives a solution given it exists, finite and we take care of cycles and this may not be optimal but is complete.If branching factor is b and max depth is m then time complexity is O(b^m) and the space complexity is O(b*m) as at every level we keep track of its siblings.
    Breadth First Search (BFS) strategy is to expand the shallowest node first and fringe is a FIFO queue. It will also give us a solution if there is any and finite. The solution is Optimal if all costs are 1 and is complete anyway. Time complexity is O(b^m) as in the worst case the node we search for ends up at the last level and space complexity is O(b^m).
	What if we combine the best of both worlds i.e., we take depth-first search’s space-efficiency and breadth-first search’s fast search. This is what Iterative Deepening Search (IDS) does. It calls DFS for different depths starting with an initial value and for every call DFs is restricted to from going beyond this particular depth . So what it does is DFS in a BFS fashion. One important thing to note here is, we visit top level nodes multiple times. The last (or max depth) level is visited once and level before that is visited twice, and so on. At first it might seem expensive, but it turns out to be not so costly, since in a tree most of the nodes are in the bottom level. So it does not matter much if the upper levels are visited multiple times. This gives optimal solution just like BFS. Time complexity is O(b^m) as we may search the whole tree ans space complexity is O(b*m) as it uses DFS for every level.
    BFS finds the shortest path in terms of actions but not least cost path. Uniform Cost Search (UCS) is the same as BFS except that this expands the cheapest cumulative cost node which is in the fringe. Here fringe is a priority queue with priority of lowest cumulative cost. In this algorithm we should stop only if the goal we found is dequeued so that we might not miss any other optimal solution. This search gives an optimal solution and so a complete solution given it exists and finite. If the solution costs 'C*' and arcs minimum cost 'c' then the effective depth is C*/c. Time complexity is O(b^(C*/c)). Space complexity is also O(b^(C*/c)).However in case of negative cumulative costs this may not give us optimal solutions.
    All these algorithms are the same except the fringe strategies. We can implement a priority queue with priorities assigned according to the algorithm.
    Informed search uses domain knowledge. It generally uses a heuristic function that estimates how close a state is to the goal. This heuristic need not be perfect. This function is used to estimate the cost from a state to the closest goal.A heuristic is a function that estimates how close a state is to a goal. It is designed per problem. Examples like Euclidean distance and Manhattan distance are heuristic functions.
    Greedy Search is one of the algorithms for informed search which uses a strategy of expanding the node that seems closest with respect to heuristic value(estimate of distance to nearest goal for each state). This may not give an optimal solution but gives a complete solution if there exists any finite solution. This may be a boon in case of no obstacles and outperforms both BFS and DFS but gets worse when there are obstacles and end up being worse than both BFS and DFS.
    A* search can be seen as a combination of greedy and UCS where the algorithm benefits from heuristics of greedy algorithm and optimal solution from UCS. In this Uniform-cost orders by path cost, or backward cost g(n) and greedy orders by goal proximity, or forward cost h(n). For every node we have a g value and a h value. A* search orders by the sum f(n) = g(n)+h(n). A* algorithm should only terminate if we dequeue the goal state so that we might not miss any other optimal path. This doesn't give an optimal solution, if h(n) is pessimistic and over-estimates the cost. We need estimates to be less than actual costs. If this happens we can ensure that we get an optimal cost. 
	So, why does A* search gives us an optimal path? To prove this lets assume 'A' is an optimal goal node and 'B' is a sub optimal goal node and h is admissible heuristic. To prove this we just have to prove that A exits the fringe before B. For an arbitrary node n on the path from start state to A , f(n)=g(n)+h(n) and f(n)<=g(A), as g(A) is true cost. As B is suboptimal f(A)<f(B). Now if f(A)<f(B) then every ancestor node of A expand before B(f(n)<f(B)) and so we can say A also expands before B. Therefore, A* search is optimal.    
	Inadmissible heuristics bring sub-optimality by covering good plans on the fringe whereas admissible heuristics don't allow bad plans and never outweigh true costs. A heuristic 'h' is admissible if 0<=h(n)<=h*(n) where h*(n) is the true cost to a nearest goal. Manhattan distance and Euclidean distance in the case of pac-man game are examples of admissible heuristics. The admissible heuristics function depends on how we are assigning true costs between two states.
	In A* search we do additional computation w.r.t given nodes. the more better heuristic function the more complex and thus more computation is needed. Uniform-cost expands equally in all directions where as A* expands mainly towards the goal also checking all directions in the way.
	A* search has various applications like video games, Resource planning, speech recognition etc.. Most work we do in solving a search problem is in coming up with admissible heuristics. In most cases we relax the problem and find a solution which will be the admissible heuristics. In some cases we often use inadmissible heuristics where we are okay with suboptimal solutions.
        																	
																				"AI can do and will do"  -- Me


References:
https://hackernoon.com/search-algorithms-in-artificial-intelligence-8d32c12f6bea
Lecture slides and videos
https://www.geeksforgeeks.org/iterative-deepening-searchids-iterative-deepening-depth-first-searchiddfs/
Wikipedia
