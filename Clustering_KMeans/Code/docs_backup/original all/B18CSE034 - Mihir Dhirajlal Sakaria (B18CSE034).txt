AI which is short form of Artificial Intelligence is a technology being developed and used in day to day life. It is used to acheive specific goals in best possible way. We will be using the term rational here in a technical way. We can say rational as acheiving predefined goals maximally and it concerns what decisions are made and not the actual thought process behind it. Also goals are expressed in terms of the utility of the outcomes. Like for example i want to go from City A to City B in "minimum possible time" . So goal is to reach to City B from City A in minimum possible time. So methods like ,Shortest path from place A to place B , or a path with least possible traffic or best road condition and calculating the expected time for each method and giving the route which is give help us reach in minimum time.

Human brains are said to make very good rational decisions but not perfect. So AI basically tries to function like a human brain, but the structure may be different. For example, for an airplane, the basic idea of wings came from birds, but the design of wings of planes were different.
Various examples of what AI can do are
1. Play a decent tennis game
2. Give us best possible route from place A to place B.
3. Make a phone call based on speech recognition
4. Language conversion etc
5. Drive a vehicle
What it cannot do yet.
1. Converse with a person for an hour
2. think of new ideas like a human brain does.
All of the above examples are from broad terms like decision making,Game playing,Logic,Robotics,Vision,Natural Language,etc.

This was a broad summary of Artificial Intelligence.

AI mostly comprises of agents which help complete the predefined goals. For examples there is a machine which has to get an apple from the tree so the agent will do a precalculation or preplanning of how its going to jump with what velocity etc to get the apple.
There are some agents like reflex agents,Planning agents etc.
Reflex agents are the agents which take action on the basis of the current situation and dont consider the aftermath of the action. So this means that it is not necessary that the reflex agents are rational.
Planning agents are the agents which takes action after considering the consequences of the action which it is going to take. So it also must have a model in which the space/environment evolves on the basis of the actions taken. Important thing to note is the consequences of the action considered are based on the computational power and other factors.
Search Problems
A search problem consists of a state space, a successor function which contains the actions, costs etc and finally start state and goal state. So a solution is a sequence of the actions which transforms a start state to a goal test.
But what does a state space have? So essentially a state space keeps only the data/information needed for planning. So it depends on the objective. If we want to find the shortest path then it will have the information about the data about the current state like co-ordinates.A Successor function, which updates the current state and moves to next state and try to move towards the goal test. So in shortest path problem successor function will update the current state. Goal test can be if (x,y) == END;
A world state contains all possible information about the environment.
If we consider all possible states and link them corresponding to their next states, we will eventually get a graph or a tree but we can rarely build a full graph because it would not fit in the memory.
SEARCH SPACE GRAPHS.
A search space graph is a mathematical representation of a search problem.In a state space graph , the nodes represent the different configuration of world of problem. An edge denotes the action taken and leading to which node. And a goal test is represented by node(may more than 1). Important thing to note is that each space in state space graph occurs only once.


SEARCH TREES
These are different from a search graph. These may not contain all the states, but may contain several possible paths. Also there may be repeatition of states. We donot make a full state space tree because it might exceed memory. In case if there is a cycle in the graph then space tree would be infinite. Thus we dont calculate the whole space tree. Each node doesnot represnt a value or a place, but a path.
Also we take some steps so that we donot go into an infinite cycle or exceed memory.

So the formation of search tree depends on the algorithm with which we are expanding the nodes. For example if we are using breadth first search method, then all the neighbouring nodes of the nodes will be checked first and then proceed to next nodes where as in depth first search , we will take a node and expand till we reach the leaf node or the end point of search and then expand the different child of the root node.
There is a term called fringe which tells us what are the nodes that are yet to be expanded. 
Using DFS in a search graph to make a tree will give us a solution if it exists but may not be the most optimal solution. 
The general definition of Tree Search is :
function TreeSearch(problem,strategy) returns a solution, or a failure initialise the search tree using initial state of problem.
loop do
      if there are no candidates for expansion then return failure.
      choose a leaf node for expansion according to strategy
      if the node contains a goal state return the corresponding solution
      else expand the node and add the corresponding nodes to the search tree.
end
The leaf nodes correspond to the search tree's leaf nodes that are yet to be expanded(these are maintained by the fringe)	 

Search Algorithm Properties 
There are a few search algorithm properties like complete,optimal etc. An algorithm is said to be complete if it guarantees to find a solution. An algorithm is said to be optimal if it is guaranteed to find the least cost path.
So if a tree has height m and branching factor b at each node , then there are total of b^m nodes in the tree. 

So if we check the algorithm properties of DFS algorithm , taking the fact into consideration that we are taking care of cycle and no negative nodes are there,
Complete - Yes DFS algorithm will be complete. 
Optimal , not necessarily.
maximum time complexity = O(b^m)  and space complexity = O(m) because if we use recursive approach the maximum height which can be reached is m. so only m nodes will be stored.
We also have to keep a track of each element if it is being repeated again so that there is no cycle. Thus it will require extra space.  Also this extra space will be less than the total nodes because if we are exploring a single child of a node, then we dont have to keep track of other child nodes of the node which we are exploring. So the space complexity will be O(bm). It would have been O(m) if there were no cycles. but since we are taking care of cycles too so in total we will need O(bm) space.

Now if we check the algorithm properties of BFS algorithm, then 
complete- YES 
optimal - Not neccesarily.
So BFS expands nodes which are only above the shallowest solution and let its height be equal to s. So the search time will be O(b^s);
The space taken by fringe will be O(b^s)

On average DFS has better space complexity than BFS. 
So if the solution is close to the root node(in upper layers) then BFS will outperform DFS in most cases 
else if the solution is deep down the tree, then  DFS will outperform BFS.

So we can calculate the search state tree using a parent first child tree and maintain fringe using LIFO stack. So whenever we traverse a graph we push all the child nodes in the fringe stack and then explore the top element of the stack and repeat the same push and explore the node until we find the final answer. Note that we will not stop the process when we push the node, but we will stop the process when we are popping the node from the fringe. This has important meaning covered later.

There is a combination of both dfs and bfs which can be used to have dfs's advantage and bfs's advantage at the same time. 
Set the initial height as 1 and then run a dfs till that height. if we find the node then close the program. if not then increase the height by 1, and again run dfs till that height. 
Repeat this process till we find the solution node. This is wastefully redundant but will result in solution earlier on average.

We used to the above traversal methods to find the goal node with minimum number of actions, not least cost path. Now we will do a cost sensitive search.
Uniform cost search which will expand the nodes on the basis of the cumulative cost. Here comes the part that why we stop the program when we pop from the stack. This is because here, if we pop it as soon as we get to the node we might miss the other path which has lesser cost than the present cost of reaching here .Thus we terminate the program when we pop the goal node from the stack.

Algorithm properties of uniform cost search algorithm
lets say the cheapest cost is C* . and the cost of least edge is e. so effectively there will be C*/e edges in between. So depth till which we will be calculating will be C*/e. 
Thus the time complexity will be O(b^(C*/e)) and space complexity will be O(b^(C*/e)) 
If the cost if finite and the edge cost is positive then yes it is a complete solution.  
And yes it is the optimal solution. But if the edges were negative too, then we might get a solution which may or may not be optimal.  The disadvantage of the uniform cost solution is that we have to search in all the directions and does not give information about the goal location .

Conceptually if we see, then the fringes are basically priority queues. So the priority for every method is different . Like in Uniform cost search , the priority was minimum cost . In DFS and BFS we can assign time stamp for the nodes and remove it from the fringe based on minimum and maximum timestamp for DFS and BFS respectively. This can be done using a single priority queue. 

Sometimes there can be different errors too. Sometimes the model may have insufficient information. for example google maps gives shortest distance from place A to place B. But it might give a solution through the private property of a person and thats not allowed in real life. So thats wrong.

An example of how we can convert some problem into a Search State Tree. Lets say we have a pile of pancakes of uneven sizes. We can flip only a top x number of pancakes. So we have to flip those pancakes until all of the pancakes are sorted from top to bottom. We have to find the minimum number of pancake flips to do that. So lets say we assign cost as the number of pancakes flipped in a single go. So that way we have n-1 possible states given n were total number of pancakes. Thus we have State Space Graph with weights of edges as the cost of flipping the pancakes. Then we can do Uniform cost search to find the optimal solution.

Most of the above search algorithms and methods were uninformed searches because we didnt knew where we were going. Just trying to get the best solution. 
Now Moving towards the Informed Search in which we will be having the direction of the goal state and proceed towards it.
Heuristics :
A heuristics is a function that estimates how close we are to a goal. A heuristic doesnot tell us anything about the solution. It only gives us estimate about how close we are to our goal. So following a heuristics function is like a greedy method, which may not lead to the goal state.
So in pancake problem , we can assign heuristic value as the the largest pancake which is out of place. But in certain states we cannot decide if we are moving towards the goal or away from the goal as there can be two different states in which 3rd pancake is out of place.
Idea behind this method is based on greedy approach. We expand the node which seems closest. Rest of the approach is the same of State tree traversal. Just the successor function is changed. It may happen that we may end up doing more work than a normal bfs or dfs. So worst case possible scenario occurs when there is a badly guided dfs. For example if we use the greedy method and reach very close to the solution but there is no further path there to explore then we will eventually end up doing more work. We can say the greedy method is a reflex agent which does not take account of the consequences of the actions taken.

We can combine the advantages of both greedy and Uniform cost search algorithms. For example we assign two costs in it. The heuristic cost denoted by h(n) and path cost which can be denoted by g(n). So we will move foward by selecting the minimum sum of g(n) and h(n) i;e g(n)+h(n) should be minimum. This algorithm is called A* search algorithm. The rest is the same , just we have used the Queue method as we discussed earlier. Also we dont stop the program/algorithm when we enqueue the goal node. We stop it when we dequeue it else we would miss the optimal solution. In different methods the value givben by g(n) and h(n) will be different. The disadvantages may be that the agent might over estimate the cost to reach the goal node and it will be biased towards one function, hence giving us the wrong solution. So a solution to this might be that we impose a constraint on the heuristic function ,that its value should not exceed the value of actual cost(value of an edge or something like that). In other words the value of h should be optimistic . Such a heuristic function is called admissible heuristic function.
A definition would look like 0<=h(n)<=h*(n) where h*(n) is the true cost to the nearest goal. 

In case of pacman problem, we define the heuristic function as the euclidian distance or the manhatten distance . In case of pancake problem we can define the heuristic function as the largest pancake which is out of order.

Optimality of A* search. 
Imagine B is on the fringe, and A is also on the fringe. Also given that B is suboptimal solution and A is optimal solution. Now we consider an ancestor n of A. So n has already been expanded. Thus n expands before B. Thus we can say f(n) = g(n)+h(n), and f(n)<=f(A) and since B is suboptimal solution, then f(A)<f(B). thus we can say that f(n)<f(B) . and hence A will expand before B. hence we will get the optimal solution before any other suboptimal solution.
We can say that the amount of search done by A* search will be less than the uniform cost search. This is because, in A* we know the direction in which we might get the optimal solution. But in Uniform cost search we dont know the direction of the search, so we end up exploring more values than the A* approach.
Examples of A* applications can be video games, pathing /routing problems , resource planning ,machine translation, speech recognition etc.  

Creating Admissible Heuristics 
In most of the difficult search problems ,solving them optimally requires finding admissible heuristic function. Like it may be a case where our heuristic function is admissible, but takes a lot of computational time, or may not be admissible but takes less computation time. So if our objective is just to find a solution in less time, then we can do a A* search with an inadmissible heuristic function provided that heuristic function takes less computational time.
Often the admissible heuristics are solutions to the relaxed version of actual problem. Lets say we had to go from a place Z to place C. In the actual problem we are supposed to go there through some of the given nodes while the admissible heuristic might calculate the euclidian distance of place Z to place C. So if we relax the constraint of moving through the nodes and allow the agent to directly move from place Z to place C., then it will give the optimal solution. 

Looking at another problem which is the 8 puzzle. In this, we have a grid of 3X3 and have 8 numbers from 1-8 and one empty slot. Now we can move a number to an empty slot to change the configuration of the grid. So given a state we have to move to a state in which we have empty slot as the first position and rest of the numbers are sorted in ascending order. In this case we may define the heuristic function as the number of slides(or numbers) which are displaced from its desired location, but there can be many states in which the heuristic values are the same. This heuristic function is admissible if this a relaxed problem , like the constraint of sliding the slide to the empty location is lifted and we can directly move a slide to another. A statistic provided by Andrew Moore states the number of nodes expanded on certain number of steps to move. In UCS(uniform cost search) for 4 , 8 and 12 steps we had to expand 112,6300,3.6*10^6 nodes respectively. But if we use A* search algorithm and use this heuristic, then for 4,8 and 12 steps we had to expand 13, 39 and 227 nodes respectively. This is significant change in optimization. 

The resources used for the preparation of the nodes are :
video lectures uploaded on the google classroom taught by Dr. Yashasvi Verma ,faculty at Computer Science Department IIT Jodhpur. 
Notes are from 1st september to 11th september. 