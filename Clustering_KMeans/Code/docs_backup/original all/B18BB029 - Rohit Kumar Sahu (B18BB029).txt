Name: Rohit Kumar Sahu
Roll No. : B18BB029
Mode-1 notes from Sept 1 - 11


Before going to Artificial Intelligence, let's first see what intelligence. Intelligence is the capacity to learn from prior experiences, and solving problems.( dictionary meaning). Wouldn't it be awesome if the computers were capable of thinking? , That's exactly what AI is. We humans have knowledge inherent to us, But if machines were able to think, this would be artificial.The fundamental feature of AI algorithms is the analysis of the data. How do you believe people are learning new things? They listen, They observe and they learn that way. Equally, the computers understand. 


The definition of AI can be in many ways, it depends on the way we classify them whether by behaviour or thought process or performance. We expect they have to behave rationally, which means achieving goals. They concern what type of decisions are taken in attaining those goals and expressed as the utility of the outcomes. Therefore a better title describing this course would be "Computational Rationality." They may or may not perform many functions like they can play table tennis, put dishes and fold laundry, translate language but they may not write an intentionally funny story because they must be taught the emotions like funny, angry but it is not possible to train in these aspects. There are fields where AI is good at including Natural Language, where they are master at speech technologies, Perception related, Image processing like detecting images and as robots, which are the face when we discuss robots. They are good at these game playing, logic which a robot performs so the word AI describes as a robot for freshers.


An AI system is composed of action and environment where actions are performed in their environment. Agents are essentially a machine that serves as a perceiving and acting agent. So, a rational agent prefers actions which maximises its utility. Factors like the environment affect the utility. Agents are basically divided as 1.Reflex agent and 2. Planning Agent. A reflex agent is the one where agents act on what they visualise currently. They don't think of the consequences as they don’t have memory to store the utilities, as they are named reflex(act as per situation).


A Planning agent is the one who understands the consequences as they store the goal and make track of that. They further divide into 1. Optimal and 2. Complete. Optimal is where the goal is achieved with less cost or best way whereas complete is where goal is achieved in any possible way whether it is achieved in best or not it won’t  consider that.


Search Problems: A Search problem comprises state space, which represents possible states, successor functions( how an agent acts like actions and cost preferred), a start state and a goal test. A solution is defined as a set of actions preferred by the agent from start state(initial position) to reach the goal state(final state). Search problems are typically models where assumptions are made before trying to solve the problem like when we travel to a destination, cities are denoted as states, roads or the distance between two are successor functions but in a problem like eating all dots states will be the location(dots boolean), actions are the NSEW four directions and the successor function work is to update the location. A state space is denoted as the problematic approach of search problem which can be trees or graphs where every node represents the state of the agent. Each state occurs a single time in a state space graph. In a search tree children correspond to successors and the root as a state. Whenever we encounter a loop graph would denote it as a cyclic but a tree makes it even big making them infinitely large. So, while using a tree we keep constraints or corner cases whereif a cycle is detected the work is broken. Search Algorithms are divided into two, Uninformed and informed. It takes Time, Space Complexity and Optimality as consideration while concluding the best methods.


Uninformed Search algos have no idea of how far the target node is, it knows only the way of traversing and distinguishing between a leaf node and a target node. It searches each node without prior knowledge and is therefore often called blind search algorithms. They are of 3 types DFS, BFS and Uniform Cost search. A fringe, is a data structure which stores the possible nodes from a current node is used in these 3 Algorithms. 


In a  Depth-First Search(DFS) the algorithm works as it starts from the starting point and deepens  it to the last node in that path and backtracks if the goal is not attained in this direction. The time complexity in performing DFS is O(1+b+b^2+...b^m) i.e., O(b^m) where b is the branching factor( number of nodes in a path) and m is the maximum height. Space Complexity takes O(bm) as it takes all nodes(b*m) into the fringe.


A Breadth-First Search(BFS) algorithm works as it starts from the starting node and visits all its adjacent nodes present at that level and maximises its search. Time Complexity for this includes O(1+b+b^2+... b^s) i.e., O(b^s) where s is at which state the goal is obtained. Space complexity is O(b^s). These 2 algos work when the cost of travelling to every node from the neighbour is 1. DFS outperforms BFS when goal is at maximum height and BFS overcomes when goal is nearer to start and at a broader distance. DFS stores nodes as height wise and occupies less space and overcomes BFS where space is taken much by storing each level .


If cost in reaching one  node to another is other than 1 then Uniform Cost search comes into play. It is completely different from DFS and BFS as it includes varying costs where the prior two have costs of 1 from one node to another. The approach is similar to the BFS where nodes are visited level wise. It starts as a main root and adds its neighbours in a fringe and then the node with minimum weight is expanded and proceeds further by taking sum at every level(cumulative weight). The algorithm does not stop when it reaches the goal, it searches for another path where the cost would be lesser, when it pops out the goal node finally from the fringe the algorithm stops. The time taken to perform(Time Complexity) this is O(b^(C*/e)) where C* is the cumulative cost in reaching the goal and e is the effective or optimal cost for the goal. Space Complexity for this approach is same as TC O(b^(C*/e)). Optimality is an advantage and disadvantage includes it is traversed in all possible paths in reaching the goal and giving the minimum cost which takes more time and space in this approach. Graph search is usually a better approach than tree but in taking a large set graph fails.


The name Informed search algorithm itself says that it consists of the information of how long or far is the goal state. In other words, the algorithm for Heuristic searching is the same too. There may be chances of not finding the optimal path but it provides the solution in less time and space that too almost optimal in many cases. It is used in converting Non Polynomial problems(brute force) to polynomial problems. There are many methods in finding heuristics like Euclidean (straight line) distance, it is found by calculating the coordinate distance between two points ( start and goal) or Manhattan distance where both vertical and horizontal distance of goal and start is calculated. They are classified basically as Greedy and A* Search.


Greedy Search Algorithm follows the flow like BFS search where the neighbours are visited first. The greedy search algorithm expands to the nearest minimum cost node and excludes the other paths. Heuristic function here is defined as the distance of the goal from the respective node. Function with less value is preferred at every step. So the nodes are pushed to a priority queue data structure where it gives the minimum heuristic and good path.


A* search algorithm is used in informed searching or when we know some information regarding the goal from a certain node. It combines both Greedy and Uniform Search Algorithms. In this search the heuristic function is defined as the summation of costs of greedy and UCS and denoted as f(n) = g(n)+h(n) where h(n) denotes forward cost( distance of goal from that node) and g(n) denotes backward cost( cumulative distance of node from the start). Admissible heuristics is when the heuristic function at a node is lesser than or equal to the true cost to the required goal and defines as 0<=h(n)<=h*(n) where h(n) is forward cost and h*(n) is the real cost. For Admissible heuristics, we can conclude that it produces the optimal solution for our problem. Inadmissible heuristics may encounter error in producing optimal  solutions as optimal cost is greater than actual cost.


Properties of A* search : It expands or traverses towards attaining the goal with minimum cost. We cannot guarantee that A* search produces optimal but it gives a good or almost correct solution with less time and space. The Applications where A* are used immensely are Speech recognition, Robot motion planning, video games, Language Analysis etc.
Reference:
1. Self Learning Videos( Uniformed and informed search )
2. Classroom Slides