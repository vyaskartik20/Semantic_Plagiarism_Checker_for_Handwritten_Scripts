Artificial Intelligence has become one of the favorite storylines for many Hollywood movies and even though it is imaginary and we don’t find it happening in the real world, still people find it very fascinating as we see machines which has thinking powers like Humans, have reasoning and problems solving skills which can make our life better. Broadly speaking Artificial intelligence is intended to make smart and powerful machines that possess intelligence which means thinking power in similar ways like human brains and behave accordingly in a productive manner than humans in some tasks. Computational complexities arise while making decisions Reasonably and sensibly. Arriving at a Rational decision simply involves making choices after reviewing full available information, even minute details, and evaluating all options to choose the best possible alternative to achieve the goals with the highest associated utility. Rational choices aim at maximizing the utility. Inspiration to make intelligent and smart machines that can think and behave rationally like humans are somehow motivated by the functioning of the Human Brain. Neurons in Humans' brains are good at learning and remembering from past experiences but are not always perfect, this helps in making Rational choices. Similarly, Intelligent machines should be designed in a way such that they have a memory to remember information and to create computational simulations.AI can perform several things like working as a bot, good at facial features recognition, speech recognition, and can even perform some household chores, but it still has some limitations like driving in crowded areas or talking successfully with another person for an hour or more[1]. When we are talking about AI, we are defining an Agent who can perform such tasks. An agent can be defined as an artificially intelligent machine, robot, or a computer who can perceive information or signal from an environment and can act accordingly[1]. The selection of rational actions is determined by three factors first, what an agent perceives from the environment, features of the environment, and availability of possible actions. And AI Aims in designing such a Rational agent. How agent plans are classified into two types Reflex Agent and Planning Agent. Reflex agents are the ones who perceive Information from the environment and act based on the current scenario. They don't think about the future consequences of their actions, they just behave[1]. We can't assure that they have a memory to store past decisions. The reflex agent being rational will ultimately depend on the environment and actions taken. While on the other hand Planning agents do not randomly choose any path, they understand the consequences of their actions. They are somewhat aware of their working environment and try to keep track of their path.
Example -Let’s take a simple Pac-man game without a ghost. Here agent aims to eat all the dots. So the agent will choose arbitrarily the path and will start eating dots encountered. But it might be possible that the chosen path at the end has a loop(P shaped path), so the agent will start eating dots on the straight path but after completion of a loop, it will be stuck because no dot left on the traveled path. As a result, it won’t be able to eat all dots in the environment. In this example the agent behaved in a reflex manner, he had not considered the future consequences that he might get stuck after taking this path. While on the other hand planning agent would have chosen a path, considering future outcomes.
Optimal planning implies choosing the best possible path to achieve the goal. While Complete Planning focuses on achieving the goal, not on the path followed to reach the goal therefore the chosen path might not be the best possible path. Example - suppose we have three coordinate locations on a plane A, B, C forming a triangular shape, reaching point C from A can be optimally achieved by taking Euclidean Distance between A and C, instead of taking a path from A to B then B to C.Note that Optimal planning is always complete planning[1].Ideally, we should aim in achieving optimal solutions, and this is where the complexity arises. Planning normally executes the whole plan at a time, while replanning comes with many different sub-plans executed one after another, and such an agent which plans continuously is called a Replanning Agent[1].
To behave rationally agents need to perform some tasks using search algorithms. While Formulating Search Problem different parameters need to be considered, first is state space, which depicts all the available possibilities or states in an environment. The start state is referred to as the initial or starting state and the goal state is a final state which an agent aims to achieve, it could be many. A goal test is some type of function that tells us whether or not the state reached is a goal state[2]. The successor function accounts for actions and the cost associated with the actions, It describes how agents act in given state space. The solution is described as step by step actions taken to achieve the Goal from the start state[2]. While doing search tasks we first take approximations to model the environment. Let’s consider an example of traveling in Romania and here we aim to travel from Arad to Bucharest with the shortest path. In the map all the state space is Cities, while the start state is Arad, Goal Test is Bucharest, and the successor function will be defined as roads which are connecting adjacent cities and cost as distance covered, the solution can be traveling sequentially from Arad-Sibiu-Rimnicu Vâlcea -Pitesti-Bucharest since this path has the lowest Distance[3]. Every Search problem we are trying to approach is determined in the context of the World State. And the World State keeps all possible and minute details of the environment while the search state only considers information needed for planning the path[1]. Let’s take an example based on Pac-Man. Let’s assume the world state consists of possible positions of Pac-Man agent, counts of power dots and normal dots, three ghosts, and their position and agent’s possible actions. And here we have two problems, one which aims in the planning path and another which aims to eat all the dots. While in the first problem, states are location coordinates(x,y), actions can be taken in NSEW directions, the successor function will monitor and update the location as we need to trace our path. The goal test is the end location(x,y). While in the second problem states additionally consists of booleans representing the presence or absence of dots. Actions will be the same as NSEW, the successor function will also include the process of updating boolean to keep track of dots eaten and the dots remaining. Here the Goal test will change and it will represent all eaten dots, while the state could be anything. So we can say that the goal test is not always the same as a goal state[1].
The search problem can be approached either by creating a state-space graph or a Search tree. In the state-space graph, all possible states are represented by nodes, and arrows connecting nodes represent the resulting state as a consequence of actions. Each state will occur only once while constructing a state-space graph, nodes might have multiple edges and they can be approached from several directions.Building a full state space graph is not always feasible and requires large memory for its storage but it can also give us vast information.The search tree begins from the root node which is the initial node and their branches denote child nodes which can be further be explored. Unlike the graph, there might be an iteration of some states. While searching the tree, the path taken to reach the Goal state can be easily traced out. In graphs, if loops are not taken care then it will ultimately lead to an infinite search tree and we won’t be able to reach our goal, so cycles in the graph should be avoided with some constraints. Because of restraint on memory and computational complexity we might never explore a complete search tree. While searching across the Tree we need to maintain a fringe, which will keep a track of nodes that were seen but not yet expanded. So basically fringe consists of partial or sub plans. General Search Tree algorithms will be in such a manner, first, we will start with a frontier containing the root node. Second, we will repeat the following steps, if the frontier does not contain any node then return no solutions. Choose the node from the frontier based on the strategy. If that node contains a goal state then return solution, else expand the node and add it to the frontier. And what sub-plan to be picked from the fringe will completely depend on the Algorithm to be used[1]. Eg if we are implementing a Uniform cost search then the path with the least cumulative cost will be explored first from the fringe.While in the DFS path which will lead to the deepest nodes will be explored first.The tree search algorithm has been broadly classified into two sub-parts Uninformed and Informed Search Algorithm. Let’s first begin with the Uninformed Search Algorithm. The depth-first search is an Uninformed search Algorithm that will begin from the root node and will expand the deepest node in the tree[4].In DFS implementation Last in First Out(LIFO)  principle is followed, which is standard stack. So basically DFS will pick one path and explore until dead-end and then it will backtrack and explore other nodes. If a solution exists and loops are taken care then DFS will always lead to a solution, therefore it is complete. We are not always sure to get an optimal solution as we don’t perform an exhaustive search. If on the first path only we found the goal then the search will stop and it won’t be looking for other solutions that might be optimal. If we define m as max depth and b as branching factor then the Time complexity for search will be O(b^m).DFS has a very high time complexity because it will search for the deepest node in the search tree. The space complexity of DFS will be O(bm), because we don’t need to store every node, but we need to keep track of memory. Breadth-first search is another Blind search algorithm that will always expand the shallowest node simply means it will expand nodes at the same level and then goes to the next level. For maintaining a fringe queue can be used, which follows the First in first out principle. If s is the depth of solution then Time complexity is defined as O(b^s) and space complexity will also remain the same as O(b^s) as here we need to keep track of every expanded node. Yes BFS is complete if the solution will exist and a number of levels will be finite( s will be finite).BFS can only be optimal in a scenario where the cost associated with an action is 1 while moving from one node to another node because later on, the cost will increase as it is cumulative. In average conditions, DFS has better space complexity than BFS.BFS will outperform DFS in a scenario where the branching factor would be less and the goal is near the root node so that it can be easily accessed.DFS can outperform BFS in a scenario that has a high branching factor and solutions would exist for an example in the left-most part of the tree at depth, and if the suppose solution exists at the depth at the rightmost branch then it won't be feasible to use a Depth-first search.Iterative deepening is another search algorithm and it is a combination of both DFS and BFS. It takes advantage of Low space complexity of DFS and BFS ability to find a shallow solution. First, we fix the level of depth (like suppose initially we fix depth to 1 )which we need to iterate, then we will search for a solution at a fixed depth. And if a solution is not found then we will move to the next level of depth and repeat the same process.It seems highly superfluous but still, it is better than DFS, as it finds a solution at the upper level instead of searching one path till the dead-end. This algorithm would work best and redundancy would be reduced when the goal is in the upper right portion of the tree.Another Uninformed search algorithm is Uniform Cost Search which paves the path based on the least cumulative cost. To maintain a fringe, the priority queue can be used, which will prioritize the least cost. While searching it will first explore the node with the least cumulative cost. Yes UCS is both complete(if finite steps and positive cost) and as well as optimal as it considers the least cost at each path.Normally UCS doesn’t explore the complete tree, but if  suppose cost turns out to be negative then we need to explore every node to find the optimal solution.UCS will have the same time and space complexity as DFS which is O(b^number of required steps).Example- Let's take a weighted tree.The first fringe will store the root node since the cumulative cost is 0.Then the Root node will be taken out from the fringe and all successor of the root node will be placed into fringe and the one with the least cumulative cost will be taken out from the fringe and explored further. In such a manner we will explore the current chosen node until a goal is reached with the least cost.As we have seen during searching, the agent tries to simulate all the possibilities to reach the goal and eventually chooses one plan.The accuracy of the model will determine the effectiveness of the search.Example- Google maps are simulations of real-world scenarios.And they sometimes show several paths to reach a particular destination.And suppose if they don’t specify that one of the paths is from someone’s personal property which is not right and here our search algorithm will fail.The issue with a model can affect search results.
One of the foremost limitations of Uninformed Search Algorithms is that their implementation is slow, as they don’t have prior information regarding the goal's location, so they search blindly in all possible directions. And this issue can be resolved by implementing informed search algorithms. Informed searches use a special function called the Heuristic function, it estimates how close or how far an agent is from the goal with respect to a particular state. It gives an estimate not actual value. For each problem, we need to design a unique Heuristic. The heuristic may not provide us an optimal solution but guides an agent towards a goal instead of searching in all directions. The heuristic value of the goal state is always zero. Greedy Search Algorithm is an Informed search algorithm that expands the nodes which look closer to the goal under the guidance of the Heuristic function. The greedy search algorithm is prioritized with the least Heuristic value. If finite nodes then the Greedy Algorithm is complete, but not assured to find an optimal solution. Example- let's take a weighted tree, with the heuristic value defined at each node.First, the initial state is explored and all sub plans are placed into the fringe. The one with the least heuristic value will be extracted from the fringe, not the one with the least cost and similarly, we would explore that current state until it reaches zero heuristic value that is the goal state. Greedy Search Algorithm turns out to be worse in a scenario where we reach the states near the goal, but since there is no path connecting that goal we might end up searching more number of nodes, because of badly guided heuristic function. This can be resolved by putting some constraints on heuristic functions, so to avoid misguidance. The disadvantage of Greedy Search is that it will always choose the path closer towards the goal from its current state and will not understand that moving slightly away from the goal can even give the best solution. So we can interpret that agents implementing greedy search somehow behave reflexively.On average Greedy Search, Algorithm performs better than DFS and BFS, since it has some guidance as compared to searching blindly in all directions.And the performance of the Greedy Search Algorithm will depend on the heuristic function as well as the working environment. At the end, we need an optimal solution with the least amount of exploration. And one such algorithm is A* search.A* search is the combination of both Uniform Cost Search and Greedy Algorithm.UCS has slow implementation but will ultimately reach an optimal solution.On other hand Greedy Search guides towards goals, with the faster implementation.
Example - suppose we have a weighted directed graph with the heuristic value associated with each node and cost associated with paths between two nodes. Earlier while implementing greedy search we only accounted for associated heuristic value, not the cost associated with paths. But here we will combine the strategy of UCS accounting for cumulative associated cost with the path or backward cost which will denote the actual path covered till now as g(n) and Greedy Algorithm strategy of prioritizing forward cost, which indicates a closeness to the goal as h(n). From here we can formulate a search tree where each node is associated with g  as cumulative cost value to reach a particular node and h as heuristic value.A* prioritizes based on the value of g(n) + h(n) . Here g(n) is the cumulative cost, and h(n) value is of a particular node. The main question arises that when we should stop searching, at the time of enqueueing or dequeue of the plan containing goal. While enqueuing the plan we are not considering the minimum cost, and we cannot assure that the chosen plan is optimal. But at the time of dequeue, we would have many sub plans in the fringe and the optimal one can be chosen accordingly. Therefore at the time of dequeue, we can ensure an optimal solution. All the Algorithms discussed so far only differ based on how they are prioritized.DFS is prioritized based on finding the deepest node in the frontier, BFS on finding the shallowest node at a level. While UCS is based on cumulative backward cost, in greedy search heuristic function is considered and in A* search, we use summation of both backward and forward cost.
Before commenting on the optimality of A* search, let’s consider an example. Suppose we have a directed graph defining heuristic value at each node. Two possible plans are connecting the start state and destination, one with the least cumulative cost but with very high heuristic value for the intermediate node even greater than the actual path cost. While others have higher cumulative value compared to path 1. In this, A* is expected to pick the optimal path, but it has selected the path with a higher cumulative cost.It went wrong because the heuristic function of an intermediate node of path 1 has a very high value than the actual path cost,so basically, the heuristic function started over-analyzing which led to a bad solution.A* can work optimally by putting some constraints on the heuristic function in such a way that the value of the Heuristic function at the particular node should be less than or equal to the actual cost required to reach the goal from that state.Such Heuristic functions which underestimate the actual distance to be covered is known as Admissible Heuristic. Inadmissible Heuristic overestimates the actual distance to be covered to reach the goal.If we have multiple nodes to reach the goal then the heuristic function h(n) is admissible if its value is less than or equal to the actual cost of the shortest path and greater or equal to zero.
Let’s try to prove the optimality of A* search when we might have multiple goals. Suppose we have a tree with two Goal nodes A and B.A is considered optimal while B is considered sub-optimal and h can be defined as an admissible heuristic function. Let n be the predecessor node of optimal goal node A. Here we need to prove that optimal node(A) will be considered first while dequeuing the fringe. Let’s presume B is already in the fringe and will proceed further where f(n) is the combination of both g(n) and h(n) , since h is an admissible, value of g(A) is greater than or equal to f(n) and at node A h value is 0 ,so f(A) is equal to g(A).So from here, we conclude that  f(A) is greater than or equal to f(n). The value of g(A) is greater than g(B) because B is a suboptimal node and h value at both goal states will be 0. Therefore we can say f(A) is also greater than f(B).From the above to proven point we can claim that node n is explored before node B.This proves that all the predecessors of the A will be expanded before B.Hence optimality of A* search is proved when h is admissible.
On Comparison with UCS ,A* is expected to search less for goals because UCS rely only on the backward cumulative cost and will explore uniformly in all directions but A* considers additional heuristic functions to avoid blind search which ultimately increases the computational cost of each node , and can be optimal if Admissible Heuristic function is used. Better Heuristic Functions which can provide higher accuracy in estimations are more complex and possess higher computational cost for each node.UCS will explore almost in all direction but end up finding the optimal solution, while greedy search explores nodes towards the direction of goal , but might not promise optimal solution but in the case of A*, it is expected to explore less than UCS but greater than Greedy search and will ultimately reach the optimal solution.
A* has several applications like designing video games eg-Pac Man, used in the analysis of language-searching complete sentences from the information of two or more words,recognition of speech and translation of some text based on available literature etc.
While creating Admissible Heuristic functions we somehow try to relax the problem by removing some constraints,so we can come up with different action plans.Eg supposes we have a map connecting two cities with several intermediate cities. Instead of traveling between different cities to reach the goal via road,we can try taking straight-line path to the goal directly from the start state via flight.In such scenario, we cover less distance compared to actual path distance, and this can be termed as an admissible heuristic. Even though Inadmissible heuristics might not provide optimal solution, we still use in certain problems where we need to find the solution quickly and where the solution is important not optimality.
Example- Suppose we have a 3*3 puzzle with 8 titles numbered 1 to 8 and one empty block.We aim to achieve a state where titles are arranged sequentially(1,2,3….) in a horizontal manner with an empty state at beginning.Here total states will be 9 factorial. Successors can be 2,3,4 in a particular state and actions will be sliding of nodes into empty space according to the strategy. And we can define less cost to the node nearer to the goal.We can define the heuristic function as a number of titles incorrectly placed from its ideal location. And this Heuristic can be admissible if we relax some constraints on tile movement.We can comment that average node expansion in the case of UCS will increase exponentially(because of blind search) when we are 4,8,12…. step away from ideal location compared to A* search(because of Heuristic Function). 


| Introduction to AI |
We start with the first lecture where we discuss and try to answer the questions what is AI and what it is capable of? We tried to relate our current understanding of AI with the concepts of Sci-Fi movies. We tried to answer the questions: Why do we like such Sci-Fi movies with AI concepts? And what is common between all such movies? We concluded with the answer that we like such movies because this gives us an idea of how things will be in the near or far future and thus gives us a feeling of experiencing the future virtually. Something common between all such movies is that we try to give AI a human-like appearance. This describes the main goal of AI: making machines or programs which are as smart as humans or smarter than humans. After this, we discussed the rationality of the decisions and tried to answer what are rational decisions? Rationality is not associated with the decision that has been made and not the thought process behind that decision meaning it doesn't matter whatever you thought about the problem, how you analysed it, before you came to the decision or how the program or machine has analysed the problem before it made that decision. In scope of rationality, goals are associated with some utility which should be maximized. An example of utility: Utility can be defined as the time spent in reaching the goal subtracted from the time spent in reaching any goal in the worst possible case. Being rational by maximizing this utility means reducing the time taken to achieve the goal and reducing the computational cost. For, this course we adopt the term: Computational Rationality. Then, we discussed the rationality of human brains. Although human brains are not perfect, they can be considered very good at making rational decisions. But, brains are not as easy as a computer program to read, study, reverse-engineer and learn. However, many researches are going on trying to learn about the human brain on how it makes decisions, how it learns any activity, etc. So, that these can be later incorporated in the AI programs to make them better. Then, we discussed the remark,"Brains are to intelligence as wings are to flight." One may take inspiration from the brain to make rational decisions, increase utility of an AI program but such approaches may not always be fruitful. The inspiration of making an airplane came from birds' wings but the airplane is not designed like a bird. In a similar manner brains may act as an inspiration for AI but they cannot be directly implemented feasibly. The two main inspirations that the brain can provide us in terms of making rational decisions are: Memory and Simulation. We need to properly think about a problem if we want to make a rational decision and while thinking about such problems, we're also required to associate them with our past experiences or knowledge(memories). Then, we discussed the tasks which currently AI are capable of doing. AI may be able to buy groceries weekly on the web, but it can't do the same in a grocery store. Although it can answer the questions asked, AI would still fail in successfully conversing with another person. And since humour and art is subjective, we haven't been able to teach AI how to intentionally write a funny story. We also discussed some tasks which AI can do efficiently. Speech technologies, like Google Assistant in our phones, can be considered as a good example where the AI can recognise speech, process it for a response, and convert the text response to a speech. Many examples are there where AI uses vision to learn. In robotics, image processing is used to learn from humans and then train the robot on how to move. Infact, in robotics there are many other AI examples. There are robo-soccer games, self-driving vehicles, etc. AI has also been used in Mathematics and Logic. It is used to develop logical systems which can perform various tasks like proving theorems, question-answering, etc. AI are also capable of defeating humans in games basically because it's easier for them to do high computations. We also discussed examples of AI which are being extensively used around us, like spam emails classified, fraud detection, route-planning (Google maps), search engines, etc.
After being introduced to AI and it's capabilities, we get ourselves into how AI is designed. We start with discussing about agents and designing rational agents based on our understanding of Computational Rationality and utility. An agent can be described as an entity which interprets the situations, processes for what should be done, and then does it. A rational agent is one which always maximizes the utility by the decision it makes. For making rational decisions, the agent should be capable of properly interpreting the environment. Then we discussed how Pac-Man, a game, resembles an agent as it perceives the environment on how it's surroundings are, processes what is the best that can be done and then does it.

| Uninformed Search |
After this, we discussed "Search". We discussed how a normal problem can be analysed as a search problem, what is uninformed search and what are different methods of uninformed search. But, before getting into it we needed to understand more about agents and what are different types of agents. An agent interprets the environment, and taking that as a base thinks of an approach which should be suitable to achieve the goal optimally, and then act accordingly. Based on how they select approaches, agents can be classified into reflex agents and planning agents. Reflex agents are the one which make decisions according to the current environment without considering how your consequent actions will affect the environment. Such decisions can be made on the basis of memory, meaning how it has performed in the past, or it can be made on the basis of defined rules for any particular given environment. This gives rise to the question, "Can a reflex agent be rational?" It would totally depend on the environment and current state of the environment whether a reflex agent acts rationally or not. So, a reflex agent may or may not  be rational. Planning agents are those which make decisions according to the current state of environment as well as the changes that will be made after the action of the agent. So, such a decision is based on a defined set of rules or models on how the environment will change when the agent will make a particular action. For this you also require a well defined goal. So, when it comes to planning agents, two types of plannings are defined: Optimal and Complete planning. A complete planning is the one in which it is ensured that if a goal is achievable, it will be achieved. There can be many ways of achieving the goal, also there may be multiple goals, the planning which achieves the goal with the maximum utility is called an Optimal planning. A planning can be either complete, or both optimal and complete, or even none of them. Some Agents may come up with a whole plan at ones while some may require replanning at some points. These can be termed as planning and replanning agents respectively. Then, we come to "Search". So to convert a problem into a search problem, we must understand what is a state. An environment can be different at different points. All these possible environments are called states. To define a search problem, we need a few things: A state space, which contains possible defined states; A successor function, which defines how a function progresses in the state space(actions); A start state; A goal test which checks if we've reached the goal. A solution can be defined as the possible actions or sequence of actions which lead the problem from start state to goal state. Then we discuss that search problems are just models of the real problem. We can't actually extract htee whole real problem because that would be very difficult to solve, so we define a simpler search problem which is solvable. We understand this using an example of "Travelling in Romania''. The real world problem would be how to go from one city to another, but we can define a simpler search problem to solve the actual real problem. We consider Romania as weighted graph data structure with cities as vertex and distances as the cost of edges connecting cities. Now, looking at the problem, the state space would include the cities. The successor function would include how we move to adjacent cities with the cost(distance) associated. We define a particular city as a start state and we keep a goal test, i.e. if we've reached the city we wanted to. We'll soon come to the solution of this problem. Meanwhile, we take a deeper look into the state space. So, the world state is the state space which includes all the details of the states. And we define a search state which always contains the details which are required or the planning. Considering the example of pathing problem, search state would include states, the defined actions, the successor functions and the goal test. We then define state space sizes. State space size is the no. of states possible for a given problem. Taking example of a Pac-Man game, where a pac-man has to eat all the dots while two ghosts move to shoot at it in one given direction. For this, we define world state, which could include the no. of agent positions, the no. of dots, the no. of ghost positions, and since the pac-man can have different possible directions, the no. of possible directions. We can use permutations and combinations to find out the no. of possible states accordingly which would give us the state space size. We also try to find state space and state space sizes of other examples to understand it better. Now, we come to how these search problems are solved efficiently. For this, we'll have to convert the state space graph, provided by the problem, to search trees. We try to look at the search problem from a mathematical approach and try to view it as a state space graph. In this, nodes can be defined as the states, the edges link a node to its consequent successors or action results, the weight of an edge can be linked to some parameter of the real problem(example - time taken), and there is a goal test which includes the goal nodes. In the state search graph, every node will be unique, stats won't occur more than once. Then we try to view these state search graphs as search trees. This search tree comprises the plans and their outcomes. The root node shows start state, every node shows a state and corresponds to the path or plan that led it there, children are successors or the outcomes of actions. It is not actually feasible to build the complete search tree for most problems. In some cases, the search tree may even be of infinite size, e.g. search tree corresponding to a state search graph containing cycles. 
Now, we explore searching in a search tree. Searching in a search tree includes some steps: expanding the plans, maintaining a fringe of partial plans. We should try to expand as less as possible. There are many methods of searching in a tree. A general search tree initializes the tree with start state, and terminates when there are no more candidates for expansion. The search is based on three important ideas: Fringe, Expansion, Exploration Strategy. The Exploration Strategy decides which nodes are to be explored. We will be discussing three types of search: Depth First Search - DFS, Breadth First Search - BFS, Uniform Cost Search - UCS. First, we'll look into DFS. The idea behind DFS is to completely explore all the paths leading from a particular node before starting to explore its neighbour. DFS is a complete planning, when we avoid cycles, because it will explore the whole tree before it terminates the search. But, DFS isn't an optimal planning. DFS will end when it finds the first goal state. It may occur that there may be a shorter unexplored path to a goal state which never gets explored by DFS because it has already found a goal state while it was looking in depth of a node, i.e it lies on the right side of the found goal state. DFS always settles for the leftmost solution. We can define time complexity and space complexity and no. of nodes in terms of b - the branching factor and m - the maximum depth of the tree. No. of nodes are O(b^m). DFS has a high time complexity O(b^m) but a good space complexity (fringe) equal to O(b*m). 
In BFS, the idea is to explore all the nodes in one level before going to the deeper level. BFS is a complete as well as optimal planning as long as the cost of all edges is 1 or same. The time and space complexity can be defined in terms of branching factor b and depth of shallowest solution s. It has a good time complexity O(b^s) given the solution is shallow. But, it has a very high space complexity(fringe) O(b^s). DFS is a better option when the goal state is to be found very deep. And BFS is a better option when the goal state can be found in shallow levels.
We know that DFS has a good space complexity and BFS has good time complexity for shallow solutions. To combine the advantages of DFS and BFS, iterative deepening is used. The idea of iterative deepening is to limit the depth for DFS and iteratively executing it with a constantly increasing depth limit. This involves a lot of redundant search, i.e. it will explore the explored nodes again and again. And that's why it is suggested only for shallow levels. This is useful because BFS has a very space complexity which this approach can overcome.
BFS is not a cost sensitive search, i.e. it finds the shortest path to goal state in terms of no. of actions, not in terms of costs. Hence, Uniform Cost Search is used when there are costs associated with actions. The idea of UCS is to expand the node which has the least cost. So, we'll add paths to fringe and expand the path which is cheapest and add the new paths to fringe. We then expand the cheapest path at that point. In this, fringe is implemented using priority que with the priority as cumulative cost. Till the time UCS finds a solution, it has explored all the paths cheaper than the cheapest solution. Effective depth can be represented as (C*)/e where C8 is the cost of solution, and e is the minimum cost of the edges. UCS is a complete and optimal planning given that the cheapest solution has a finite cost. The space complexity(fringe) is given by O(b^((C*)/e)). UCS explores the increasing cost contours. For all these uninformed searches we learned about, they have two setbacks: they don't have prior information of the goal location; And UCS, BFS and iterative deepening explore in all the directions and not specifically towards the goals while DFS explores in a particular direction but regardless of goal location. We should also note that the agent doesn't try to explore all the plans actually in the real world, it just simulates. And the search is only as good as the model. Any fault in the model can lead to faulty search, e.g. if a road isn't in the model of Google maps, it will never show that path even if it is shortest.

| Informed Search |
We've seen different Informed Searches. Now, we'll look at the informed searches: Greedy Search and A* Search. Before that, let's brush up our understanding of search. A search problem comprises of states, action and costs, successor function, start state and goal test. For the search problem, we construct a search tree in which nodes represent plans for reaching those states and the plans have a cost, which is sum of the cost of the actions comprising that plan. A search algorithm builds that search tree, defines its way of maintaining the fringe, and aims to find the solution. Some algorithms are successfully able to find the optimal solution. The main setback of the uninformed searches is that they don't have an information of the goal location. We try to overcome this in Informed Search. For an informed search we need an entity that can tell us or guide us regarding the goal location. We define such an entity heuristic as a function that estimates that how far the current state is from the goal state. Every search problem has a different heuristic. To understand heuristic function, we look at the example of a simple pac-man game which has only one dot at some defined coordinates in the 2d space, and there are maze-like structures around. To have an estimate of how far we are from the goal state, we define heuristic as the Euclidean distance between the pacman and the dot. We can also use Manhattan distance as a heuristic here. This will help the agent to avoid searching in directions away from the goal. We look at the example of "Travelling in Romania" problem discussed before. In this we have an estimate of distances of cities from the goal city as a heuristic function. This will help the agent in not searching the paths through the cities which have longer paths unless required. We take a look at an example "Pancake Problem". There are 4 different sized pancakes lying in a column manner. You've to arrange the pancakes in increasing order of size top to bottom. You can flip a complete stack of first i pancakes, 1<=i<=4. And the cost of every action is the no. of pancakes in the stack you flip. Here a heuristic can be used equal to the number of the largest pancake that is still out of place. It would guide the agent.
Now, we come to our first Informed Search, Greedy Search. As it goes by the name, the idea of the greedy search is to lead the path which seems the best at this point. Talking in terms of search trees, we'll expand a node and then expand the path which has the least heuristic value and not look at the previous node's neighbours. We're basically expanding the node that seems closest to the goal. Usually, it is a very good approach and directly leads us to the goal state(may not be optimal). But, in the worst case, it may act like a badly guided DFS and explore the whole tree before exploring the goal state. This may happen due to the poor choices of heuristics. The Greedy Search may lead to suboptimal goals. 
Now, we'll take a look at our second informed search, A* Search. The idea is to overcome the setbacks of UCS and Greedy Search by combining them. UCS expands nodes on basis on backward cumulative costs g(n) and Greedy Search expands nodes on basis of forward estimated costs h(n),i.e. heuristic. In A* we expand just like a greedy algorithm but instead of looking at the h(n) we compare the (h(n) + g(n)) and then expand. This will lead us to optimal solutions. But, the issue is that it may lead us to a suboptimal solution before that optimal solution, so we need to take care that we don't stop the search when we encounter the goal, but stop it when we dequeue a goal. This is because A* search may encounter a suboptimal plan leading to goal before the optimal goal but the algorithm will always dequeue the optimal goal first.
Then we look at an example, where A* search fails. We have a directed graph of three nodes, S- start, G- goal and A- another node. Costs defined as S->A=1, A->G=3, S->G=5. Heuristics are: h(S)=7,h(A)=6,h(G)=0. We can clearly see that S->A->G has a cost 4 and S->G has a cost 5, so the former is optimal. But, when we perform A* search on this, we will get S->G as the result. This happens because of our faulty heuristics. We should have actual bad goal cost< estimated good goal cost to avoid such failures of A* search. So, we need estimates to be less than or equal to actual costs. Here, we introduce the concept of admissibility of the heuristics. An admissible heuristic should may slow down bad plans, but it should never outweigh the true costs. On the other hand, inadmissible heuristics are those which may break optimality bi trapping good plans on the fringe. We define an admissible heuristic h(n) as the one which is always less than or equal to the true cost. 0<=h(n)<=h*(n) where h*(n) is the true cost to a nearest goal. Manhattan and Euclidean distances in Pac-Man example and the largest out of position pancake heuristic in "Pancake Problem '' are examples of admissible heuristics. In the Pac-Man example, Pac-Man has to travel at least the Manhattan distance for reaching the goal. Similarly, the least you have to do to bring the pancakes in order is flipping equivalent of that cost. Finding out admissible heuristics is a major task while using A* search in practice. A* search will give us optimal solutions given that the heuristics are admissible. It can be proved that A* search will give optimal solutions. Let's consider that in the search tree, A is the optimal solution and B is the suboptimal solution. Say we have B in fringe and not A at a point during search. We consider that an ancestor of A, n, is on the fringe. Since heuristic h is admissible, f(n)=g(n)+h(n) will be less than g(A) which is cumulative backward cost till A, and g(A) will be equal to f(A) {h(A)=0}, so f(n)<=f(A). And f(A)<f(B) because g(A)<g(B) because A is optimal and B is suboptimal. So, all ancestors of A and A expand before B. Thus, A* search is optimal.
Now, we'll discuss some properties of A*. Unlike UCS, A* search expands toward the goal whereas UCS expands in all directions equally. A* search just like UCS also ensures optimality. If we compare the Greedy Search, UCS and A* search in Pac-Man example, it can be seen that in greedy search agent explores very less wrong plans before it reaches the goal, A* explores a little more wrong plans before it reaches goal but doesn't explore away from the goal, whereas UCS explores almost all the paths, before it reaches the goal. A* search is also useful in designing automated opponents un video games, robot motion planning, etc.
Now, we take a look at how we create heuristics. Creating Admissible Heuristics is the major part of solving hard search problems. One way to come up with admissible heuristics that we think of relaxed forms of those problems. If we take the example of Manhattan distances in Pac-Man example, We can think of a relaxed form of the problem as a problem where there are no mazes around, so the best way to reach the goal would be by covering the Manhattan distance in terms of least distance travelled. In the "Travelling in Romania" problem, we can see straight line distance as a heuristic which is the outcome of a relaxed problem flying between the initial and final cities. 
We take a look at 8 puzzle problem in which we have a block-shifting puzzle with 9 slots, * blocks namely 1 to 8 and one empty space to enable shifting. Any state can have 2 to 4 successors where actions include shifting blocks into empty spaces. We can define cost as the no. of actions we make. And the goal test will be if they are arranged in order. We look at one type of heuristic in this, in which we take heuristic= no. of tiles misplaced. And it will be admissible, because if you relax the problem of taking out, shuffling and putting it back in that's the least no. of actions you'll have to perform. In this, we can see that this A* search takes significantly less no. of paths explored before we reach goal.

In the mid-1950s John McCarthy, the father of AI coined the term �Artificial Intelligence� and he defined it as �the science and engineering of making intelligent machines� [1]. Today even after 70 years the fundamental meaning has not changed a bit and we define it as �the study and design of intelligent/rational agents�. Rational agent here refers to the entities (agents) which can perceive an environment and can select an action (from a set of possible options) which could maximize its chances of success [2]. In a nutshell, AI is the science of making machines which can think and act rationally. 
AI has been used in a variety of fields. AI-based machines have beaten humans in chess (deep blue) and jeopardy (watson). It is widely used in self driving cars, speech recognition, image recognition, Natural language processing, and in the field of robotics. Deep blue was an AI based chess playing computer and about which Drew McDermott once said: �Saying Deep Blue doesn�t really think about chess is like saying an airplane doesn�t really fly because it doesn�t flap its wings� [6]. 
These intelligent agents can be categorised into two types viz: reflex agents and planning agents. Reflex agents choose the best action based on their current percept. They ignore the future consequences of their actions. Hence, they are generally not expected to be rational. Still, they may act rationally for some environments (just by luck) e.g.: There is only one action available to the agent for every transition, in this case the reflex agent will act rationally. On the other hand, planning agents plan their actions, before proceeding to the goal i.e. they come up with a sequence of actions needed to achieve the desired goal [4]. To do so, they must have a model that could determine the consequences of their actions. i.e. a model which could output the future possible actions corresponding to a current action.
Let�s define some of the key terms used in AI viz: state space, start state, goal state, successor function, solution. A state space is nothing but the set of snapshots of all possible scenarios our environment could be in. Start state is that snapshot from where our agent will start its journey and the goal state is the final state that our agent wants to achieve. A successor function outputs a set of states reachable from a chosen state. Generally, there is a cost function which calculates the cost of every transition. Solution is the sequence of actions which the agent took in order to transition from start space to the goal state. A solution could be either complete or optimal. In complete solution we are only concerned about finding the goal while in optimal solution we want to find our goal in the best possible way. E.g. finding the solution with the least cost.
As described earlier, our intelligent agents will select the most optimal action from a given set of actions. To do so, it first needs to search (among all the actions) and find that optimal action. Formally, Search is the process of navigating from a start state to the goal state by transitioning through intermediate states [3]. To plan our path, we first need a well-organized structure representing state transitions. We use state space graphs and search trees for that. State space graph is a representation of a search problem where all the states our environment could be in, are represented as nodes and all the nodes are unique. Each transition is represented as arcs between the current state and the future state (after the action). The arcs may have a cost associated with it. Generally, a state space graph could be very big (in terms of memory) and are rarely fully build. Search tree follows a tree data structure where the start state is the root node and successors are represented as children. We can see that in search tree, nodes might not be unique. Further, we don�t generally construct a full tree as the size could be very big and we don�t really need to do that. Most of the times we construct a partial tree and expand it, till we reach the goal. 
We now have our search tree and we need to search and find our goal state. We need to keep this in mind that we should expand as few nodes as possible, any unnecessary expansion will just increase the time and space taken. The fundamental strategy is simple. We will maintain a fringe of partial plans (initialized with the root node i.e. start state) and then choose a plan from the fringe (according to some sets of rules) if the fringe is empty, we will return failure. Else if the leaf node of our partial plan contains the goal state, we will output the plan and return. Otherwise, we will expand the leaf node and add the resulting nodes/plans in the fringe. Note here that after choosing a plan we are removing it from the fringe. There are many search algorithms and they all follow this fundamental idea. However, they are differentiated by the rules they follow while choosing a plan from the fringe.
Depth first search (DFS) is a search algorithm which follows LIFO stack implementation of fringe. As the implementation follows LIFO rule, we will be expanding some left prefix of the tree. And only after exploring a whole left prefix, we will go to the next subsequent one. In the worst case, our goal could be at the rightmost bottom and then we will need to explore the whole tree to get to the goal. If b is the branching factor and m is the maximum depth. It may take O(b^m) time and the space complexity would be O(b*m). DFS guarantees a complete solution (given that it exists and cases like infinite cycles are handled). However, the solution might not be an optimal one as the algorithm will give the leftmost solution ignoring the cost. One benefit of DFS is that it is space efficient.
Breadth first search (BFS) is a search algorithm which follows FIFO queue implementation of fringe. Because of queue implementation it expands the shallowest nodes first and then goes to the next deeper levels. Let�s say the branching factor is b, the maximum depth is m and we are at some level s. And we define the cost as the depth from the root node. Then there would be b^s nodes at that level (s). Therefore, our fringe will need to store O(b^s) nodes, and searching among them will also take O(b^s) time. So, one thing is clear that BFS is not memory efficient. Just like DFS if a solution exists BFS will find it, but the catch here is that the above found solution would be the optimal one (least depth) as FIFO rule forces our algorithm to find the shallowest solution. 
We can conclude that if the goal state is somewhere deep down in the left part of the tree DFS will perform better but if the goal state resides in the right part and in a shallow region BFS will outperform DFS.
Let�s assume a case where we know that our goal state is somewhere in the right part and in a shallow region. Using DFS here won�t be a good idea as it will first explore the left prefix of the tree but we also can not use BFS because we are bounded by memory constraints. Therefore, we need to find a way so that we could take advantage of memory efficiency of DFS and FIFO implementation of BFS. It is done using iterative deepening. We basically run DFS for a certain depth limit (assume 1). If we do not find a solution, we increase the depth limit by 1 and then run DFS for this new depth limit (assume 2, 3, 4, �). We repeat this process till we find a solution. Basically, we are applying DFS iteratively under the constraints of depth limit and each time penetrating an extra level. This will take more time in comparison to a normal BFS but it will give better memory efficiency.
BFS can only find an optimal solution if the cost is defined in terms of depth. Otherwise it does not guarantee an optimal solution (least cost path). We can say that BFS or DFS are not cost sensitive. Uniform cost search (UCS) is a cost sensitive algorithm in which fringe is implemented as priority queue where priority is defined as the cumulative cost. And every time we choose the least cost state and expand it. This cost sensitive strategy leads us to the least cost solution. The solution will be not only complete (given that all costs are positive and finite) but also an optimal one. Let�s assume our solution costs C* and the arcs cost at least e, then the effective depth would be around C*/e. If our branching factor is b then it will take O(b^(C*/e)) time and the memory taken would also be the same, i.e. O(b^(C*/e)). The time and space complexities look familiar to BFS. Further, we could improve the efficiency of UCS if we could provide it a sense of direction as in general it is exploring options in every direction and it is blind to the goal. 
Note here that UCS follows a priority queue fringe rule. If we define this priority appropriately, UCS can easily be converted into DFS or BFS. We are using queue or stack just to get rid of log(n) overhead as insertion and deletion in priority queue takes log(n) time. Further, all of them are uninformed search, i.e. they are blind to the goal state while searching. They just know what the goal state is but apart from that they know nothing.
Informed search has some knowledge about the goal state e.g.: how far they are from the goal state. To incorporate additional information regarding our goal state we define a heuristic function. This function estimates how close a state is to the goal. For example, we can take Manhattan distance or Euclidean distance as the heuristics for Pacman problem or city and the shortest path kind of problem. For pancake problem heuristic could be the number of the largest pancake out of place. These heuristic functions are particularly designed for a problem. Meaning, a proper heuristic for a problem might not be a good heuristic for another problem. 
So now we have a function which could potentially guide us towards the goal. We are not blind and directionless. A simple method to find the solution would be using a greedy search. A greedy search algorithm will greedily choose and expand states which are closest to the goal state thinking the closest states will most likely take it to the goal. But, in most cases this greedy approach will take us to the wrong goal and if turning back is not allowed, we will be stuck. For example, lets suppose A is our goal state and B is the closest to A. Now assume greedy approach takes us to B thinking B is the best choice. But there is no path connecting A and B. In this case we will be stuck at B if going back is not allowed. Even if we are not stuck a worst case could be like a badly guided DFS in which we may end up exploring all the paths. Therefore, greedy search is neither complete (going back is not allowed otherwise it will be complete) nor optimal. 
Greedy search is bad and it is not even complete but one best thing about greedy is that it is not blind. UCS has everything but the sense of direction. If we combine both of them then we can have a UCS which is not directionless. This combined algorithm is A* search. It has a path cost or backward cost g(n) borrowed from UCS. And a goal proximity or forward cost h(n) taken from greedy search. 
So, the strategy of A* is the same as UCS. But the priority definition would change to f(n) = g(n) + h(n). One thing to note here is that we should terminate and come out with a plan only when we dequeue a goal not while enqueueing a goal. Otherwise we may miss some better possible plans. This A* solution seems optimal but when actual bad goal cost is less than estimated good goal cost it will not be optimal. In order to avoid that case, we need to ensure that the estimates are always less than actual costs.
We cannot play with cost function. The only thing in our hand is that we can change the heuristics (f(n) = g(n) + h(n)) and choose a better function which satisfies the above criteria. We call it admissible heuristics. Formally defining if our heuristic function follows 0 <= h(n) <= h*(n), where, h*(n) is the true cost to a nearest goal, then it would be an admissible heuristic. The problem with inadmissible (pessimistic) heuristics is that it may trap some better plans in the fringe and output a non-optimal result. On the other hand, in admissible (optimistic) heuristics the bad plans can never exceed the true costs. So, it will always output the optimal result. 
Our A* search will always give optimal solution if this admissible heuristic used. The proof is: let�s assume two nodes corresponding to the goal state viz: A (optimal goal) and B (non-optimal goal). And assume that B is in the fringe, an ancestor n of A is also in the fringe. Now, if both A and B are in the fringe then obviously the priority queue will output A (optimal solution). Therefore, we need to prove that n will expand to A before B expands. Now, f(n) <= f(A) because the heuristic is admissible. And also, f(A) < f(B) because A is the optimal solution. Hence, f(n) <= f(A) < f(B) therefore, n will expand before B. 
When we observe the expansion contours of UCS and A*. We see that UCS will expand equally in all directions as it is blind towards the goal. While, A* expands manly towards the goal as it knows where the goal resides. However, it does not follow a direct straight path to the goal to ensure optimality. The A* search algorithm is used in a variety of problems usually as a pathfinding models, such as video games, language analysis, speech recognition, routing and resource planning [5].
We just assumed that we got an admissible heuristic and it solved all our problems. But, creating an admissible heuristic is itself a tedious task. One way is that we relax the problem i.e. We reduce or remove some of the constraints of the problem and then solve the relaxed version and try to find an admissible heuristic function. For example: For a city and shortest path problem we may relax the problem and assume that a person can fly and take a straight-line path directly to the goal state. Now the heuristic of this relaxed version (straight line distance) might give us an admissible heuristic. For an 8-puzzle problem there is a constraint that we must interchange a tile with a blank. If we relax the problem then we can assume that any tile could be interchanged with any other tile (or blank). So, this relaxed version will give the heuristic = number of tiles misplaced which could be an admissible heuristic. 


Introduction to AI
Artificial intelligence (AI) is a recent and exciting field in science and engineering which aims to build intelligent systems that think and/or act like humans. This field is relevant to all the fields which require automation of intellectual tasks. In an era with an exponentially increasing population and incessantly increasing demands for products and services, there is a dire need for automation of manual tasks because robots are much more efficient than humans in terms of speed and capacity. AI aims to extend this automation from straightforward mechanical tasks to creative and intellectual tasks. There are four main approaches in AI, visually, building machines that think as a human, act as a human, think rationally or act rationally. The 'thinking as a human' approach is based on cognitive science, which studies the actual inner workings of the human brain. The 'acting as a human' approach is defined by the Turing Test, in which a human interrogator poses some questions to a machine, and if the human interrogator cannot tell whether the responses are generated by a human or a computer, the system is considered to be intelligent. The 'thinking rationally' approach is based on the field of logic. Problems are reframed in the form of formal logical statements and arguments are constructed by inferring valid conclusions from a set of premises. But such a reframing of problems into formal statements isn't always possible. The 'acting rationally' approach is more general as it also includes forms of rationality other than logical inferences, such as reflexes. An agent is something which acts, but it is also expected to operate autonomously, perceive its environment, adapt to changing environment and create and pursue goals. We are going to study the design of intelligent agents that act rationally under a given situation. The term 'rational' here means 'in the best possible manner'. Rationality depends on the decisions made by an agent while solving a given problem, and not on the thought process behind them. A goal-based agent starts by formulating a goal based on the utility of the outcomes (the utility is just a numeric value assigned to each outcome which describes how desirable the outcome is). Acting rationally then corresponds to maximising the expected utility. AI is inspired by the working of the human brain. But instead of trying to mimic the human brain, it aims to understand the underlying principles of intelligence itself. The brain teaches us that memory and simulation are key to decision making, which motivates many approaches in AI. There are many tasks for which there exist AI models which far outperform human experts. For example, the most recent chess engines are able to convincingly defeat world's best human chess players. Similarly, object and face recognition, image classification, speech recognition, language translation, and self-driving cars are fields in which AI proves to be quite effective. On the other hand, there are tasks in which we have still not been able to effectively apply AI, such as having long conversations with a human and generating intentionally funny stories. Both these tasks require some form of creativity, and defining 'funny' formally in the latter case, which is quite difficult to do. This course is about the design of rational agents which can find a solution of any general problem defined in terms of an initial state, transition model, action space, a set of world states and a cost function.

Types of Agents and Planning
There are various types of agents. Two such types are reflex agents and planning agents. Reflex agents are agents which do not consider the future consequences of their actions, i.e., they do not plan ahead. They adopt a greedy approach, i.e., they consider only the current percept or state they are in and choose the best action possible based on the current situation (and possibly the past states the agent has encountered). A reflex agent may or may not have memory. If it does have memory, then it would make decisions based on the history as well as the current world state. A reflex agent may or may not be rational as a greedy approach may not always lead to the optimal solution. For example, in a navigation problem, the agent may end up in a dead end because it just kept on going closer to the destination in terms of displacement from it, but did not consider whether there actually exists a road to the destination. The second type of agent, called planning agent, considers the future consequences of its actions and makes decisions based on these hypothesized consequences of its actions. In order to do this, the agent must have some model of the world, including a transition model, which determines which actions are possible in a given state and how the state changes in response to a given action. The agent also formulates a goal test, which is a test that takes a state as input and determines whether it is a goal or not. The agent then plans ahead about how to reach the goal using the model. A planning may be optimal or complete. A complete planning guarantees that it will find a solution if one exists, but it may not be optimal. An optimal planning guarantees that it will find the optimal solution. Therefore, every optimal planning is also complete, but not vice-versa.

Search Problems
A search problem consists of an initial or start state, a successor function, a cost function and a goal test. The successor function takes a state and an action which is permitted in this state as its inputs, and outputs the state resulting from the action applied on the input state. The set of actions and the successor function implicitly define the state space of the problem. The goal test determines whether a given state is a goal state. A search problem can be modelled as a directed graph with nodes representing states and edges representing actions. A solution to the problem is a sequence of actions or a plan (a path in the directed graph) which transforms the initial state to a goal state. The path cost function takes a path (a sequence of actions) as input and outputs the cost associated with it. An optimal solution is a solution which minimizes the cost of the path. States can be of two types, world states and search states. A world state contains information about each and every detail of the environment, while a search state only contains the information relevant to the problem, i.e., it abstracts away the unnecessary details of the environment. The information contained in a search state depends upon the problem. For example, if the goal is to reach a given city from a starting city (pathing problem), the states would be all the possible cities the agent could be in. Now, if the goal is changed to visiting the maximum possible tourist attractions while visiting each city at most once, the states would be all the possible cities along with the number of tourist attractions in each city. The goal test may correspond to a fixed set of states as in the first example, while it may be just a condition not relating to a particular state as in the second example.

State Space Graphs and Search Trees
A state space graph is a directed graph which is used as an abstract model of a search problem. Each node corresponds to a world state and the arcs (edges) represent actions which lead to successor states (nodes). Each state appears only once in the graph. The goal test corresponds to one or more goal nodes. A search tree consists of nodes where each node represents not only a state but a complete plan to reach that state, i.e., each node in the search tree corresponds to a path in the state space graph. The root of the tree corresponds to the start state and there is a unique path between the root and each node, which corresponds to the specific plan to reach that node. A state may appear multiple times in a search tree, as there can be more than one path to reach the state from the start state. In practice, both the state space graph and the search tree may be quite large and therefore infeasible to fully construct in memory. So, they are constructed incrementally, nodes are added when they are required. 

Searching in a Search Tree
The basic strategy for searching is to maintain a fringe consisting of tree nodes which are potential candidates for expansion in the future. Initially, the fringe contains the root node (initial state). Since each tree node represents a partial plan, the fringe is essentially a set of partial plans which we may expand in the future. By expansion, we mean that we remove the node from the fringe and insert all possible successor nodes of this node into the fringe. The major question is how do we decide the priority of nodes in the fringe, i.e., which node (plan) do we pick next for expansion. This choice directly affects how quickly are we able to find a solution, and therefore, how efficient our algorithm is. The general search algorithm proceeds as follows:

function TreeSearch
Inputs: problem, strategy
Output: a solution or failure
1. Initialise the search tree with the start state as the root node
2. Repeat the following:
3.		If there are no nodes left to expand, return failure
4.		Choose a leaf node from the tree according to strategy
5.		If this node corresponds to a goal state, return the corresponding plan (solution)
6.		Expand this node and insert the resulting successor nodes into the search tree

The tree is represented by a fringe which contains the candidate nodes for expansion. The strategy here determines the priority scheme of the nodes in the fringe, i.e., which node to explore next. Loops in the state space graph can be handled by ensuring that there are no repeated nodes in any given path from root to a node in the search tree. If a node encountered is already present in the path, we do not expand it again.

Uninformed Search Algorithms
Such algorithms don't have any prior information or indication about the position of the goal node(s) in the search tree. These algorithms blindly explore the search tree and have no idea whether their current strategy would eventually lead to a goal node or not. The goal state is only determined once the goal test is performed on the state. DFS, BFS, iterative deepening and UCS come under this category, and are described next.

Depth-First Search (DFS)
This algorithm works by expanding the deepest node in the fringe first. The fringe is a last in first out (LIFO) stack in this case. The last node pushed into the fringe is the first one to be expanded. This effectively results in searching an entire path from the root to a leaf and then proceeding to the next path. This strategy is complete if we are able to take care of cycles, i.e., given a solution exists, it is guaranteed to find it; but the solution found may not be optimal. The reason for this is that as soon as we first encounter (pop from fringe) a goal node, we stop and return the solution, and thus never check more optimal (low cost) paths that may exist. Let the branching factor of the search tree be b and the maximum depth be m. In the worst case, the goal node may be present in the last (deepest) level in the tree. In this case, we are pushing and popping each node in the tree once. The total number of nodes in the tree are b^0 + b^1 + ... + b^m = O(b^m). Thus, the worst-case running time of this algorithm is O(b^m). Now, at any given moment, if we are expanding some node k, the fringe would contain k and its siblings as well as all the ancestors of k up to the root and all the siblings of each ancestor. Thus, the fringe stores at most b nodes at each level, so the space complexity of this algorithm is O(b*m), since the maximum depth is m. Note that we don't need to store the whole tree or graph in memory because we are generating the nodes only when they are first needed. Also, in order to prevent m becoming infinite, we need to take care of cycles. This can be achieved by ensuring that no node occurs more than once in any path from root to a node. If we ensure this, the algorithm would be complete, but still may not be optimal because it returns the leftmost solution.

Breadth-First Search (BFS)
This algorithm works by expand the shallowest node in the fringe first. The fringe is a first in first out (FIFO) queue, which results in a level-order traversal of the search tree. It returns the shallowest solution it finds, i.e., the goal node at the smallest depth is found and its solution is returned. Assuming that the shallowest goal node is at a depth s, the number of nodes explored before reaching the goal is b^0 + b^1 + ... + b^s = O(b^s), so the running time would be O(b^s). Also, in the worst case, when the goal is present in the last level, we would end up exploring the whole tree, giving worst case complexity of O(b^m). Since the fringe may need to store all the nodes at a given depth, the space complexity is O(b^s) (in worst case, s = m). Also, if a solution exists, s must be finite, and thus, this strategy is complete. But we still need to take care of cycles since the algorithm would never terminate if no solution exists and cycles are not accounted for. It gives the optimal solution when the cost of each edge (action) is 1, but may or may not be optimal in other cases.

BFS vs DFS
Although in the worst case both algorithms would end up exploring the whole tree leading to the same asymptotical time complexity, DFS would outperform BFS in terms of space complexity. In some specific scenarios, however one may outperform the other in terms of running time. If some solution exists in the left part of the tree and at a deeper level, DFS would find it faster, while if some solution exists at a shallower level and towards the right part of the tree, BFS would find it faster. If we could approximate the position of the solution in the tree by using some heuristic, we could employ the more suitable strategy.

Iterative Deepening
This algorithm combines the space advantage of DFS with the time advantage of BFS for shallower solutions. It works by first restricting the maximum depth to explore to 1 and then applying DFS, then incrementing it to 2, then again applying DFS, and so on. We are effectively incrementing the search area level by level, until a solution is found. Although it results in redundant computations as we repeatedly explore the top part of the tree, it may be efficient for relatively shallow solutions due to its level-by-level exploration strategy, while still using lesser memory as compared to BFS. Moreover, the major work occurs at the lowest level permitted during some iteration, the redundant work is relatively smaller compared to the new work.

Uniform Cost Search (UCS)
BFS guarantees a solution which consists of the minimum number of steps or hops to reach a goal state from the start state. But it may not provide the optimal solution in the case when each action has some non-negative cost associated with it and our optimal solution is required to minimize the sum of the action costs in the solution. In this case, UCS guarantees an optimal solution. It works by maintaining a fringe implemented by a priority queue, which prioritizes the nodes with smaller cumulative costs. The cumulative cost of a node in the search tree is the total cost of the path from root to this node in the tree (i.e., the cost of the plan to reach this node). The invariant maintained is that whenever we pop a node from the fringe for expanding, all the nodes with smaller cumulative costs than this node have already been explored. No node with a smaller cumulative cost can occur after this node since all the costs are non-negative. Therefore, the first time we pop out a goal node from the fringe, we are sure that the corresponding solution obtained is optimal since all subsequent encounters with a goal state would have a greater cumulative cost. So, we can stop when we first pop out a goal node.
Now, suppose that the cost of the optimal solution is C, and the cost of an edge is at least e. As described above, we would explore all nodes with cumulative costs less than C before reaching the goal node. The goal node would be present at a maximum depth of C/e. Thus, in the worst case, we may end up exploring all the nodes in the levels 0, 1, .., C/e, since every node deeper than the level C/e surely has a cumulative cost greater than or equal to C. So, the worst-case time complexity is O(b^(C/e)). The space complexity is also O(b^(C/e)) as the level C/e contains at most b^(C/e) nodes. Given that the optimal solution has a finite cost and all action costs are non-negative, this algorithm is both complete and optimal.
Since UCS is an uninformed search, it blindly explores all possible directions without any prior information about which direction the goal may be present in.

A note about the fringe
The fringe, in general, is a priority queue, with different ways of assigning priorities for different strategies. A single implementation could suffice for all above algorithms, with the only change being in the definition of priority. For example, in DFS, nodes with later timestamps have higher priority than those with smaller (earlier) timestamps. But in the case of DFS and BFS, the additional log(n) factor of a priority queue can be avoided by using a stack and a queue respectively.

It's just a simulation!
The agent doesn't actually try out all the different strategies in the real world, rather it simulates the above strategies on models (approximations) of the real world. Once a solution is obtained via simulation, it can execute the solution steps (action sequence) in the real world. The planning is carried out using models, thus, the plan obtained is only as good as the model used. The more accurate the model, the more practical the plan for execution in the real world. For example, a map is a model of the street layout of a city in the real world. A map may not account for roads which pass through someone's private property, and hence are restricted. In this case, the search algorithm may include restricted roads in the solution. But it is not the shortcoming of the algorithm, but rather of the model itself. 

Informed Search Algorithms
These algorithms make use of some information about the goal to guide the decision-making process (i.e., which action to choose in a particular state). The agent makes use of some heuristic which it can use to gain some insight about where the goal node might be in the search tree. Here, we define a search heuristic as a function that takes a state as an input and outputs an estimate of how close it is to a goal state. In contrast to the search algorithms described above, which are general algorithms which work for any search problem, the search heuristic varies depending on the particular search problem at hand. Different heuristics could be useful for different situations. For example, for pathing problems, some useful heuristics are Euclidean distance and Manhattan distance. These heuristics measure the actual physical distance between some state and the goal state; hence, they are useful for path-finding problems. If the heuristic value reduces due to some action (moving from one location to another), it indicates that the agent is getting closer to the solution (destination). However, the agent may not necessarily be guided in the right direction by blindly following the heuristic. Next, we see some informed search algorithms based on search heuristics.

Greedy Search 
This algorithm, as the name suggests, works by choosing the node from the fringe which is estimated to be the nearest to a goal node. Continuing the above pathing problem, the greedy algorithm will choose the node with the least value of the heuristic (the node which is nearest to the destination) from the fringe for expansion. Thus, in this algorithm, the priority is assigned to a node based on the value of the heuristic corresponding to that node. This algorithm is complete because it will eventually end up searching the entire search tree, and therefore, if a solution exists, it will be found eventually. It is a common case that the initial guidance provided by the heuristic may lead the agent to end up at a dead end. Once these values of the heuristic are eliminated, other paths would be searched by the algorithm. In the worst case, when there are many paths which come very close to a solution but do not lead to the solution, this algorithm may end up exploring a much greater part of the search tree than even the uninformed search algorithms. Hence, this algorithm may or may not be more efficient than an uninformed algorithm, depending on the particular problem, the environment and the heuristic used. Since this algorithm chooses the best possible actions based on only the current state and doesn't account for the future consequences of its decisions, it is a kind of reflex agent. Due to its greedy nature, the solution obtained may not be optimal. 

A* Search: Combining UCS and Greedy Search
This algorithm tries to combine the optimality property of UCS and the less computation required (efficiency) in Greedy search. The UCS algorithm prioritizes nodes by the path cost or cumulative cost of a node n from the root, also called backward cost g(n). On the other hand, the greedy algorithm prioritizes nodes by the estimated goal proximity of the nodes (based on some heuristic), also called the forward cost h(n). A* search uses the function f(n) = g(n) + h(n) to prioritize the nodes in the fringe. f(n) is an estimate of the sum of how much distance we have already covered and how much distance is approximately left to be covered to reach a goal node. 
Here again, we observe that we stop only when we dequeue a goal node from the fringe, not when we enqueue it. The A* and greedy search only differ in the way they assign priorities to the nodes and can be implemented in the same manner as the uninformed algorithms. The A* search is complete but may not always give the optimal solution, which we see next.

Optimality of A* Search
The A* search may not give optimal solution in cases where the heuristic function overestimates the forward distance of a node from a goal node, i.e., it estimates the forward distance of a node to be larger than the actual distance (path cost) from the goal. In such cases, the algorithm may end up preferring another plan with lower value of f(n) but higher value of the actual path cost. The optimal plan was not chosen because its estimated cost was greater than the estimate for the non-optimal plan, leading to a non-optimal solution. In order to resolve this problem, we need to impose a constraint on the heuristic that it needs to be less than the actual forward cost (i.e., the actual path cost between a node and a goal). In other words, the heuristic should be optimistic instead of pessimistic, it may underestimate the forward cost, but not overestimate it. This constraint ensures optimality in the A* search algorithm.

Inadmissible (Pessimistic) and Admissible (Optimistic) heuristics
Inadmissible heuristics may break the optimality of a search algorithm by overestimating the actual cost of plan, which may lead to the optimal plan not getting chosen for expansion from the fringe. On the other hand, admissible heuristics have the effect of slowing down bad plans but never trap good plans in the fringe because they never overestimate the true costs.

Definition of Admissible Heuristics
A heuristic h is said to be admissible (optimistic) if 0 <= h(n) <= h*(n), for all nodes n, where h*(n) is the actual shortest path cost from the node n to the nearest goal state. For example, in the pacman pathing problem, Manhattan distance is an example of an admissible heuristic, because the minimum actual path cost to the destination can't be less than the Manhattan distance. Similarly, since Euclidean distance is always less than or equal to the Manhattan distance, it is also an admissible heuristic for the above problem. Finding an admissible heuristic is a major step in applying the A* search to a problem.

Proof of Optimality of A* Search
Let's assume that A is an optimal goal node, B is a suboptimal goal node, and h is an admissible heuristic. We claim that A will exit the fringe before B.
Proof:
Let's assume that B is in the fringe and some ancestor n of A (maybe A itself) is also in the fringe (we can safely assume this because the root node is also an ancestor of A and it is present in the fringe at the beginning). We claim that n will be expanded before B. To prove this, we first show that f(n) <= f(A) [1]. We know that f(n) = g(n) + h(n) and f(n) <= g(A) (since h is admissible). Also, g(A) = f(A) since h(A) = 0. Thus, f(n) <= f(A). Secondly, we observe that f(A) < f(B) [2], because g(A) < g(B) (since B is suboptimal) and h(A) = h(B) = 0 (since h for goal node is 0). From [1] and [2], we conclude that f(n) < f(B), and hence n will be expanded before B. Now, since n was chosen arbitrarily, it implies that all ancestors of A would be expanded before B. Once the parent of A is expanded and A enters the fringe, it will be expanded (popped out) of the fringe before B since f(A) < f(B). Hence, the claim is proved.

From the above claim, we can conclude that the A* search is optimal, given that the heuristic used is admissible.

Properties of A* Search
The A* search when used along with an admissible heuristic function, is not only optimal, but it also needs to search less than UCS in order to reach a solution. A heuristic is relatively better if it approximates the shortest actual cost to a goal state more closely than some other heuristic. The calculation of such a heuristic for each node increases the amount of computation required for each node, but reduces the search area in the search tree. Compared to the greedy search, A* explores more nodes in the search tree, but ensures an optimal solution, because it also considers the backward distance.
In practice, an inadmissible heuristic may be preferred over an admissible one when computing the former is considerably faster than the latter, and non-optimal solutions may be acceptable.

Creating Admissible Heuristics
It often involves relaxing the problem at hand by introducing new actions and ensuring their cost is less than the actual path costs. For example, in a pathing problem in a city, we may relax the constraint to only travel through roads and allow flying straight to the destination. This would provide the Euclidean distance as an admissible heuristic for this problem.
