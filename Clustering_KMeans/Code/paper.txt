Authorship attribution supported by statistical or computational methods has a long history
starting from 19th century and marked by the seminal study of Mosteller and Wallace (1964)
on the authorship of the disputed Federalist Papers. During the last decade, this scientific field
has been developed substantially taking advantage of research advances in areas such as
machine learning, information retrieval, and natural language processing. The plethora of
available electronic texts (e.g., e-mail messages, online forum messages, blogs, source code,
etc.) indicates a wide variety of applications of this technology provided it is able to handle
short and noisy text from multiple candidate authors. In this paper, a survey of recent
advances of the automated approaches to attributing authorship is presented examining their
characteristics for both text representation and text classification. The focus of this survey is
on computational requirements and settings rather than linguistic or literary issues. We also
discuss evaluation methodologies and criteria for authorship attribution studies and list open
questions that will attract future work in this area. 
The main idea behind statistically or computationally-supported authorship attribution is that
by measuring some textual features we can distinguish between texts written by different
authors. The first attempts to quantify the writing style go back to 19th century, with the
pioneering study of Mendenhall (1887) on the plays of Shakespeare followed by statistical
studies in the first half of the 20th century by Yule (1938; 1944) and Zipf (1932). Later, the
detailed study by Mosteller and Wallace (1964) on the authorship of ‘The Federalist Papers’
(a series of 146 political essays written by John Jay, Alexander Hamilton, and James
Madison, twelve of which claimed by both Hamilton and Madison) was undoubtedly the most
influential work in authorship attribution. Their method was based on Bayesian statistical
analysis of the frequencies of a small set of common words (e.g., ‘and’, ‘to’, etc.) and
produced significant discrimination results between the candidate authors.
Essentially, the work of Mosteller and Wallace (1964) initiated non-traditional
authorship attribution studies, as opposed to traditional human expert-based methods. Since
then and until the late 1990s, research in authorship attribution was dominated by attempts to
define features for quantifying writing style, a line of research known as ‘stylometry’
(Holmes, 1994; Holmes, 1998). Hence, a great variety of measures including sentence length,
word length, word frequencies, character frequencies, and vocabulary richness functions had
been proposed. Rudman (1998) estimated that nearly 1,000 different measures had been
proposed that far. The authorship attribution methodologies proposed during that period were
computer-assisted rather than computer-based, meaning that the aim was rarely at developing
a fully-automated system. In certain cases, there were methods achieved impressive
preliminary results and made many people think that the solution of this problem was too
close. The most characteristic example is the CUSUM (or QSUM) technique (Morton &
Michealson, 1990) that gained publicity and was accepted in courts as expert evidence.
However, the research community heavily criticized it and considered it generally unreliable
(Holmes & Tweedie, 1995). Actually, the main problem of that early period was the lack of 
2
objective evaluation of the proposed methods. In most of the cases, the testing ground was
literary works of unknown or disputed authorship (e.g., the Federalist case), so the estimation
of attribution accuracy was not even possible. The main methodological limitations of that
period concerning the evaluation procedure were the following:
• The textual data were too long (usually including entire books) and probably not
stylistically homogeneous.
• The number of candidate authors was too small (usually 2 or 3).
• The evaluation corpora were not controlled for topic.
• The evaluation of the proposed methods was mainly intuitive (usually based on
subjective visual inspection of scatterplots).
• The comparison of different methods was difficult due to lack of suitable
benchmark data.
Since the late 1990s, things have changed in authorship attribution studies. The vast
amount of electronic texts available through Internet media (emails, blogs, online forums, etc)
increased the need for handling this information efficiently. This fact had a significant impact
in scientific areas such as information retrieval, machine learning, and natural language
processing (NLP). The development of these areas influenced authorship attribution
technology as described below:
• Information retrieval research developed efficient techniques for representing and
classifying large volumes of text.
• Powerful machine learning algorithms became available to handle multidimensional
and sparse data allowing more expressive representations. Moreover,
standard evaluation methodologies have been established to compare different
approaches on the same benchmark data.
• NLP research developed tools able to analyze text efficiently and providing new
forms of measures for representing the style (e.g., syntax-based features).
More importantly, the plethora of available electronic texts revealed the potential of
authorship analysis in various applications (Madigan, Lewis, Argamon, Fradkin, & Ye, 2005)
in diverse areas including intelligence (e.g., attribution of messages or proclamations to
known terrorists, linking different messages by authorship) (Abbasi & Chen, 2005), criminal
law (e.g., identifying writers of harassing messages, verifying the authenticity of suicide
notes) and civil law (e.g., copyright disputes) (Chaski, 2005; Grant, 2007), computer forensics
(e.g., identifying the authors of source code of malicious software) (Frantzeskou, Stamatatos,
Gritzalis, & Katsikas, 2006), in addition to the traditional application to literary research (e.g.,
attributing anonymous or disputed literary works to known authors) (Burrows, 2002; Hoover,
2004a). Hence, (roughly) the last decade can be viewed as a new era of authorship analysis
technology, this time dominated by efforts to develop practical applications dealing with realworld
texts (e.g., e-mails, blogs, online forum messages, source code, etc.) rather than solving
disputed literary questions. Emphasis is now given to the objective evaluation of the proposed
methods as well as the comparison of different methods based on common benchmark
corpora (Juola, 2004). In addition, factors playing a crucial role in the accuracy of the
produced models are examined, such as the training text size (Marton, Wu, & Hellerstein,
2005; Hirst & Feiguina, 2007), the number of candidate authors (Koppel, Schler, Argamon, &
Messeri, 2006), and the distribution of training texts over the candidate authors (Stamatatos,
2008).
In the typical authorship attribution problem, a text of unknown authorship is assigned to
one candidate author, given a set of candidate authors for whom text samples of undisputed
authorship are available. From a machine learning point-of-view, this can be viewed as a
multi-class single-label text categorization task (Sebastiani, 2002). This task is also called
authorship (or author) identification usually by researchers with a background in computer
science. Several studies focus exclusively on authorship attribution (Stamatatos, Fakotakis, &
Kokkinakis, 2001; Keselj, Peng, Cercone, & Thomas, 2003; Zheng, Li, Chen, & Huang, 
3
2006) while others use it as just another testing 
This paper presents a survey of the research advances in this area during roughly the last
decade (earlier work is excellently reviewed by Holmes (1994; 1998)) emphasizing
computational requirements and settings rather than linguistic or literary issues. First, in
Section 2, a comprehensive review of the approaches to quantify the writing style is
presented. Then, in Section 3, we focus on the authorship identification problem (as described
above). We propose the distinction of attribution methodologies according to how they handle
the training texts, individually or cumulatively (per author), and examine their strengths and
weaknesses across several factors. In Section 4, we discuss the evaluation criteria of
authorship attribution methods while in Section 5 the conclusions drawn by this survey are
summarized and future work directions in open research issues are indicated.
2. Stylometric Features
Previous studies on authorship attribution proposed taxonomies of features to quantify the
writing style, the so called style markers, under different labels and criteria (Holmes, 1994;
Stamatatos, Fakotakis, & Kokkinakis, 2000; Zheng, et al., 2006). The current review of text
representation features for stylistic purposes is mainly focused on the computational
requirements for measuring them. First, lexical and character features consider a text as a
mere sequence of word-tokens or characters, respectively. Note that although lexical features
are more complex than character features, we start with them for the sake of tradition. Then,
syntactic and semantic features require deeper linguistic analysis, while application-specific
features can only be defined in certain text domains or languages. The basic feature categories
and the required tools and resources for their measurement are shown in Table 1. Moreover,
various feature selection and extraction methods to form the most appropriate feature set for a
particular corpus are discussed.
2.1 Lexical Features
A simple and natural way to view a text is as a sequence of tokens grouped into sentences,
each token corresponding to a word, number, or a punctuation mark. The very first attempts
to attribute authorship were based on simple measures such as sentence length counts and
word length counts (Mendenhall, 1887). A significant advantage of such features is that they
can be applied to any language and any corpus with no additional requirements except the
availability of a tokenizer (i.e., a tool to segment text into tokens). However, for certain
natural languages (e.g., Chinese) this is not a trivial task. In case of using sentential
information, a tool that detects sentence boundaries should also be available. In certain text
domains with heavy use of abbreviations or acronyms (e.g., e-mail messages) this procedure
may introduce considerable noise in the measures.
The vocabulary richness functions are attempts to quantify the diversity of the
vocabulary of a text. Typical examples are the type-token ratio V/N, where V is the size of the 
4
TABLE 1. Types of stylometric features together with computational tools and resources
required for their measurement (brackets indicate optional tools).
vocabulary (unique tokens) and N is the total number of tokens of the text, and the number of
hapax legomena (i.e., words occurring once) (de Vel, Anderson, Corney, & Mohay, 2001).
Unfortunately, the vocabulary size heavily depends on text-length (as the text-length
increases, the vocabulary also increases, quickly at the beginning and then more and more
slowly). Various functions have been proposed to achieve stability over text-length, including
K (Yule, 1944), and R (Honore, 1979), with questionable results (Tweedie & Baayen, 1998).
Hence, such measures are considered unreliable to be used alone.
The most straightforward approach to represent texts is by vectors of word frequencies. The
vast majority of authorship attribution studies are (at least partially) based on lexical features
to represent the style. This is also the traditional bag-of-words text representation followed by
researchers in topic-based text classification (Sebastiani, 2002). That is, the text is considered
as a set of words each one having a frequency of occurrence disregarding contextual
information. However, there is a significant difference in style-based text classification: the
most common words (articles, prepositions, pronouns, etc.) are found to be among the best
features to discriminate between authors (Burrows, 1987; Argamon & Levitan, 2005). Note
that such words are usually excluded from the feature set of the topic-based text classification
methods since they do not carry any semantic information and they are usually called
‘function’ words. As a consequence, style-based text classification using lexical features
require much lower dimensionality in comparison to topic-based text classification. In other
words, much less words are sufficient to perform authorship attribution (a few hundred
words) in comparison to a thematic text categorization task (several thousand words). More
importantly, function words are used in a largely unconscious manner by the authors and they
are topic-independent. Thus, they are able to capture pure stylistic choices of the authors
across different topics.
The selection of the specific function words that will be used as features is usually based
on arbitrary criteria and requires language-dependent expertise. Various sets of function
words have been used for English but limited information was provided about the way they
have been selected: Abbasi and Chen (2005) reported a set of 150 function words; Argamon,
Saric, and Stein (2003) used a set of 303 words; Zhao and Zobel (2005) used a set of 365
function words; 480 function words were proposed by Koppel and Schler (2003); another set
of 675 words was reported by Argamon, Whitelaw, Chase, Hota, Garg, and Levitan (2007).
A simple and very successful method to define a lexical feature set for authorship
attribution is to extract the most frequent words found in the available corpus (comprising all
the texts of the candidate authors). Then, a decision has to be made about the amount of the
frequent words that will be used as features. In the earlier studies, sets of at most 100 frequent
words were considered adequate to represent the style of an author (Burrows, 1987; Burrows,
1992). Another factor that affects the feature set size is the classification algorithm that will
be used since many algorithms overfit the training data when the dimensionality of the
problem increases. However, the availability of powerful machine learning algorithms able to
deal with thousands of features, like support vector machines (Joachims, 1998), enabled
researchers to increase the feature set size of this method. Koppel, Schler, and BonchekDokow
(2007) used the 250 most frequent words while Stamatatos (2006a) extracted the
1,000 most frequent words. On a larger scale, Madigan, et al., (2005) used all the words that
appear at least twice in the corpus. Note that the first dozens of most frequent words of a
corpus are usually dominated by closed class words (articles, prepositions etc.) After a few
hundred words, open class words (nouns, adjectives, verbs) are the majority. Hence, when the
dimensionality of this representation method increases, some content-specific words may also
be included in the feature set.
Despite the availability of a tokenizer, word-based features may require additional tools
for their extraction. This would involve from simple routines like conversion to lowercase to
more complex tools like stemmers (Sanderson & Guenter, 2006), lemmatizers (Tambouratzis,
Markantonatou, Hairetakis, Vassiliou, Carayannis, & Tambouratzis, 2004; Gamon, 2004), or
detectors of common homographic forms (Burrows, 2002). Another procedure used by van
Halteren (2007) is to transform words into an abstract form. For example, the Dutch word
‘waarmaken’ is transformed to ‘#L#6+/L/ken’, where the first L indicates low frequency, 6+ 
indicates the length of the token, the second L a lowercase token, and ‘ken’ are its last three
characters.
The bag-of-words approach provides a simple and efficient solution but disregards wordorder
(i.e., contextual) information. For example, the phrases ‘take on’, ‘the second take’ and
‘take a bath’ would just provide three occurrences of the word ‘take’. To take advantage of
contextual information, word n-grams (n contiguous words aka word collocations) have been
proposed as textual features (Peng, et al., 2004; Sanderson & Guenther, 2006; CoyotlMorales,
Villaseñor-Pineda, Montes-y-Gómez, & Rosso, 2006). However, the classification
accuracy achieved by word n-grams is not always better than individual word features
(Sanderson & Guenther, 2006; Coyotl-Morales, et al., 2006). The dimensionality of the
problem following this approach increases considerably with n to account for all the possible
combinations between words. Moreover, the representation produced by this approach is very
sparse, since most of the word combinations are not encountered in a given (especially short)
text making it very difficult to be handled effectively by a classification algorithm. Another
problem with word n-grams is that it is quite possible to capture content-specific information
rather than stylistic information (Gamon, 2004).
From another point of view, Koppel and Schler (2003) proposed various writing error
measures to capture the idiosyncrasies of an author’s style. To that end, they defined a set of
spelling errors (e.g., letter omissions and insertions) and formatting errors (e.g., all caps
words) and they proposed a methodology to extract such measures automatically using a spell
checker. Interestingly, human experts mainly use similar observations in order to attribute
authorship. However, the availability of accurate spell checkers is still problematic for many
natural languages. 
According to this family of measures, a text is viewed as a mere sequence of characters. That
way, various character-level measures can be defined, including alphabetic characters count,
digit characters count, uppercase and lowercase characters count, letter frequencies,
punctuation marks count, etc. (de Vel, et al., 2001; Zheng, et al., 2006). This type of
information is easily available for any natural language and corpus and it has been proven to
be quite useful to quantify the writing style (Grieve, 2007).
A more elaborate, although still computationally simplistic, approach is to extract
frequencies of n-grams on the character-level. For instance, the character 4-grams of the
beginning of this paragraph would be1
: |A_mo|, |_mor|, |more|, |ore_|, |re_e|, etc. This
approach is able to capture nuances of style including lexical information (e.g., |_in_|, |text|),
hints of contextual information (e.g., |in_t|), use of punctuation and capitalization, etc.
Another advantage of this representation is its ability to be tolerant to noise. In cases where
the texts in question are noisy containing grammatical errors or making strange use of
punctuation, as it usually happens in e-mails or online forum messages, the character n-gram
representation is not affected dramatically. For example, the words ‘simplistic’ and
‘simpilstc’ would produce many common character trigrams. On the other hand, these two
words would be considered different in a lexically-based representation. Note that in stylebased
text categorization such errors could be considered personal traits of the author (Koppel
& Schler, 2003). This information is also captured by character n-grams (e.g., in the
uncommon trigrams |stc| and |tc_|). Finally, for oriental languages where the tokenization
procedure is quite hard, character n-grams offer a suitable solution (Matsuura & Kanada,
2000). As can be seen in Table 1, the computational requirements of character n-gram
features are minimal. 
Note that, as with words, the most frequent character n-grams are the most important
features for stylistic purposes. The procedure of extracting the most frequent n-grams is
language-independent and requires no special tools. However, the dimensionality of this
representation is considerably increased in comparison to the word-based approach 
(Stamatatos, 2006a; Stamatatos, 2006b). This happens because character n-grams capture
redundant information (e.g., |and_|, |_and|) and many character n-grams are needed to
represent a single long word.
The application of this approach to authorship attribution has been proven quite
successful. Kjell (1994) first used character bigrams and trigrams to discriminate the
Federalist Papers. Forsyth and Holmes (1996) found that bigrams and character n-grams of
variable-length performed better than lexical features in several text classification tasks
including authorship attribution. Peng, Shuurmans, Keselj, & Wang (2003), Keselj et al.
(2003), and Stamatatos (2006b) reported very good results using character n-gram
information. Moreover, one of the best performing algorithms in an authorship attribution
competition organized in 2004 was also based on a character n-gram representation (Juola,
2004; Juola, 2006). Likewise, a recent comparison of different lexical and character features
on the same evaluation corpora (Grieve, 2007) showed that character n-grams were the most
effective measures (outperformed in the specific experiments only by a combination of
frequent words and punctuation marks).
An important issue of the character n-gram approach is the definition of n, that is, how
long should the strings be. A large n would better capture lexical and contextual information
but it would also better capture thematic information. Furthermore, a large n would increase
substantially the dimensionality of the representation (producing hundreds of thousands of
features). On the other hand, a small n (2 or 3) would be able to represent sub-word (syllablelike)
information but it would not be adequate for representing the contextual information. It
has to be underlined that the selection of the best n value is a language-dependent procedure
since certain natural languages (e.g., Greek, German) tend to have long words in comparison
to English. Therefore, probably a larger n value would be more appropriate for such
languages in comparison to the optimal n value for English. The problem of defining a fixed
value for n can be avoided by the extraction of n-grams of variable-length (Forsyth &
Holmes, 1996; Houvardas & Stamatatos, 2006). Sanderson and Guenter (2006) described the
use of several sequence kernels based on character n-grams of variable-length and the best
results for short English texts were achieved when examining sequences of up to 4-grams.
Moreover, various Markov models of variable order have been proposed for handling
character-level information (Khmelev & Teahan, 2003a; Marton, et al., 2005). Finally, Zhang
and Lee (2006) constructed a suffix tree representing all possible character n-grams of
variable-length and then extracted groups of character n-grams as features.
A quite particular case of using character information is the compression-based
approaches (Benedetto, Caglioti, & Loreto, 2002; Khmelev & Teahan, 2003a; Marton, et al.,
2005). The main idea is to use the compression model acquired from one text to compress
another text, usually based on off-the-shelf compression programs. If the two texts are written
by the same author, the resulting bit-wise size of the compressed file will be relatively low.
Such methods do not require a concrete representation of text and the classification algorithm
incorporates the quantification of textual properties. However, the compression models that
describe the characteristics of the texts are usually based on repetitions of character sequences
and, as a result, they can capture sub-word and contextual information. In that sense, they can
be considered as character-based methods. 
A more elaborate text representation method is to employ syntactic information. The idea is
that authors tend to use similar syntactic patterns unconsciously. Therefore, syntactic
information is considered more reliable authorial fingerprint in comparison to lexical
information. Moreover, the success of function words in representing style indicates the
usefulness of syntactic information since they are usually encountered in certain syntactic
structures. On the other hand, this type of information requires robust and accurate NLP tools
able to perform syntactic analysis of texts. This fact means that the syntactic measure
extraction is a language-dependent procedure since it relies on the availability of a parser able 
In a similar framework, tools that perform partial parsing can be used to provide
syntactic features of varying complexity (Luyckx & Daelemans, 2005; Uzuner & Katz, 2005;
Hirst & Feiguina, 2007). Partial parsing is between text chunking and full parsing and can
handle unrestricted text with relatively high accuracy. Hirst and Feiguina (2007) transformed
the output of a partial parser into an ordered stream of syntactic labels, for instance the
analysis of the phrase ‘a simple example’ would produce the following stream of labels: 
It should be clear by now, the more detailed the text analysis required for extracting
stylometric features, the less accurate (and the more noisy) the produced measures. NLP tools
can be applied successfully to low-level tasks, such as sentence splitting, POS tagging, text
chunking, partial parsing, so relevant features would be measured accurately and the noise in
the corresponding datasets remains low. On the other hand, more complicated tasks such as
full syntactic parsing, semantic analysis, or pragmatic analysis cannot yet be handled
adequately by current NLP technology for unrestricted text. As a result, very few attempts
have been made to exploit high-level features for stylometric purposes.
Gamon (2004) used a tool able to produce semantic dependency graphs but he did not
provide information about the accuracy of this tool. Two kinds of information were then
extracted: binary semantic features and semantic modification relations. The former
concerned number and person of nouns, tense and aspect of verbs, etc. The latter described
the syntactic and semantic relations between a node of the graph and its daughters (e.g., a
nominal node with a nominal modifier indicating location). Reported results showed that
semantic information when combined with lexical and syntactic information improved the
classification accuracy.
McCarthy, Lewis, Dufty, and McNamara (2006) described another approach to extract
semantic measures. Based on WordNet (Fellbaum, 1998) they estimated information about
synonyms and hypernyms of the words, as well as the identification of causal verbs.
Moreover, they applied latent semantic analysis (Deerwester, Dumais, Furnas, Landauer, &
Harshman, 1990) to lexical features in order to detect semantic similarities between words
automatically. However, there was no detailed description of the features and the evaluation
procedure did not clarify the contribution of semantic information in the classification model.
Perhaps the most important method of exploiting semantic information so far was
described by Argamon, et al. (2007). Inspired by the theory of Systemic Functional Grammar
(SFG) (Halliday, 1994) they defined a set of functional features that associate certain words
or phrases with semantic information. In more detail, in SFG the ‘CONJUNCTION’ scheme