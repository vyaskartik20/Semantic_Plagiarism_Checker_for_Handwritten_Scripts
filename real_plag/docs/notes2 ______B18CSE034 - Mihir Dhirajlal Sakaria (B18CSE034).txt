In the previous part we had described about the number sliding problem and in the end we had relaxed certain constraints. In the relaxed approach our computation had decreased by a significant amount. Whenever we do a relaxation we ensure that it will be acting as a lower bound. Each heuristic function has a lower bound for it to be admissible or not. So when we relax some constraint we ensure that it is the lower bound. 
For a problem, there can be various admissible heuristics. So there will be a tradeoff between accuracy of heuristics and computation of the heuristic. 
On the above example we can choose manhatten distance as the heuristic function rather than the tiles out of place. So in comaprison to previous heuristic, there will be more computation however it will provide better approximation of the problem. So using this heuristic function in relaxed part, there will be lesser calculation involved. 
So how about we administer the actual cost as the heuristic function? We can do it, also it will be the admissible heuristic function as it will always be less than or equal to the actual cost. But the thing which is wrong here is that we will have to increase the computation and the nodes visited will be larger than some other better heuristic function. So it maynot be always be feasible.
So there is always a tradeoff between a quality of estimate and the number of nodes expanded and searched. So with A*, if we use actual cost as the heuristic function, then it will become UCS and thus computation will be the same as UCS. 
A* will become UCS(Uniform Cost Search) if we choose heuristic function as 0. 0 always can be chosen as a heuristic function. 
Dominance : We can say that a heuristic function ha dominates hb if for every node , ha(n) >=hb(n) .So if ha is admissible , then hb is also admissible. Also it may happen that we are not able to compare two heuristics, but there is some ordering between them, which helps us select one.

Graph Search. Till now we have discussed Search Trees. In tree search it may happen that we have to do a redundant search. But in graph search this is avoided.
So when we do a graph search , then we store the expanded node in a set, and not in the list as in list we will have to traverse through the whole list in order to know about the existance of a particular node or not. So it will increase computation. Set helps here, it wont take long to find that the node is comouted or not.
So whenever we expand a node, we first check if it is in the expanded set or not. If it is there we will not expand it. Else we will expand it and store it. So this appraoch is complete as it will eventually give a solution. The optimality of the appraoch can be decided by the way we choose to expand the node from the fringe.
There can be cases in which , the A* approach in graph search may fail to give us the optimal solution. Lets say we have a graph with 5 nodes S,A,B,C and G with heuristic distance to G from node 2,4,1,1,0 repectively. We have edges from S to A, S to B, A to C, B to C,and C to G. Each edge has weight 1,1,1,2 and 3 respectively. Using A* search approach here will give us S,B,C,G as the answer but the optimal solution would be S,A,C,G.
This is because A(1,4) will be regarded as higher than B(1,1). The problem here is in heuristic function. So we may have to make changes in it, this is because if we ignore the heuristic part, we get correct answer. 
So this is where we will use the term called consistency of heuristics. Earlier we used term admissible heuristic function , which means that the heuristic costs<= actual costs. Now , we will use admissible and consistent heuristic function. 
In case of consistency, the h(A) - h(B) <= arc cost(AB) or the edge cost AB. Also h(A)- h(B) should also be greater than 0. So in previous example, the heuristic function was inconsistent , which is why we got wrong answer. 
We can see, that if we use a consistent heuristic function, then the function f(n) = h(n) + g(n) will not decrease.

Considering when we use consistent heuristic function in A* appraoch. So in tree search, the priority is always based on the f function.Thus we are expanding in the increasing value of f in fringe. So in graph search, we are about to expand a node and we find that there is another path which is of less cost then will explore that path first. f function will be set such a way by the consistent heuristic function that it will help us reach the optimal path.
Thus we can say that A* search gives optimal solution (given we are using consistent heuristic)
So to summarise, A* uses both forward and backward costs, is optimal with admissible/consistent heuristics. And the key part is the design of heuristic. We often try to use relaxed problem.
Algorithm is some thing like, we are given a problem and a fringe. Algorithm will return either a solution or a fringe. Add the root node in the fringe and add its adjacent nodes to the fringe. Expand the node from the fringe whose cost is lowest and remove it from the fringe and add the expanded node in a set. Repeat untill we reach the solution node. If in any case the fringe is empty, then we return a failure.  

First the question arise, why do we need search for. We assume that the world has single agent,deterministic actions, fully observable states, which change on the basis of actions. Planning is needed because we are given a goal and we are interested in identifying the path of the goal. And there can be multiple paths for the goal like different depths, cost, hops etc. So lets say in A* search algorithm, it has a heuristic function which helps us guide to the final destination. So characteristics like these are characteristics of planning.
Secondly it is identification of tasks. In this we are given goals and we have to look for the goal. The goal is important. For example we are two values x,and y. So lets say x can take values from positive integers and y takes values from negative integers. This puts a constraint over the values of x and y.Our objective may be to assign these values as per the constraint. So it may happen that the goals might be at the same depth, but the approach might be different.

Standard Search Problems. In this, each state is considered an arbitrary data structure which defines the current moment or value based on our actions. It basically behaves like a black box. A state changes on the basis of the successor function which is determined by the algorithm. 
Constraint Satisfaction Problem is a subset of search problems. In this we are given variables(X) which are to be set in accordance to the domain given to us(D). So in this goal test can be the set of constraints with which we can set the variables to the values equal to some subset of D in accordance to the constraints. An example is map coloring problem. This is basically we have a set of states in a country and we have to colour these states. The constraints are such that no two adjacent state should have the same colour. We are also given the set of colors to choose upon. 
The method of showing these constraints can be different, i;e implicit and explicit way. Implicit can be like Na != Nb (node a != Node b). Explicit way can be like (Na,Nb) should belong to {(red,green), (green, blue) ....}. So the solution can be  assignment of the variables from one of the given possibility.
Similarly we can have an example of having a chess board of 4 X 4 in which we have to place 4 queens such that they cannot kill eachother. So the variables can the the co-ordinates of the chess boards. We can set the constraints such that the queens cannot kill each other. Like in a single row, place only 1 queen and in a single column place a single queen. Don't place a queen on the diagonal of already placed queen. We need one more constraint in which we can check if the total queens placed is equal to 4. Because placing no queen also satisfies the given constraint. The constraint mentioned here is implicit constraint. 
Better approach can be to associate the variables to each row. Because in each row, these will be one queen. lets say Q1,Q2...QN (generalizing the problem to N) . (Q1,Q2) will belong to {(2,4),(1,3)...} (difference between the indicies should be greater than or equal to 2 so that they dont lie in the diagonal). These are just the constraint for a pair. Similarly we have to form constraints for all pairs and ensure that all of them are being followed. 
A binary constraint problem is a problem in which every constraint has atmost 2 variables. For example a graph coloring problem. Also the binary graph constraint, in which the nodes are the variables and edges determine the constraint. So a the n-queens problem can also be designed according to the binary grah constraint. The nodes determining the positions of the queen and edges determining constraints like only a single queen in a row or something like that.
Another example is cryptarathmetic. In cryptorathmetic , we give input numbers in encrypted form and it should give result in the encrypted form itself. So we maintain the variables as the characters, and in case of addition another variable is needed which determines the carry for the addition.  Their domain can be numbers ranging from 0 to 9. constraints should be such that if two variable's sum exceed, then they should be split into two (ten's place and one's place) and the two should be assigned to the correct variables. Another constraint can be that the variables should be different and each should take different value. So when there are more than two constraints to take care of, we put different kind of auxillary nodes and we connect variables to these nodes. So it would denote the constraint containing more than 2 variables. This is how we can represent more than 2 constraints in a tree. These auxillary nodes do not correspond to any variable.
Another example can be a sudoku. In this we are given 9x9 grid and we have to fill the grid with numbers 1-9. So the variables can be the emppty grid and the constraints can be that no number should repeat in the row,column and 3x3 grid. So connect these variables of rows using a auxilary node to represent the constraint that only 1 number can occur only 1 time.

The Waltz Algorithm is used for interpreting the line diagram of the solid 3-D objects that we show to it. It is one of the early example of CSP. It basically studies the geometry. So lets say we are given a cube in which there is a cube shape hole on one of the corner. So the corner in the cube shape hole can be said as the inner corner. So if the input as inner corner is given, then using waltz algorithm we can find all other inner corners , and similarly we can find all outer corners. All the constraints are laid such that we are able to find the desired result. 
There can be various varieties of CSPs. For example discrete variable, which have further two categories, finite domain, and infinite domain. (Queen placing problem and length problem etc) . Another is continuous variable problems in which we can have variables like timestamp or some other linear constraints. In general we have methods with which we can solve continuous variable problems more quickly than the discrete variable problems.
Another classification can be on the basis of varierty of constraints. There are three types of such constraints. 
One is unary constraint in which a variable cannot be equal to a certain value, so all the other values except that value become its domain. Second is binary constraints which involve pair of variables. example in map colouring problem. Third is higher order constraint which involve more than 2 variables. For example in sudoku problem. We saw that in CSP the paths are not usually important but to reach the goal is our primary job. For example in, map coloring problem we can have multiple solutions or in queen placing problem too. In real world too, we mostly prefer constraint satisfaction part and not the optimization part. 
Some examples of CSPs can be scheduling problems, hardware configurations, Fault diagnosis etc. In scheduling problems, for example in class scheduling problems, we have to schedule the classes such that minimum numbwer of classes are used for conducting the classes etc.

Standard Search Formulations of the CSP. In this, we can come up with any search algorithm like bfs, dfs etc.Intially all the variables shall be unassigned. The states (or nodes) can be made as the values filled in the variables. The successor function will assign values to the varaible in accordance to the constraints. Our final goal would be to assign values to all the variables and satisfying all the constraints given to us. 
Let us study about the results of applying the bfs in CSP. We are discussing map coloring problem here. Initally none of the node is assigned any colour. Then one of the node will be coloured with one of the possible colour. So in total there can be (number of nodes)*(number of colours) children of the root node in search tree. This will give us a significant branching factor. First we will have to complete the whole tree then we will have to check if all the constraints are being matched or not. This will lead to a very high computational time, since we are forming the whole tree everytime.
For DFS, we will first color the entire tree, then check of the constraints. So basically we are able to get a solution within n steps(n is the number of nodes). So this is comparitively faster than bfs. So we will mostly use dfs in CSPs. 
Obviously we can make make changes to the DFS approach here inorder to reach the solution more quickly. In dfs, we saw that we assign some color to the node and then we keep on proceeding with that colour, even if the assignment of color is violating the constraint. So this is like the extra computation which can be avoided if we check the violation of constraint at early stage and stop that iteration there itself. This will be done using Backtracking Search. 
Backtracking search algorithm is uninformed search algorithm since we are using dfs as base algorithm and we are not using any other extra information about the goal. We just assign order to the variables. This means that we assign values to the variable in a certain order. Also we will check the constraints on every assignment. If violation of any constraint take place then we stop at that point, go back and check other nodes. So using these are two improvements in our dfs algorithm and we name this method as backtracking search.
By ordering we mean that we assign to some variable lets say x1, first. Then x2,x3 and so on. So at level one, we will assign lets some value , lets say 3 to it, and then go to variable x2 and assign some other value lets say 4 to it. Now we check if any of the constraint is being violated or not, if it is then we stop there , go back and change the value assigned to x2. if for all values possible for x2 the constraints are violated then we assign some other value to x1 and proceed similarly.
First of all we are given the variables , so we first arrange them in order. Then we also arrange the values in the domain too. Then we assign the values and proceed as per the method explained above. Ordering the variables and values improves the performance of backtracking.
Basically if we order the values which are to be assigned, we can overcome some of the ways which could lead to the violation of restrictions. So arranging the values to be assigned is a kind of filtering. Also filtering means to detect inevitable failure early. We also have to think about the structure of the problem, which will help us to validate the constraints of the given problem.
Forward Filtering is basically keeping the track of the domain of unasssigned variables and reject the options that could lead to a possible failure or violation of constraint. So a forward checking will be to cross the values from the domain of unassingned varaible which will lead to violation. For example in map coloring problem, we have 2 nodes n1,n2, and 3 possible color R,B,G. If we assign n1 to R then we will cross off R from the domain of n2. This will help in forward checking. So in forward checking we basically check for the empty domain for the variables that are yet to be assigned values.
We define consistency of an arc as follows. An arc from A->B is said to be consistent if and only if for every A in tail there is some B on the head which could be assigned without conflicting with any constraint. To maintain the consistency of the arc, we will remove values only from the domain of the tail of the arc, not from domain of head of arc. Once we remove the value from any of the domain, we will have to check for rest of the arcs too. So in case of n nodes, we will have to check for n*n arcs again if we delete some value. This means we have to do a lot of computation on each assignment. So it can be said to be a disadvantage of this method. The runtime of algorithm is O(n^2*d^3) where n is the number of head nodes and d is size of domain. This is because of we have to check for each arc again after editing the value of the domain of a node. Also we then will be checking for each value in domain of tail node to domain of head node that makes it d^2. But since in case of changing the value, we have
to again iterate over the domain and again check for the domain of the tail and head node. Thus it becomes d^3.
Limitations of arc consistency can be said as that we are checking for only a pair at a time, so this may lead to having no solution left(without knowing it), or one or two solutions left. Also arc consistency runs inside a backtracking search, it is not run differently.
We earlier listed that ordering helps in finding the solution and helps in filtering. So usually in ordering, we start with the variables which have the smallest domains. This method is called Minimum Remaining Values(MRV). We always choose the minimum sized domain variable so that in future we donot have any constraint violation. This is kind of greedy approach and the variable is called most constrained variable. When we have multiple choice to assign some value to the variable, then we assign only that value which will lead to very less shrinking of domain of other varaibles. This value ordering is called least constraining value. Again we choose the part least, so that the chances of violating the constraints also reduce. 

In all of the above search algorithms, the one thing that was common was that we were given start state and the goal state, and we somehow had to make it to the goal state. So we saw that the agent basically simulates the paths first, and when it finds one as per the conditions, then it moves on the path. So it first plans ,and then moves. But in case of local search algorithms, the agent will not work in this way. In local search algorithm(Hill Climb Algorithm), the agent first looks at the neighbouring states and then makes a move. 
Some examples of algorithms for the local search. Random search, Random walk in which we randomly pick a neighbour of the node and go to that node. In case of dead end, we go back and choose another and return the path if we reach the destination. 
lets check another example, Hill climbing(greedy local search) max version. We will be given input variables, problem and other necessary details. So for a given node, we will check the neighbouring states and check their values. If the value is greater than the current node, we move to that node. Else we return that node as the local maximum. We donot look ahead of the immediate neighbors of the nodes. Analogically we can say that it is similar to walking through the maze with amnesia.
So suppose we are solving the problem of n queens using the local search algorithm. Lets say we are given one queen in each column. So successor function will move the queen from one square to another in the same column. The heuristic function can be the said as the nmber of queens that are attacking each other, so we would like to minimize it. (in this case it should be zero) 

Incase of local search algorithms , we just take care of the current state and no other state. We move only to the neighboring states, so very little amount of memory is used. We don't store the paths. Thus this will help in finding solutions in infinite and continuous state spaces. 

When we say complete solution, we mean a solution that meets our requirements. We saw that whenever we start from some value, we it will have a value corresponding to that state (the objective function). We try to optimize this with respect to our requirements but not always acheive a complete solution. We might reach a solution, but it may not be complete. For example in queen placing problem , (lets say we have to place 8 queens on 8x8 chess board such that they wont be able to kill each other) we observed that in 14% of the times we were able to find the solution and 86% of time we get stuck on a local solution. But we reach solution in like 4 steps and get stuck in 3. So when we have very high number of states, then we will be able to find the solution quickly.
The drawbacks are as follows. : 
1. If there are too many local maximas then we will not get complete solutions most of the time. 
2.This class is said to be as plateaus. If there is only a single maxima and rest of the values are constant, then at most cases we will not get the solution, as in the immediate neighbourhood there is no increase/decrease in values. So it will tend to stay on that state or may change the state but the final solution will not be complete. The program will stop before it comes to complete solution in most cases. 
3. If we are moving in 2 dimensional space, and we have the neighbourhood function such that it moves in either x direction or in y direction, but in certain cases we might get a solution by moving along the diagonal which the neighbourhood function doesnot allow. So we might not get a complete solution there too. This class is called the diagonal ridges.      
We can allow sideways moves which will be explained later, which will help us prevent the plateau problem and reach the complete solution. Also it will improve in solving the queen placing problem by increasing the 14% solve rate to 94% solve rate.This is called tabu search. The drawback of this is that the number of steps increase significantly, because in sideways walk we are allowing the algorithm to store some states contrary to the hill climb algorithm in which we didnt store anything and didnt check anything except for current and neighbouring states. So this will increase the number of steps involved.
What we can do is maintain a queue, and we can keep adding the states in it. We also have to fix the length of the queue so that we dont add too many states. Also we have to ensure that we are not getting stuck in a loop or not checking a bunch of states again and again. That would be waste of computation. 
We may add up a few stochastic variations.
If we take a look at standard hill climbing algorithm, there are quite high chances that we may get stuck in local minima. To prevent this, we may add some random selection or random restart in our algorithm. While choosing the next state we can maintain a probablity p with which we will choose the next appropriate state and with probability 1-p we will choose some other state randomly. With this we may not be going in the best direction everytime, but we will do some improvements in terms of finding the solution. With random restart we mean that we randomly choose the starting point and run the algorithm and find the solution. If we do it several number of times, there is high chance that we end up getting a complete solution. 
So in general if we have to perform local search, this algorithm gives the best result. We may add some conditions for terminating the program, for example we add a condition like search for 100 states, or perform 100 iterations etc. Also if we are not always itnerested in getting the best solution, or wih some relaxed conditions we can add that too. For example in placing queen problem, we can add a condition that we are okay with atmost 2 queens in attacking position. This way we can terminate the program earlier than it should. 
We can also combine both greedy and random walk approach. What we can do is initially choose a start state which has the best neighbouring states and start from there, chooose the neighbor with probability p or choose random state with probability 1-p. This way we are using greedy and randomness together. Since both individually are asymptotically complete , so together they will also be asymptotically complete.  

In certain cases where we have a plateau /shoulder along with the increasing and decreasing curve of the objective function, we can do a local search/hill climb search when the values are either increasing/decreasing, and when we reach a plateau we do an exhaustive search which will lead to the end of the plateau and we may begin to do the local search/hill climb search again. This way we are saving the extra computation of exhaustive search for the whole plot. The disadvantage is the time lost in exahustive search during the plateau period. 

Next comes the simulated Annealing algorithm which is inspired from the idea of random walk. So we will be using the modified hill climb algorithm with randomness included in it. We will be choosing the next state with probablity p and some provisions will also be made to also choose the less relevant states too. We will do this by providing shake , which can be assumed as a ball being stuck in a local minima and we provide a shake to it so it starts moving and goes to another minima. Gradually the amount of shake provided will keep reducing.
The algorithm is explained as follows. : 
function SIMULATED -ANNEALING(problem ,schedule) return solution state
 input: a problem, problem
	schedule,a mapping from time to temperature.
 local variables, current, node
		next, a node
		T , a temperature which will be controlling the amount of shake
current<-MAKE-NODE(INITIAL-STATE[problem])
for t 1 to infinity
	T<-schedule[t]
	if T=0 then return current
	next<-a randomly selected successor of current
	E = VALUE[next]-VALUE[current]
	if E>0 then current<-next
	else current<-next only with probablity e^(-E/T).

The way we reduce the temperature is schedule and if we ensure that we reduce the temperature very slowly then we can say that we will reach global minima.
In general it takes quite a lot of time, and is not very popular. Some problems which can be solved using this algorithm are Traveling salesman problem ,graph partitioning,graph coloring etc.

Another variant of hill climbing is local beam search algorithm 
In this we are allowed to use memory, unlike in hill climb algorithm. So we can keep track of k states simultaneously unlike in hill climb algorithm, where we used to keep track of only one state. So we choose the k next based successors and again choose the k best successors from the successors of the successsors.
The disadvantage is that we are affecting the diversity as we are choosing the best successors from all the neighbours. So we might reach to the same minima.
Again we can improvise by adding randomness here. Instead of choosing k best successors, choose the successors probabilistically by giving bias to some possibilities.

Genetic Algorithms.
This is something different from the local search algorithms. In this the successor is generated by two parent nodes and a fitness fucntion calculates the fitness of the node. It basically gives an idea of how probable the state is to survive.More the fitness better the chance of survival.
The case of the n queens problem, lets say we represent the posititions of the queens on the chess borad by a string. We assume that there is a queen in every row, so for the position of queen at the ith row, the ith position of the string will give the column number. So in genetic algorithm, we will use some randomly generated k states just like in beam search algorithm, except the fact that the next node will be decided using two states. For each state in the given population, we get the fitness of the state by the fitness function which tells how close we are to the goal.So we will use number of non attacking pair as the fitness function. Calculate the probabilities using approapriate functions. Using these probability we can find the k strongest states. Now we perform a cross over. We make a partition in a state, and take the first i bits of that state and n-i bits of the other state and form a new state. This way we form a new state. We add this partition randomly. Now the last step, we randomly change the digits of the state and we change it. This is called mutation. This sums up the genetic algorithm.  Using the crossover here is really helpful as we are able to move from one state to another in a single step,which would have taken a huge number of steps. We are joining two random states which ought to give higher fitness in the end.This may also taken out of local minimas. So its use is justified.
Genetic Algorithm is derived from Stochastic beam search algorithm.
Some advantages are as follows: it helps to jump from one step to another and also avoid local minima/maxima.
Some disadvantages are that there is alot of randomness so it is difficult to get the same result again.
Lets say we have to solve traveling salesman problem using genetic algorithm. How do we do this? Arrange the cities to be visited in order and randomly select k cities. (say first we visite 1 then 3 then 4 then 2. So 1342. and similarly we make all the states, use fitness function and find k best successors. Do crossovers and generate new states. Now we do the mutation by selecting 2 values of the string and swaping it. The values selected are such that would lead to an increase in fitness after mutation. We can also do an exhaustive swap. Iterate over all pairs, and choose the one with lowest cost.
How do we do the cross over in the traveling salesman problem? It is a kind of greedy approach. We select the first city from the parent, and compare the leaving city for that city in both the parents and choose the closer one to extend the tour. If one city has already appreared in the tour select the other one. If both have appeared in the tour then we randomly select the non appeared city.

One negative point of the genetic algorithm is that the algorithm doesnot necessarily give the population which is fitter tahn the previous one but also has characterstics of the parents which are easily mixable with others. Lets say in parent population, two states are repeating. So there are high chances that more characterstics are inherited from these classes. 

We observed that hill climb algorithm is used in discrete space. What if we want to use such an approach for a continuous space. Lets say we have a function f(x,y) = e^-(x^2+y^2) + 2* e^-((x-1.7)^2+(y-1.7)^2)). One way to do it is to discretize the space and do the normal hill climb algorithm there. Also we can adopt the idea of gradient descent or gradient ascent. So in continuous space if we have to maximise the objective function, then we apply gradient ascent, else we do gradient descent. In general we can have n variables in place of the two variables used in the previous example. We first calculate the gradient of the function with respect to each variable. The gradient gives us the direction in which we may acheive minima or maxima. So we move in that direction. In continuouse space we also have to use the step size, which will determine how big our jump/step would be unlike in discrete space where we moved from one state to another. So the stepsize (lambda) can be determined by the user. What is done in practice is that we keep the stepsize small and we keep increasing it. Another way is to initially choose a large value of step size then iteratively decrease it. This second appraoch is inspired from simulated annealing algorithm we mentioned earlier.