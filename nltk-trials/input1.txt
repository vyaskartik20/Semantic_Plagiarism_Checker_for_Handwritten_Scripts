This course of Artificial Intelligence is based on the motivation of creating machines that are intelligent and in some sense can make rational decisions on their own when faced with a problem.

MOTIVATION:
The motivation behind studying this field is manifold. Be it movies, books, or research & development in tech giants, we see a lot of work happening in this direction. The idea in itself, of machines thinking to solve a problem by themselves, is quite fascinating. This in turn makes us think what intelligence actually means in the said context. Humans use the brain to make decisions which is a complex organ and understanding its working to be able to mimic (to some extent) and maybe improve upon it in some tasks is a goal to aim for.

USE CASES:
Over the years the various areas where Artificial Intelligence has found use and made an improvement in human life are as follows   
1. Natural Language   artificial intelligence has made it possible for an individuals speech to be recognized and text to speech synthesis work.
2. Vision   Object and person recognition be it from pictures or videos has been made possible by artificial intelligence. Combined with natural language processing, it has improved the lives of visually challenged individuals with the help of various technologies like scene description, question answer based on visual data etc.
3. Robotics & Game Playing   Both vision & natural language processing when combined and used in robotics, give startling results. Along with automated robots and cars, there have been advances in computers playing and winning against world champions at multiple games such as chess and GO.

INTRODUCTION:
There is a need to quantify what it is we mean when we talk about responding like humans to situations. When we think about this, we need to make it clear as to what is rationally correct to do in a situation. An interesting point to note here is that rational thinking and how humans react to situations are two distinct things. Therefore, there has been a divided opinion over how machines are to be made intelligent. Are they supposed to act like humans? Or are they supposed to take the ideally rational path? Well discuss the two one by one.

Making machines more like humans   
The great computer scientist and mathematician Alan Turing devised a system for measuring how close a machine is to responding like humans. According to the Turing Test, a machine can be said to be capable of thinking like human beings if, on interaction with a human, it is able to respond in a manner that is indistinguishable to how a human might have responded to the same set of situations/questions. However, even though this test remains relevant after all these years, it is believed that studying the underlying principles that lead to the way humans think is more important than just trying to duplicate the process.

Making rational machines   
To be succinct, rational thinking simply means maximizing expected utility. And making machines rational in a sense is a more objective goal since there is a fixed definition that can be agreed upon. Categorizing a decision as rational depends on 4 basic factors:
The way we define what success for a task means, what the decision making entity knows about the environment, the possible paths or actions that can be taken by the said entity, what all input has been received by the decision making entity till now, so that it can base, its decisions upon those.

Based on what the outcome of a particular action/decision is, we can express a goal as that outcome which has maximum utility for us based on the measure that we use for success.

Agents and Environments:
On a basic level, we consider an agent to be an entity that receives inputs or percepts from the environment with the help of sensors and acts upon the said environment.
Note that an agents chosen action can depend on all the inputs that it has received so far but not on anything that it hasnt perceived.
Also, a world state encodes all the information about the environment in any given situation.

Rationality of an Agent: 
Rational agents are those that tend to make decisions that end up maximizing the expected utility as described above. The selection of such rational actions is dictated by the environment, the inputs received by the agent, and the possible set of actions available to the agent. Note that rational agents optimize expected utility which is different from perfection which maximizes actual utility.

Types of Agents:
We learn about two types of agents   reflex agents and planning agents.

Reflex agents tend to work in a greedy manner and take a decision based on the current input and dont think as to what might be the consequences of their actions in the future. They do not take into consideration the effect their actions would have on the world state they are in. Also, reflex agents can be rational in those scenarios where the problem posed to them has the inherent property that just reacting to the situation according to current inputs is sufficient to lead to an outcome of maximized utility.

Planning agents on the other hand tend to look into the possibilities as to what might happen should they take a certain decision. This gives rise to the concept of planning in an optimal or complete way. Complete planning would mean a certain plan being followed by an agent, which is certain to lead to a solution if it exists. Optimal planning on the other hand is a kind of complete planning (which means that it inherently includes reaching to the goal) which costs the least. For this, we need to introduce the idea of how we define cost in a certain world, but in general, we tend to want to solve a problem in as little expenditure as possible. Also, in order to be able to figure out what would happen when a set of actions is taken, the agent needs to know how the environment actually works. What state change happens when a certain step is taken. Without sufficient information in this regard, a planning agent would not be able to perform as expected. Another requirement is fixing a goal. A goal is necessary for a planning agent to be able to terminate its analysis of possible actions i.e. when a certain set of actions cause the world to reach a goal state, the work of the agent can be said to be done.

Planning vs. Replanning: A planning agent can work in either of two possible ways. It could either work out all the possibilities of actions that can be taken, decide a path (set of steps) that would lead to the goal state optimally, and only then start working on it. On the other hand, it could also start working sooner than that and just keep on replanning its route until it ends up in one of the goal states. So, it basically divides the entire problem of reaching the goal state into smaller problems, decides an optimal path for them, and ultimately ends up solving the overall problem if a solution exists.

Search Problems:
Reaching the goal state can be modeled as a search problem in which we try to find the path (optimal if possible) to the goal state.
Such a search problem has the following components, in general   
1. A state space (the states the world can be in)
2. A successor function (mapping of current state to the next state according to available actions)   an encoding of how the world works
3. A start state and a goal state.
A goal test is generally created for the agent to be able to check if the current state is a goal state or not. Note that it is important to consider a successor function that is in some sense compatible with the state space chosen to model the problem.

Modelling as a search problem:
As mentioned above, we simply model a real world problem as a search problem, so the models can vary. The level of abstraction that we deem right for our model is a crucial point of decision.

Search states:
They are DIFFERENT from the world states. A search state only keeps the details that are needed for planning a path for the problem under consideration. So depending upon the problem (goal), the amount of information contained in a search state varies.
E.g. In a game of PacMan, for the eat all dots problem we need to include booleans corresponding to all the dots and not just the number of uneaten dots left because to be able to plan optimally the agent would need to know where should the agent head next.

Representations of paths to goal states in a problem:

State Space Graph   A mathematical representation of a search problem.
Nodes are the world configurations that concern us for a given problem. Some of these nodes would satisfy the goal test and would be called goal nodes. (Stressing that they may be more than one in number). Note that since (some information of) states are being represented using nodes, all the states occur only once. (Unlike search trees). Edges represent the information obtained from the successor function defining how state to state transitions take place according to the different actions taken by the agent. Due to the extremely large combinatorial rise in the number of states as the world becomes more and more complex, it is not usually possible to be able to build such graphs in practice.

Search Tree   a tree that contains all possible sequences of actions and the corresponding consequences with the start state as the root node.
It is generally much bigger than even the state space graph as there may be multiple possible ways to reach the same state and there would exist a branch in the search tree for all such possible paths. Note that any node in a search tree represents a complete path in the corresponding state space graph (starting from the root of the search tree and going till the said node). While finding out the optimum path we try to construct the search tree on demand and as little at a time as possible.

Building a search tree incrementally: General Tree Search
Using the following algorithm we try to explore as little as possible of the giant search tree and still come up with a solution (path).
The algorithm:
1. Initialize the search tree with just the initial state of the problem.
2. Now begin a loop:
   2a. If there are no candidates for expansion left, simply return failure. (At any point of time, the candidates for expansion in a search tree are just the leaves of the tree. 
   2b. Choose any of the leaf nodes for expansion (according to the chosen strategy)
   2c. If the node contains the information of a goal state, return the corresponding solution (i.e. starting from the root node till this node (goal node)).
   2d. Else expand the node and add the resulting nodes to the search tree
3. End Loop

Two crucial things in the above algorithm are:
The Fringe   It stores all the candidates for expansion at any given time. Depending upon the implementation we may have all the predecessors of each element in the fringe so as to be able to trace the path using which we were able to reach that particular node.
Second is the Exploration strategy   This is the strategy that we depend upon to pick a node from the fringe in step 2b. Possible strategies include DFS, BFS, UCS, etc.

Various exploration strategies:
In general, there are four properties that we would want to analyze for a given search algorithm   completeness, optimality, time complexity, and space complexity. 
To be able to quantify these properties lets consider a search tree that has c children for all the nodes. Let d be the maximum depth of the search tree. Note that solutions can exist at various depths in this tree.

Depth First Search   As the name suggests we expand the deepest node first, from the candidates for expansion. Implementing this strategy involves using a stack to store the fringe nodes. In this way, with the help of the last in first out strategy, we will go deeper and deeper along one path, until it terminates and then after popping all those nodes from the stack, we start exploring alternative paths (provided we dont already have a solution). In this algorithm, we may come across multiple situations where the nodes under comparison are at equal depth, in which case we would need to break ties. How we break such ties can also cause a drastic change in the run time of our algorithm.
Properties of DFS:
Completeness   if we can somehow keep track of already occurred nodes and prevent loops, we would be able to say that this approach would lead to a complete planning.
Optimality   This algorithm is not optimal in the sense that since it goes as deep as possible starting from the left, we would just end up getting the leftmost solution irrespective of the cost (say the cost is in the form of the number of steps taken from the start state).
Time Complexity   In the worst case, the goal node could be present at the bottom right most end of the search tree in which case we might have to traverse the entire tree i.e. O(c^d) complexity.
Space Complexity   As mentioned above, we are using a stack to store the fringe, which means as we go deeper we keep on adding children for all the nodes that come up on the path, but we expand along only one of them. Hence, for d depth (in the worst case) and c children for all the nodes on that path, the space complexity would be O(dc).
 
Breadth First Search   As the name suggests we expand the shallowest node first, from the candidates for expansion. Implementation of the fringe in this algorithm is using a queue. In this way, utilizing the first in first out strategy, this algorithm first explores all the nodes closest to the current node and then goes on to the next level of the tree.
Properties of BFS:
Completeness   This algorithm is complete in nature i.e. if a solution exists, this algorithm will provide us with it.
Optimality   In general it is not said to be optimal. Even though it will lead to a solution with the minimum number of steps from the root, the cost of each edge is not always one and hence as the edge cost varies, an optimal solution using this approach is not guaranteed.
Time complexity   Again in the worst case the goal node may be in the bottom right corner of the tree in which case we would end up exploring all the nodes with a complexity of O(c^d).
Space complexity   We are using a queue to keep track of the fringe nodes here, so at any given time we store the nodes at the current level that have not yet been expanded along with the children of the nodes that are at the same level as the current node but have been expanded. This would roughly be O(c^d) in the worst case.

BFS vs DFS   a major advantage for the BFS approach if the solution is shallow and to the right of the search tree because via DFS, we might take a very long time to reach the goal.
However, if the goal is at the bottom left, DFS would greatly outperform BFS, as BFS would traverse through all the nodes above the goal before reaching it, whereas DFS would just traverse the goals ancestors.
If there are memory constraints, DFS would be preferred since we might run out of memory in the case of BFS, before we can actually reach the goal.
In general, if the cost is defined by just the number of actions and it is to be minimized, we would definitely prefer BFS over DFS.

Iterative Deepening   Exploiting the advantageous properties of both BFS (shallow solution quicker) and DFS (no memory problem).
In this algorithm, we run DFS but we cap the maximum depth that it goes to, to some constant. If we are unable to find a solution, we incrementally increase the depth cap and try to find a solution, performing DFS at every turn. This may seem to be very wasteful as we end up doing the computations for the shallow layers again and again. But it turns out, it still would be better because if we look at the work being done, typically the number of nodes in a layer is the sum of the number of nodes in all the previous layers. So, a lot of the work is being done in the last layer anyway.

Uniform Cost Search   This algorithm is useful in those scenarios where we care about the cost of the actions and it is not just about minimizing the number of actions but the overall cost.
Following this strategy, we implement the fringe using a priority queue where the priority of an added node is decided using the cumulative cost from the root to that node. We use this priority queue in a BFS fashion. This means that at any given point of time, we pick that node from the fringe that has the lowest cumulative cost till then. If we implement such a priority queue using a heap, we just have to take the root of the heap and expand it in the search tree. Whenever we find the solution it is bound to be the optimal one because we would have already gone through any paths that cost less than the final one we choose. If any other path were to lead to another goal, it would definitely cost more.
Properties of UCS:
Completeness   Since we are concerned with the cost as well and we choose the minimum one at every step, this algorithm is bound to lead us to the solution if there is one. (Assumptions being that the best solution has a finite cost and the edges all have positive costs).
Optimality   It will give us an optimal path as discussed above in the description.
Time complexity   Say the cheapest solution ends up being the one with cost C. Let the edge costs be at least e. Then the effective depth at which we must have found our solution would be C/e. This means it would be exponential in this effective depth with the base being the number of children c. Hence, O(c^(C/e)) would be the time complexity, as we would have explored all the paths with cost less than the optimal one.
Space Complexity   Here the fringe is implemented using a priority queue which at a time would include the nodes at the end of the boundary of the effective depth. This would lead to a space complexity of O(c^(C/e)) (= all nodes in the boundary of the effective depth level).
Drawbacks: It has no information to use, about the goal state. This causes it to just explore in all directions wherever it can find the smallest cost not caring whether it is actually approaching the goal with a step in that direction or not. This causes some useless computation which will be rectified using A* search below. There we would look at promising leads and not just basing our next step on the cheapest so far approach.

DFS, BFS, and UCS   What unifies them?
The only difference between the above algorithms is the fringe exploration strategy i.e. in what manner do we choose the next node to expand. The unifying factor there could be the implementation of these methods. We could just use a priority queue for all of the above approaches. Just the priority would be different in the three cases, deepest first, shallowest first, minimum cumulative cost first being the priorities to be used respectively. (There will however be an implementational overhead if we use a queue in the case of DFS and BFS, where using a stack and a queue would be cheaper space wise.)

Where do searches go wrong?
We need to always keep in mind that the problems we are solving using the above approaches are just models of some real world problems and it is not necessary that there will be compatibility all the time. Any inconsistency with the real world problem could lead to incorrect results from the algorithm. Ultimately, the search will only be as good as the modeling of the problem.

Search Heuristics:
We came to the understanding that even though UCS gives us the optimal path, it does a lot of wasteful computation since it has no information or guidance as to in what direction should it go on checking first. To this extent, we use search heuristics to give the algorithm an idea or guidance as to where should it look for the goal. 
Formally, a heuristic is a function that estimates how close a state is to a goal. Note that a heuristic is a problem specific function and cannot be generalized in nature.

Once we have the heuristic function defined for a problem we can look into two algorithms that utilize it and improve performance   Greedy and A*.

Greedy Search   This algorithm works on the principle of expanding the node that seems closest to the goal state at any instant (as it comes to know with the help of the heuristic).
What could go wrong here? 
We could end up choosing a path that looks good at the moment but ultimately leads us to a non optimal solution. The reason this might happen is that we are not taking into consideration the cost that has already been incurred, and are simply looking at the heuristic to decide the next step. 
In the worst case, this algorithm might work like a DFS that is guided away from the goal. In a way, it would explore all other paths, coming to the goal in the end.

Uniform Cost Search vs Greedy Search:
UCS is slow in the sense that it tries ALL the paths that cost less than the optimal one before coming to a conclusion. Greedy on the other hand works faster as it is guided at every instant to go towards the goal, but it might not work that well as it doesnt take into account the cost already encountered. Now, we could get the best of both by combining ideas from the two.

A* search   expands according to the minimum value of the sum of cumulative cost so far and the heuristic value at the node. In this way, it takes into account the forward (distance from the goal) as well as backward (cumulative cost incurred so far) costs in choosing the next node to be expanded from the fringe. Note that the point of termination of the algorithm is NOT when we enqueue the goal state but when it comes to dequeuing (expanding) a state and it turns out to be a goal state.
Optimality   In general, it does not give an optimal solution if the heuristic is poorly chosen. This means that the heuristic does not estimate the distance from the goal correctly, say it overestimates the distance from the goal. This is called pessimistic behavior of the heuristic.

Admissible Heuristics:
Those heuristics that slow down the bad plans but never outweigh true costs. Pessimistic or inadmissible heuristics on the other hand cause good plans to stay on the fringe for too long and lose optimality in the process.
Formally, 
A heuristic h is admissible (optimistic) if for every node the heuristic value <= the exact cheapest way to get to the goal from that node.

Optimality of A*: If the heuristic is admissible,  A* search would be optimal. 
This can be proved by considering two nodes A and B on a search tree. Say A is an optimal goal node whereas B is a suboptimal goal node. At any point of time, say B is on the fringe along with some ancestor of A (or A itself). This is the case because if B werent on the fringe, we wouldnt have anything to worry about anyway. Also, it isnt possible that none of As ancestors (or A itself) is on the fringe. This is because it would mean that it has already been processed and again we wouldnt be in this situation. Now, B will be blocked on the fringe until A is expanded which proves that A* search will always yield an optimal solution in case of an admissible heuristic. The above claim of B being blocked till A is cleared can be verified by simply using the condition of admissible heuristic which needs to be optimistic in estimating the distance from the goal and the fact that at a goal, the heuristic value is zero.

E.g. 8 Puzzle: A problem which involves a 3x3 board with tiles numbered from 1 to 8 placed in it, leaving one empty state. The goal state is arranging the tiles in some fashion, say increasing order from left to right and top to bottom. An admissible heuristic in this case could be the number of tiles that are not in the correct place. This heuristic is admissible because if a tile is not in the position it is supposed to be, it has to be moved at least once.


References: Class discussion of CS323   Artificial Intelligence.
The slides & video lectures of UC Berkeley CS188.
Russel & Norvig : A modern approach to AI (3rd Edition).
I have not taken any help from any other sources. The above notes are based on my understanding of the content.
