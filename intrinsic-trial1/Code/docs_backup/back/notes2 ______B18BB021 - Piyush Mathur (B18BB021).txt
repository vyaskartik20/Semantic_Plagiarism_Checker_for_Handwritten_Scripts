Previously we looked at the 8 puzzle problem in which we have a block-shifting puzzle with 9 slots, * blocks namely 1 to 8 and one empty space to enable shifting. Any state can have 2 to 4 successors where actions include shifting blocks into empty spaces. We can define cost as the no. of actions we make. And the goal test will be if they are arranged in order. We look at one type of heuristic in this, in which we take heuristic= no. of tiles misplaced. And it will be admissible, because if you relax the problem of taking out, shuffling and putting it back in that's the least no. of actions you'll have to perform. In this, we can see that this A* search takes significantly less no. of paths explored before we reach goal. Then, we take a look at another formulation of heuristic to solve the same problem where heuristic= Total Manhattan distance, i.e. the sum of of manhattan distances of all misplaced pieces from positons in goal state. This can be considered as a relaxed version of problem in which we can slide the pieces on other pieces and take them to their goal positions. After comparing A8 search on this heuristic function with the last heuristic function we can see, that we have to expand even less nodes than had to last time. So, it can be inferred from this that if the heuristics are tighter then we have to expand even less no. of nodes. We look at the third formulation of heuristics in which we take heuristic equal to the actual cost. We know that it will be admissible. And as it is even tighter heuristic than the previous one, we know that we'll definitely be saving on the nodes expanded because this time heuristics isn't an approximation of actual cost but it is the actual cost. But, then what is the problem with this. If we think carefully, computing the actual cost will be ver difficult, actually equivalently difficult to finding the goal. As we get closer and closer to the actual cost in our heuristics, we are saving on the no. of nodes expanded but we're actually doing more work computing the heuristic values. So, it's a trade-off between the quality of heuristic and the amount of work done computing the heuristic. We know now that for a particular problem, we can have many heuristics, so we look at the concept of dominance of heuristics. A heuristic h_a(n) can be said to be dominant over another heuristic h_c(n) if for every node n, the value of h_a(n) is greater than h_c(n). However, in some cases we may have greater value of one heuristic h_a(n) for some nodes and greater value of another heuristic h_b(n) for the other nodes. In such cases, we can define another heuristic which takes the maximum of both values h_z(n)=max(h_a(n),h_b(n)) and it can be said to be dominant over both h_a(n) and h_b(n). So, if we represent this in form of a figure these heuristics can be laid out in form of a semilattice in which at the bottom we have a heuristic which has h_t(n)=0 for all nodes n, at the top we have exact heuristic h_e(n)= actual cost and other heuristics are kept in between them arranged in terms of dominance. Exact and Zero heuristics are called trivial heuristics. 
Now, after looking at different tree search algorithms, we take a look at graph search. We start with an example which has A,B,C,D... nodes and there are two paths connecting A to B, B to C, C to D, and so on. If we make a tree for such a graph we'll have an exponentially large tree with many sub-paths occurring multiple times. If we have a tree in which we have a node E at depth 1 and we have that same node repeated on some other path at a depth 2. If we're doing BFS, it will be completely useless to expand that node again. So, we need to modify this algorithm. Simply, we should not expand any node twice( or more than that). To implement this, we use simple tree search in which we also keep a closed set which stores all the expanded nodes. Every time before expanding a node we make sure that it hasn't already been expanded, if expanded: skip, if new: add to closed set and expand. We should keep in mind that this should be a closed set and not a list, because searching in a set is better than in a list. This graph search will be complete because we're only skipping the nodes that we have already expanded. But, it will not be optimal because it is possible that we didn't expand an already expanded node which may have a shorter path than the one that has been already expanded. We take a look at an example where our A* graph search goes wrong. In the graph, S is the starting node and G is the goal node, A,B,C are the rest of node. There are paths from S->A,S->B,A->C,B->C,C->G. The heuristic values are h(S)=2,h(A)=4,h(B)=1,h(C)=1,h(G)=0. We start with S and have paths to A and B with f(A)=5,f(B)=2, so we expand B. Then, we have path to C, f(C)=4<f(A)=5, so we expand C. Then we have a path to G, f(G)=6>f(A)=5, so we expand A. We have a path to C, f(C)=3<f(G)=6, but we don not expand this C because we already expanded C in another path and hence we expand goal and the fringe is empty now. We get our path S->B->C->G that's actual cost is 6 but the optimal path is S->A->C->G with actual cost 5. So, what went wrong? The heuristics we used were also admissible. Here we are introduced to the concept of Consistency of Heuristics. A heuristic is admissible if h(n) is less than actual cost of node n from goal. But, a heuristic is admissible if h(A) - h(C) is less than actual cost from A to C.  By this, our f value that is sum of heuristic and cumulative backward cost never decreases and this leads to an optimal A* graph search. How? So, we know that in A* tree search, nodes are expanded in increasing order of their f value. With consistent heuristics, for every state s, nodes which optimally reach s will be expanded before the nodes which sub-optimally reach s. So, now we have an Optimal A* Search. We can also say that consistency implies admissibility. A* tree search is optimal with admissible heuristics and A* graph search is optimal with consistent heuristics where UCS is a special case with h as heuristic=0. 

Now we move to our next module. Constraint satisfaction problems (CSPs). For solving any search problem, we make some assumptions about the world. We assume that is has a single agent, which does deterministic actions to move to a fully observed state in a discrete state space. There can be two types of tasks: planning and identification. In planning, the path we take to the goal is very important, i.e. optimality is important. BUt, in identification problems only the goal state is important. We can say that usually in identification problems, all paths have same depths. And CSPs are a special form of identification problems. A CSP is defined as follows. A state is defined by variables Xi with values from a domain D. Goal test is defined as a set of constraints which specify that which values of of variables are allowed. To understand CSPs we take example of Map Coloring Problem in which we have to color regions in maps with a constraint that no two neighboring regions should be of the same color. We take example in which variables are WA,NT,W,NSW,V,SA,T with domains D={r,g,b} showing colors that can be used to color them. In this neighbours are defined as WA:{NT,SA}, NT:{WA,Q,SA},Q:{NT,SA,NSW},SA:{WA,NT,Q,NSW,V},NSW:{SA,Q,V},V:{SA,NSW} and T:{} . Now, constraints can be of two types: implicit, in which we majorly define what is the constrains of the problem (Here - WA!= NT,etc.) and explicit constraints in which we define all the subset of allowed values for variables (Here: (WA,NT) belong to {(r,g),(r,b)...}). And a solution will be one that satisfies all the constraints. One Solution is : {WA=r,NT=g,Q=r,NSW=g,V=r,SA=b,T=g}. Another CSP example is N queens where we have a chess board of nXn and we have n queens which we have to place in such a manner that no queen can be harmed by any other queen. So, first formulation of N-Queens can be defined as follows. Variables are Xij of all the boxes on board which have a domain {0,1} which represents 0 as empty and 1 as filled. Constraints can be explicitly defined for every i,j,k (Xij,Xik) belongs to {(0,0),(0,1),(1,0)} which means no row can have more than one queens. Similarly, it can be defined for columns and the two types of diagonals. To avoid all having insufficient placement of queens, we keep a constraint sum_over_i_j(Xij)=N. For second formulation, we know that no row can have more than one queen and there are n rows and n queens, so each row will have a queen. So, variables are Qk for queen in row k and Domain for Qk is {1,2,3,...N}. Constrains can be defined as implicit: for all i,j there should be non threatning (Qi,Qj) and explicit: (Q1,Q2) belongs to {(1,3),(1,4)...} and so on for other (Qi,Qj).
We can also look as a CSP in form of a Constraint Graph. In a constraint graph, the variables can be represented as nodes which are connected by different constrains. A constrain can be unary(linked to only one variable), binary(connecting two variables) and n-ary(connecting n variables). For a binary constraint graph, arcs show constraints. Map coloring problem can be seen as a constraint graph. This is beneficial because there are General purpose CSP algorithms which use graph structure to improve the search. We then move to another example: Cryptarithmetic. Here we have to decode the message in an arithmetic statement. In this, there are letters in arithmetic problems which represent numbers. Example - T W O + T W O = F O U R. Here variables are all the letters used for the addition in this problem (which includes variables for carry no. while solving). Variables - F,T,U,W,R,O,X1,X2,X3 which can take their values from domain {0,1,2,3,4,5,6,7,8,9}. And then constraints will be that all F,T,U,W,R,O are different, O+O=R+10*X1, and so on. So, this can be represented as a constraint graph. Here, we have constraints connecting more than 2 variables so we can't just use an arc and we represent it as a square,i.e. different from nodes(variables). Sudoku can also be seen as an example in which variables are all the open squares which take values from domain {1,2,3..9}. And constraints can be all different values in all rows, all different values in all columns, all different values in all regions(highlighted 3X3 regions). Then we come to Varieties of CSPs and Constraints. There can be varieties of CSPs depending of the type of variables. Discrete values can have finite domains or infinite domains(example- integers, strings,etc.). And there can be continuous variables. There can be a variety of constraints: unary, binary or higher order constraints. There can also be preference constraints(example- r is better than g) but in that case, this will be called a constraint optimization problem. There are many real world examples of CSPs like timetabling problem, assignment problem, etc. 
Then, we come to Solving these CSPs. Firstly we have Standard Search Formulation. So, for every search problem we need initial state, successor function and goal test. In this, initial state is empty assignment, successor function is the one while assigns values to un assigned variables, and goal test checks if all the constraints are satisfied. If we see this tree, in top layer there is one assignment and in next layer there is one more to it and so on. If we use BFS, it will take very long time, because the solution lies very deep. DFS would be better because it will at least find the solution directly, but in this also when we encounter a constraint violation, we'll have to go many levels back to correct the wrong assignment that we did, so this is also very time consuming. So, this DFS is now improved to backtracking search. Backtracking search is the basic uninformed algorithm for solving CSPs. The main idea is to set ordering of assignment of variables because assignment of variables is commutative. And to keep checking the constraints as we go, so that we don't have to expand the wrong nodes which will save time. Backtracking Search is combination of DFS, and variable-ordering and fail-on-violation. Now, we introduce general-purpose ideas to improve Backtracking search: Ordering, Filtering and Structure. In ordering, we program the ordering of assignment variables and ordering of values assigned to these variables in such a way that it reduces our no. of nodes expanded before we reach our goal state. In filtering, we detect the inevitable failure earlier by dropping the nodes which later lead to failure. And in structure, we try to manipulate the structure of the constraint graph of the problem for easier solving. First, we look at filtering methods which aim at keeping track of domains of the unassigned variables and crossing off bad options. First filtering is forward checking. For, every assignment that we make, we edit the domains of unassigned variables and remove the options which can't be chosen. For example, in the Map Coloring problem, initially all variables have {r,g,b} domain but after my first assignment of r to WA, I can't assign r to neighbouring regions so I modify domains of NT,SA as {b,g}. Similarly, we drop values for domains of different variables as we make assignments and declare failure as we encounter an empty domain for an unassigned variable. But, forward checking also have some flaws. We may reach a state where two neighbouring regions have only one and same value in their domain but the search will proceed because the domain isn't empty yet. To overcome this flaw, the concept of consistency of arcs is introduced. For an arc X->Y to be consistent, for every x in the tail there should be some y in the head which could be assigned without violating a constraint. We enforce consistency on all the arcs after each assignment in forward checking, so that we can eliminate inevitable failures earlier. It should be noted that if domain of a variable loses a value, all the neighbours should be checked again because that might have lead to inconsistency of some other arcs. And we should always be deleting from the tails. However, in map coloring problem the arcs are bidirectional, so both direction arcs are to be checked of consistency. Enforcing consistency on arcs requires work to do, but it saves expansion of wrong nodes. This can be run on O(n^2*d^2) time complexity. But, detecting all future problems is NP hard. After enforcing consistency, we can have either one solution left, multiple solutions left or even no solutions left. That's why it is run as a part of backtracking search, so if you encounter a state which has no solution you can backtrack and try other assignments. Then we come to ordering. There are two orderings: variable ordering which decides which variable should be assigned first and values ordering which decides which values should be assigned first to a variable. In variable ordering, we look for Minimum remaining values (MRV), i.e. we choose the variable which has least no. of values in its domains. We do this because we have to eventually assign values to all the variables and if we are going to have failure with a case we want to encounter it as soon as possible. So, with less values in domains, we have to check for less values of other variable's domains because they may be dropped as we proceed. It is also known as " most constrained variable " and "fail-fast" ordering. For, value ordering, we look for least constraining value which means in a domain of a variable we chose the value which will cause least no. of values dropped from other variables' domains. This is simply because we may have to assign values to all the variables, but we don't have to try out all the values for a variable. So, we try to look for the value which seems more promising in terms of goal attainment. 

After this, we move to our next module: Local Search and Optimization. We know that for planning problems, the path to the goal is the solution to the problem and in identification problems, the goal state itself is the solution to the problem. We have looked at various ways to find solutions to identification problems, mainly CSPs. Now, we'll look at the other methods knows as local search. Just like in CSPs, here also we have constraint satisfaction and now we'll also look into constraint optimization which we briefly touched when we talked about costs associated to the values in a domain (Example- Map coloring problem where r is preferred over g). In the search methods discussed earlier, we were required to manage all the paths, fringes and other entities associated wit the problem as we solved it, but in case of Local search we are not required to do so. For local search, we just need to keep track of the current state and successor function handles moving to the neighbouring states. This is very advantageous as this enables us to find paths even to the problems where the state space is huge by just using very little memory. We can also use local search problems as optimization problems where all states can be associated with an objective function and the goal is to maximize or minimize the objective function. Then we look at the Trivial Algorithms: Random Sampling and Random Walk. In Random Sampling, we generate states randomly and if we do this for a large no. of times, we can get our goal state. In Random walk, we randomly choose neighbors of the current state for a large no. of times looking for the goal state. Both the algorithms are asymptomatically incomplete. So, we look at the first algorithms based on these ideas: Hill climbing a.k.a. Greedy Local Search. In this we start with a current state associated with the problem. We have a successor associated with the states(like we used heuristic function). Then we look for neighbors with highest value of the objective function and then return the current state if it's not higher than objective function value of current state else we make the neighbouring state as current state and keep looking recursively. These values we use of searching bes neighbours may ne objective function value or heuristic function value. Visualizing this we are starting at a point on a hill and we keep on moving to the side where height increases and stop when we reach a peak(can be a local peak). Then we try to look at the landscape of search to understand what all possible states we may encounter in Hill Climbing Search. There is a global maximum(can be more than one with equal values), there are peak local maximums, there are flat local maximums which have equal valued neighbours, there are shoulders where we have a flat for some neighbours and then there is an ascent in the values(not in immediate neighbours but far neighbours). We can reach any of these depending on our start state. We try to look at the example of n-queens problem that we've discussed earlier. This will be a satisfaction problem. We can also view this as an optimization problem by associating an objective function storing no. of attacking pairs and then minimize it. For this problem we take a configuration as an example of 8-queens in which there are 17 attacking pairs and each column has a queen. So, defining this problem, our state will be some configuration of all the 8 queens. We'd like to minimize the objective function(the no. of attacking pairs). On running local search we get a solution with queens are positions A8,B3,C7,D4,E2,F5,G1,H6 considering A..H columns and 1..8 rows. But in this we have an attacking pair of G1 and D4. This is because we posed it as an optimization problem and it gave a local maximum as the result. If we would have considered it as a CSP we would have not got this as a solution. If we ran this problem on randomly generated start states, we solve the problem 14% times and get stuck on local maximum 86% times. Interestingly, it takes only 4 steps on average to solve the problem and takes 3 steps when it gets stuck on the local maximum. So, we look at he drawbacks of hill climbing. It may get stuck on a local maxima. It may encounter a plateau which has constant values for a a fairly medium neighbourhood before there is any ascent or descent so mostly it stops looking as we only look at close neighbours. And there are also diagonal ridges in which we may be allowing the movement in x and y direction (2d euclidean space) and the value doesn't increase in those direction but it increases in x+y direction(diagonal). So, we can escape plateaus or shoulders by sideways move. In this, we can keep a limit on no. of side moves when there is no uphill or downhill moves in search of an uphill(limit is to avoid infinite loops). If we run Hill climbing on 8-queens with Sideways move limit 100, we solve it 94% times. But, doing this we increase steps to the solution to 21 steps from 4 steps and now have 64 steps for each failure. We can also use Tabu search in which we maintain a fixed length queue and keep adding recent states to it and never go to states that are currently tabued. This saves us from returning quickly to the same state. Then we look at stochastic variations in hill climbing. In stochastic hill climbing, we keep a function for probability and we move in uphill moves with probability p and go for random moves with 1-p probability. To avoid getting stuck in local maximas, we use random walk and random restart hill climbing or a combination of both. Random walk is asymptomatically complete, so the idea is to combine it with greedy hill climbing. In random walk we move to a maximizing neighbour with a probability p and move randomly with probability 1-p. In Random restart hill climbing, whenever we get stuck we restart. This no. of restart may be fixed or it can run indefinitely. In Hill climbing with both, we can move greedily or randomly, or restart when found stuck. 
We now to take a look at a physics-inspired random walk. In this, instead of picking the state with highest value of objective function d, we choose a state randomly and move to it if d is positive and if it is negative move on basis of probability proportional to d. And over time we make it less likely to make locally bad moves by controlling the parameter temperature T. This prevents from making very bad moves and getting stuck on local maxima. We can also make the size of these moves random. This can be related to physical process of annealing. High T make more locally bad move and less T make less locally bad moves. Typically, we decrease T as the algorithm runs longer. Then we ake a look at the Genetic algorithms. This is a twist on local search as in this, we generate new states by combining 2 parent states. We start with a population of randomly generated states. And we have evaluation function associated with all states which is higher for better states. We randomly select among the population of states, combine them by crossing over and then introduce random mutations.



References-
[1] Lecture slides and recorded videos of this course.
[2] Informed search: https://www.youtube.com/watch?feature=player_embedded&v=OGcG4jSKOVA 
[3] CSP-1: https://www.youtube.com/watch?feature=player_embedded&v=hegL0V4ckco 
[4] Local search: https://www.youtube.com/watch?v=xcWGAjPG3hg 


