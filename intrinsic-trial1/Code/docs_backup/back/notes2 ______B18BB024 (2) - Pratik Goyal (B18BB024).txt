In the last notes we have covered that finding a suitable admissible heuristic function could be a tedious task. The main idea was to relax the problem and then find the heuristics for that relaxed version of the problem. Now let’s discuss how the extent of relaxation will affect the admissibility and computational cost. Consider two heuristics: 1. Number of tiles misplaced i.e.: tiles can teleport to their actual position, 2. Manhattan distance i.e.: every tile is free to slide in any direction, ignoring other tiles. Both the heuristics are admissible and it is obvious that the calculation of the first heuristic would be much easier than the second one. But, the second heuristic will get to the goal by expanding fewer nodes. Therefore, we see a trade-off between Expansion of nodes and the calculation of heuristics. Formally, as we get closer to the true cost, the heuristic computation cost will increase but we can get to the goal by expanding fewer nodes. 
Till now we were working with tree searches. The problem with tree search is that it allows the occurrence of repetitive states. Therefore, sometimes we can end up doing some exponential redundant work. Let’s talk about graph searches. They by definition do not support reptation of states. The implementation would be similar as of tree search, the main idea is that we don’t want to expand a state twice. Here, the fringe would be a closed set of expanded states and before expanding a state we will check if the same state has expanded before or not. Further we need to store the closed set as a set not a list as set provides faster find operation. This way each and every node will be expanded hence it’s obvious that if a solution exists, we will find it. However, the solution might not be optimal. For example, let’s assume we have already expanded a state with a non-optimal path now when we would like to expand the same node with optimal path, we won’t be able to do it. This graph search however could be made optimal using a consistent heuristic. Basically, we will try to use the idea of admissibility with respect to each arc not just the whole path. Consistency is defined as: heuristic arc cost <= actual cost for each arc. Formally, h(A) – h(C) <= Cost (A to C). Hence, after using a constituent heuristic the f value along a path will never decrease i.e. h(A) <= cost (A to C) + h(C). And this A* graph search would be optimal.
We have seen searches in planning problems. Our aim was to get to a goal state from a starting state. We were interested in finding the path to the goal. If path had costs and we were interested in finding the optimal path. We had heuristics which was sort of providing some guidance about the goal. Another type of problem where searches can be used are identification problem. Here our aim is to assign values to some variables. These assignments might be bounded by some constraints. Here, we are not interested in paths i.e.: how we reach the goal. The assignment itself is important.
In a constraint satisfaction problem (CSP), we are given a set of variables which we call states and for different states we may have different domains from which we want to choose and assign a value to our state. Finally, there are some constraints which we want our final assignments to satisfy. Consider a map coloring problem where there is a map and we need to assign color to various parts (states) of it so that neighbouring states do not have same color. Here, the states/parts are variables, domain is the set of color, and we need to fulfil the constraint that adjacent states must have different color. Second example could be an N-queen problem where there is an N*N chess board and we need to place n queens so that no two queens are in the attacking position. Here variable could be X(i,j), where, i and j are the column number and row number, domain would be the Boolean value representing absence and presence of a queen. And the constraint would be the equations such that no two queens can attack each other.
The problem can be modelled as a graph structure where nodes will represent the variables. A benefit of using graph is that it can speed up search by sometimes treating a problem as a set of subproblems. If there are too many interacting variables, we may introduce some auxiliary variables for easier representation. They act as a connector between a set of variables. A CSP problem can be broadly divided into two categories based on variables, viz, discrete variables and continuous variables. Discrete variable may include finite domains like some fixed color or some Boolean values and infinite domains like positive numbers. Continuous variables could be like solving a polynomial function. wavelength or temperature. Constraints can also be divided into unary, binary, and higher order constraints. A cost or preference might be there and could be treated as a constraint. Some of the real-world examples of CSPs are Fault diagnosis, Timetabling problems etc. 
To solve a CSP we consider them as a search problem where initial state is the empty assignment, on the path to goal state we assign values to the unassigned variables and we stop when we reach the goal state. The goals state here means al the current assignment is complete and it also satisfies all constraints. Note here that we are not concerned about the path. Our answer is the goal state itself. This search problem can be solved using either BFS or DFS. Let’s discuss both the cases.
We know our goal is at the end and we also know BFS is not very memory efficient. It stores all the nodes along its path. Therefore, is we apply BFS here and assuming that branching factor is considerable the tree will grow significantly consuming an exponential amount of memory. If we apply DFS we can get away with this memory problem without affecting the time complexity (as the goal is always the last node). Hence, we will keep DFS as a baseline algorithm and modify it to make it better. Let’s apply DFS to a CSP, we see that even if our DFS violated a constraint in the very beginning, it will go all the way to the last node (non-empty assignment) and then check if the constraints were violated or not. So, it’s not smart at all and this naïve method just can not get any worse (still better than BFS though). Now, we will see how can we improve this naïve DFS.
The simplest thing we can do is that we can stop the first time we violate a constraint rather than going all the way to the last node (non-empty assignment). Backtracking search can be used to achieve this task. It is an uninformed algorithm as we are not using any extra information apart from what is given. The idea is simple. We pick up an order (e.g.: R-B-G) and assign values according to that order one variable at a time. Simultaneously, we check if after assigning a variable, is any of the previous assignments violate any constraint or not. If we do violate, we will backtrack and choose a different assignment. Note that we won’t be backtracking to the starting node but rather to the closest node which may give us a valid solution. Remember that we are keeping DFS as the baseline algorithm and just making improvements to make it function efficiently.
There one slight problem with backtracking which gives us room for improvement. This backtracking algorithm is backtracking when it violates a constraint. Let’s consider a case where assigning a variable guarantees violation in future but this algorithm will wait for the violation to occur. This could be improved if we check if assigning a variable can lead to any violations in future or not. Indeed, this will add up to our pre computation cost but its worth it as it can decrease the search space. We can say that in this case we will reach the last node with a complete solution.
This could be achieved using filtering. One filtering method is forward checking. In this method we keep track of unassigned variables and eliminate the bad options. Second filtering method could be constraint propagation where together with shrinking the domain we will also check if our new domain is violating any constraints. Summarising, in forward checking we are just checking for the domains and shrinking it. If we encounter an empty domain we backtrack, but in constraint propagation we are improving this algorithm by simultaneously checking for the constraints. As Sir Arthur Conan Doyle puts it “When you have eliminated the impossible, whatever remains, however improbable, must be the truth”. Recall that we are improving our backtracking algorithm without changing the basic idea. 
We define consistency of an arc as X -> Y if and only if for every x in the tail there is some y in the head which gives a valid assignment [1]. If an arc is not consistent, we may shrink the domain by removing a value from its tail node to make it consistent. We are removing so to ensure that in future we don’t encounter any violations. Note here that after we remove any value from any node’s domain, we will be checking the consistency for every arcs again. If we find any violation of the constraints in the final domains we will backtrack. This process takes a lot of time. The time complexity is O(n2d3).
If we enforce arc consistency, we might have one solution left, multiple solutions left, no solutions left. There is a problem that it checks nodes pairwise so we might encounter a case where arcs are consistent and still, we don’t have any solution. One other thing is that randomly picking values from the domain we can define order so to choose the node with the minimum size domain as only that domain can shrink to zero size. The main idea is that we will choose that value which will give us the highest probability that we in future don’t encounter any zero-domain case.
All The search methods we have seen earlier simulates a path from a starting state to the goal state and the chooses the best path among them (might be complete or optimal). When it simulates a path it obviously stores them in its memory. But we see that many times we are not concerned about the path we just want to find the goal state (CSPs: map coloring, 8-queens). In local search we will see that is does not simulate any path rather it just chooses a neighbouring state. So practically it does not consume any memory and is very efficient for problems involving huge graph spaces. 
There could be two types of problem viz: goal satisfaction, optimization. In goal satisfaction problem we have seen earlier that we want to reach a goal state satisfying all the constraints. While in optimization problem we define an objective function and the goal is that we want to maximize/minimise the value of this function. These optimization problems can be efficiently solved using local search algorithms. As local search does not care about paths, and just make moves from a current state to a neighbouring state. It might not give the best available solution but generally gives a quite satisfactory solution.
Let’s say we want to minimize our objective function; our algorithm will do it as follows: From the starting state it will choose a neighbouring state with least value. If the value of this neighbour is less than the value of the current state, it will change the current state to the neighbour. Our algorithm will repeat this task till there is no lesser valued neighbour available. Hence it will give a minima as the output. Note here that it is a local minima, there may exist some minimas lesser than this. Further, this local minima totally depends on the start state. A change in the start might also change this local minima. E.g.: Hill-climbing search, where goal is to reach at a local hill (maxima).
We have seen earlier that n-queen is a constraint satisfaction problem but we can model it as an optimization problem. Let’s define the objective function as the number of queens which are in attacking position. Now all we need to do is to minimise the value of this objective function. here, we define a state as all the 8 queens are on the board in some configuration. The successor function is that we would move a queen to another square in the same column. Note here that although a queen in general make other moves too but to decrease the complexity of the question, we are constraining it to move in a single column. As we know this local search give a local minima so we might stick in a configuration where the objective function is at its minima but what we wanted is not satisfied. For example: In n-queen problem we want no two queens to be in attacking position but we might end up getting some queens in attacking positions. Therefore, ideally it would be a solution but it is now what we wanted. If we use hill-climbing approach 14% of time it will solve the problem while other time it will stuck at a local minimum. Still it is worth it because it can give output in 4 steps (complete) and 3 steps (local minima) while a state space has 17 million states.
There are few major drawbacks of Hill-climbing. The first is that it gives a local optimum. Second is that if we encounter plateaus our movement might just stop or slow down considerably and we may not even know (as we can only check for the closest neighbour). Third is diagonal ridges for that lets assume a 2-d space but we define movement only in a single dimension i.e. either X or Y. Now let’s supposed to get to a neighbourhood we need to go diagonally i.e. 1 step X and 1 step Y. But as our movement is constrained in a single direction, we won’t be able to do it.
We see that it is highly likely that our solution will be stuck in a optima. However, we can trade off improvement of our algorithm with some memory and time. What we do is if there is no downhill (or uphill) moves, we allow sideways moves. And hope that algorithm can escape from the local minima. If we allow sideways moves with a limit of 100 for 8-queen problem chances of complete solution will increase from 14% to 94%. However, the number of steps will also increase from 4 to 21. We do it using Tabu search where we maintain a queue of a fixed length. We would add most recent states in the queue and drop the old ones. We need to take care that we don’t stuck in an infinite loop but adding a currently tabu’ed state. This is a great method to avoid plateaus. An exhaustive search like BFS can also be used instead of tabu to get to a point where we have passed the plateau and then we can shift to local search.
We can also avoid getting stuck in a local minima in hill climbing by introducing some randomness in our algorithm i.e.: stochastic variations. First method is we can use random walk hill climbing in which, to get to a neighbouring state we define a probability (p) and with a probability (1-p) we will choose a total random state. Second method is random restart hill climbing which works on the principle of “if you didn’t succeed, try again”. If we didn’t find the global optima, we will choose a random state and start over. We will repeat this task till we find the global optima. We can also combine both, the basic idea is that we want to introduce some sort of randomness. All of these methods are asymptotically complete i.e. if we do this many number of times, we will find a complete solution. 
Another algorithm for local search is simulated annealing where we try to get away from local optima by introducing some randomness in the form of energy (jumps). It just like a ball falling from a hill, if the ball sticks in a minima; we give it some energy and hope that it would jump through that local minima and get to part of the hill where it has room for improvements. Basically, we are providing randomness so to go to worse states which could turn out to be beneficial if we are stuck in a local optima. However, in practice it is very slow therefore, until unless global optima is very important, we don’t generally use this algorithm. We may also decrease/increase the energy (Temperature-T) as we go towards our final goal.
Let’s discuss local beam search, it is similar to hill climbing but we are given some memory. Assume we are allowed to keep track of k states. Now we start from k randomly selected states and check their successors/neighbours. Out of all the neighbours we choose the k best neighbours. And continue the process till we find the solution. The problem with this algorithm is that we are choosing the k best neighbours among all the neighbours so we might end up getting neighbours which will take us to the same minima. Hence, it will affect the diversity of our states. We can introduce randomness here by not choosing the k best ones but choosing the k best ones with some probability (p) and with 1-p we will choose the others. (p might not be equal for every state)
Let’s discuss genetic algorithms. The analogy here is natural selection. The best one survives and the not suitable ones generally dies. Let’s say a state outputs two neighbours (children). We define a fitness function which calculates the fitness of a state. The more fit neighbours are more likely to survive. To introduce randomness, we borrow the concepts from evolution like random selection, cross over and random mutations. We follow the idea of local beam search and choose k random states but instead of a single state we choose a pair of states to get successors. We find the fitness of each state (chances of survival) and the using that fitness we randomly select the k states. Observe that this process is random hence, there is high chance that the states which has high fitness will repeat and the low fitness states will be removed. Using the new k-states we perform partition (cross-over). We randomly select a point and then interchange the left portion of a state with the left portion of another state. And this crossed-over generation becomes our next successor. After this we introduce random mutations in our successors. This process is totally random just like real genetic mutations.
Let’s see the benefits of cross over, first thing to note is that the output of cross over is very different from their parents. We couldn’t have just reached to that state in a 1-2 moves as they are not in close proximity. This is basically taking us quite far from the initial state and may even take us away from local optima. The major drawback is that it is very uncertain because at every point we are introducing lot of randomness. Therefore, it is not easy to reproduce the results. Apart from best fit natural selection theory, one another perspective of looking at these genetic algorithms is that it follows the concepts of how animals and their progenies are related. Like the cross linkage happens in chromosomes segregations. Two chromosomes one from father side and one from mother side are fused together. However, they can be categorised into two clusters i.e.: father side and mother side. But still they are fused together at a point and when they segregate some part of father chromosome takes the mother chromosomes and vice versa. That’s why the progeny has the quality of both father and mother. Here also the progeny will have a mixed characteristics of their parents.
The method of introducing mutations might be different for different problems. For example: in a travelling salesman problem we cannot choose the best k cities to travel. So, introducing mutation there won’t be the same as discussed above. We first define a permutation of the cities to represent the problem and then to introduce mutations we do something like randomly swapping two indexes from the ordered set. Or may use greedy or exhaustive search techniques to swap. But the main idea is that we aren’t removing any of the cities while introducing mutations. A repeated or removed city won’t be a valid solution. However, any of the set in which every city is present will be a complete solution. And among these complete solutions we need to find an optimal solution.
To introduce cross over we can pick a parent (randomly) from the two states and then choose the first city and then use a greedy way to choose cities leaving that city in both parents. Simultaneously maintaining a Boolean of visited cities. So that no city is picked if it is already visited.
Many algorithms are nowadays based on the idea of hill climbing. One of them is Gradient Descent. It follows the same logic but on a continuous space. Let’s assume a continuous space to apply hill climbing here we first need to discretise the space. Or we can use the idea of gradient descent/ascent to move. If we want to maximize the objective function, we will use gradient ascent (going upwards) otherwise gradient descent (going downward). Assuming we have a continuous and differentiable function of n variables. We then calculate the partial derivatives which gives us tangents at that point and then we decide where do we need to move. To decide how big step, we are taking we consider a factor (lambda) and multiply our partial derivative with that factor. So basically, we are changing our X(final) <- X(initial) – (lambda)*(partial derivative). We repeat this process till we get the tangent equals zero. This point will give us the local optima. To choose lambda we have two methods. Either we can choose a larger lambda and then decrease its value or can choose a smaller lambda and increase its value till our function is decreasing. We may follow other higher methods too.

Reference:
1. http://www.cse.iitd.ac.in/~mausam/courses/col333/autumn2019/
2. http://ai.berkeley.edu/home.html
