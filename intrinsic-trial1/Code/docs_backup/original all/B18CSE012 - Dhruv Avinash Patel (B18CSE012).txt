Artificial intelligence
Understanding and building Intelligence


#Defining AI
All the definitions of AI generally are looked at from the perspective of two dimensions. Rationality and thought process/behaviour. This leads to 4 different approaches people usually take to make an AI model.
      1. Thinking like human (Cognitive Modeling)
      2. Thinking Rationally (“Law of thought” approach)
      3. Acting like human (The Turing Test Approach)
      4. Acting Rationally (Rational agent approach)
#Intelligent Agents
Let's use the example of terminator to explain the concept of Intelligent Agents. He came to interact with earth and used this eye-camera to perceive his environment and his machine gun hand to shoot anything he doesn't like.
In this case, terminator is an agent because it is perceiving the environment(earth) through his sensors(eye-camera) and acts upon that environment through the actuators(his machine-gun hand).
A percept would be an agent's input from the sensors at a given instant. (The current input from the eye-camera of the terminator). A percept sequence of an agent would be the complete history of everything it has perceived.


#Reflex Agent
The actions selected by a reflex agent are based on the current precept (which could be saved in a memory) and not any previous or future percepts. A precondition for reflex agents to work properly is that the environment should be fully observable.


#Planning Agents
The decisions hypothesized by this kind of agent are based on the consequences of the actions, towards attaining the desired goal.


#Defining a problem
There are 5 main components of a well defined problem: Initial State, action function returning set of action available at a particular state, transaction model returning the state after a particular action on a state, a goal test checking if a state is a goal state, and a path cost returning a cost to each transaction.
A solution is a sequence of transactions from initial state to a goal state, and an optimal solution would be the one with lowest path cost.
Generally a problem comes under either Toy Problem (with concise and exact description) or Real-world Problem (general problem formulation with no exact description).
Let's take the example of a 3x3 sliding puzzle to define the problem. A state would represent the location of all 8 tiles and the blank void. Initial state would be the one we start with, and actions would consist of moving the void to left,right,up or down(depending on the position of the void). The transaction model would return the state after an action on a state. Goal test would be the state of final arrangement and path cost would be the ratio of hardness to do an action at a state.


#State Space Graph
The initial state, actions and transaction model make up the state space of a problem. Each node in the graph represents a state linked together by edges representing the actions. A path would be a sequence of states connected by actions.


#Search Trees
A search tree would be an action sequence tree starting from initial state as root. The child nodes of a state would represent the successor of the actions of the parent state. Unlike the state space graph, in the search tree, each node/state also represents the action sequence it takes to reach that state (the path).
Theoretically, we try to compute both the state space graph and Search tree on demand and as less as possible.


#Searching a tree
In general, a search problem involves looping from an initial state, choosing a leaf node according to the exploration strategy and check it. if not the goal state, expand the leaf nodes to the tree. The three important things here are to decide the strategy, the method of expansion and the fringe we choose. Fringe is a (ordered) set of unexplored nodes we want to visit next.
Certain properties to consider in a search problem are, the algorithms completeness, optimality, and the space and time complexities.


##Depth-First Search
We go through the child node first (strategy) and have a LIFO stack for the fringe. This method can be complete if we avoid the cycles in the state space graph, but it does not find an optimal solution. If the states are finite, the time complexity would be O(total states), given no loops, and fringe’s space complexity would be O(branching factor*maximum depth).


##Breadth-First Search
We go through the sibling states first (strategy) and have a FIFO queue for the fringe. This method is complete ,but it only finds an optimal solution, if all actions have the same cost. The time complexity would be O(branching factor^goal state depth), and fringe’s space complexity would be O(total states).


##Uniform Cost Search
We go to the cheapest neighbouring state first (strategy) and have a priority queue according to the total cost for fringe. This method is complete if costs are not negative, and it will always find the optimal solution. The time complexity and the fringe space complexity  is O(branching factor^(solution costs/least arc cost)). Though it is optimal and complete, it searches for every direction and has no idea about the goal location.


If the solution is expected to be found in the greater depth of the tree, we could have an iterative DFS search with depth limit, to leverage the BFSs time complexity and DFSs space complexity.
Also, all the fringe strategies used in the above search algorithm can be implemented by a priority queue with different priority.
#Informed Search


##Heuristic Function
Heuristic function estimates the cost of the cheapest path from current state to the goal state.
NOTE: The output of the function only depends on the state of the input node, and not other nodes except the goal state.


##Greedy Search
Greedy search leads you to nodes, which seems closest to the goal, first. The fringe priority would therefore depend on the heuristic function (or the straight line distance). This method is not complete for the search tree version if we find any loops, but in case of a state space graph, it is complete (given the no. of nodes are finite). The space complexity for the tree version is O(branching factor^maximum depth) (but with a good heuristic function, it can be reduced)


##A-Star Search
The cost function is the summation of the heuristic function (forward cost) and the cost to reach the node (backward cost). This method is both optimal and complete, given the heuristic function is admissible.
An admissible heuristic function never overestimates the cost to reach the goal, hence not getting stuck in a loop of good plans. Or h(x)<h*(x)[true cost to a nearest goal].
An admissible heuristic function would always lead to an optimal solution because, even if a parent node of the optimal solution has the sub-optimal goal state as a child, optimal solution will expand before any other goal state since cost to optimal solution and its ancestors will be always less than other path leading to other goal states.
Since the cost function for A* is non-decreasing, we can have counters for each state. In this case we expand to states with cost <= cost of optimal path, unlike UCS where we expand in all directions equally.
A-star search can be helpful in many applications like path finding games(pac-man) and robots, language analysis and speech recognition and much more.


##Relaxed Problems
A relaxed problem is a superset of a problem with less restrictions. It’s state graph has the same state nodes but more edges (actions). Therefore, the optimal solution for a relaxed problem is the same as the original problem, unless an edge provides a shortcut. Hence, the cost function for a relaxed problem is the admissible heuristic function for the original problem.


#Reference
 Artificial intelligence : a modern approach/ Stuart Russell, Peter Norvig.