Name: CHINTALA NIRMAL SRINIVAS
Roll No: B18CSE010

NOTES(Sep1-Sep11)

Introduction:
	Definition of AI depends on the type of classification we are talking about(in terms of thought process or behaviour or or performance).We want AI which think like humans and act like humans and also think rationally and act rationally. In this context rational means achieving goal. Acheiving goal depends on some factors. For example if a student wants to go from home to college, there are factors like weather condition and traffic which matters in reaching the goal. 

Goal is expressed in terms of utility. Utility means giving some value to our goal. So maximizing utility happens if we are being rational in reaching our goal.The main aim of any AI would be maximizing the expected utility. By comparing brain and AI we learn that we actually motivate from other things to build up new things but we may not build similar things. For example  idea of making aeroplanes comes from birds but the wings of aeroplanes have different functioning and completely different to the functioning of wings of birds. 

There are tasks which an AI can perform and also some tasks which AI cannot perform. For example, playing a decent game of table tennis or folding laundry are some of the tasks which AI can do. But, tasks such as writing a funny story or having a conversation with a person for an hour may not be done by an AI because an AI must be taught initially what is funny which cannot be done programmatically.


Agents and Planning:
	Agent: Agents are generally machines or robots(in particular non-living things) which are used by humans to perform tasks. We want agents to be rational that means we want maximum utility. So a rational agent maximizes utility. Factors such as percepts, environment etc. affect the rationality of the agent.We generally talk about the agents which can plan. Agents can plan depending on how we program them. There are different types of agents. 

First one is reflex agent.Reflex agents are the agents which acts depending on what actually they visualize currently. They do not think about the consequences of their actions. Reflex agents may or may not have memory. So they cannot store any past state or utlity(those agents who do not have memory). 

Planning agents are another type of agents which understands the consequences of the actions. They will keep track of their goal. They must be aware of the model of the world to understand the consequences. 
There are two types of planning(optimal and complete). Optimal planning is the planning in which the goal can be reached in the best possible manner whereas when we reach the goal in anyway possible it is called complete planning.


Search Problems and Search Algortihms:
	Search problems: Search problem consists of state space(one of the possible states), successor function(stats that how the agent moves in a particular state), start state an goal test. A Solution is sequence of actions which makes the agent reach goal state(final goal of the agent) from start state(initial state of the agent).

Whenever we have to solve search problems, we initially model the environment i.e, proper assumptions are made before we start solving the prolem.Parameters of a search problem depends on the type of the problem. For example if our problem is to find a path to reach final state, state space will be locations in the path, actions will be the four directions, successor functions update locations and finally goal test will be the final state. But if the problem is to eat all the dots, states will be the locations including the dots, actions will be four directions, successor functions will be updating locations and dots and goal test will be eating all the dots. A goal test need not be a state, in this case it is the condition. 

State spaces can be represented in terms of graphs or trees depending on how we wre interpreting them. Each node in the graph or tree represents the state of the agent. When an agent moves from one state to another state, two nodes representing these states will be connected by a directed path. So when we consider all possible states, it would be a graph. In state space graph each state occurs only once.

 A Search tree may not contain all the possible states as in the search graph. In search tree a state can be repeated which is not possible in the case of search graph. In search tree, root node will be the start state and children will be the successors. In search tree, possible paths are shown between any two states and in search graph possible states are shown. If there is a cycle in the search graph, there will be repeated structures in search tree making it infinitely large, so we put some constraints to avoid this. Searching/Expanding nodes in a search tree depends upon the type of algorithm we are using.(DFS or BFS). 

Fringe means the all the possible nodes that can be expanded from a partcular node. When we are using queues for search problem, initially fringe queue conatins root node and it will be taken out and put it in the main queue and it's possible expansions will be pushed into the fringe. Expanding the nodes depend upon the strategy we use. When we use DFS, from root node we expand only the first node which is connected to it and in the first node, we do not expand all the nodes connected to it. Maintaining a fringe depends on how we are solving the problem.


Properties of Search Algorithm:
	Properties of search algorithm: DFS will produce complete solution if we take care of the cycles. But it may not produce he optimal one because it stops as soon as the goal is reached. That path need not be an optimal one. Time complexity for DFS woulb be O(b^m) where b is the branching factor(no. of children that are directly connected to a node) and m be the height. Because we are traversing through every node, in the worst case, we will traverse through all the nodes. Total number of nodes in the tree are 1+b+b^2+......b^m. Space complexity will be O(m). Beacuse we will save only one node from one level in the path. Space complexity of fringe will be O(bm) because in every level we need to keep track of all the nodes which are on the same level and side to it(next sibling). So in every level, b number of nodes needed to be taken care of. Since there are m levels, space complexity will be O(b.m).

 In BFS, time complexity will be O(b^s) where s is the level in which goal is obtained. In BFS we will travrse through all the nodes in the same level before we go to next level.So space complexity will be also same as time complexity i.e, O(b^s). BFS provide a complete solution and it provides optimal solution only if the costs are1, here cost means cost of moving from one node to next node.  DFS outpeforms BFS when the goal needs to be find at higher levels i.e, far from the root node. Similarly BFS outpeforms DFS when the goal needs to be find is at near to root node(at lower levels).

When we perform DFS to solve a search problem we don't stop when we encounter our goal test, we push it into the fringe and we stop when we pop the goal test from the fringe and push into the main stack. Each element of a stack can be a linked list(path). DFS has better space complexity than BFS. 

Iterative deepening: If the goal is near to the root node (not at higher levels), we set the limits of depth and through iterative method, we search for the goal. For example if the depth of the goal is 4, we initially set the limit to 1,we apply dfs and search for the goal upto level 1, if the goal is not found limit is incremented and again DFS will be applied from the root node. In this way eventhough the search operation is repeated on some nodes, the space complexity will be efficient and we don't need to search for nodes at depths greater than the depth of the goal.


Uniform Cost Search:
	If there are different costs to the paths, we need to have an algorithm which finds a path with lowest cost.Those search operations are cost sensitive search. We could do this using dfs or bfs, but we will have another parameter which needs to be taken care of i.e, cost.Uniform cost search: Search operation based on BFS to find the goal with minimum cost. First we will have a root node in our fringe, we will take it out and expand it. Now the children may have different weights, so we select the node with minimum weight and expand it. Again among all the possible nodes in the fringe, we select the node with minimum weight and add it to the cumulative weight. 

In this way, if we reach our goal, we will have cumulative weight but we so not stop here as there may be a path with least cost so the we continue the operation until we find another path which is the least cost to the goal or no path with least cost. We will stop when we pop out the goal from the fringe that means we have got a goal with least cost. I the total cost of the solution is C* and minimum cost of an arc is e, then in the worst case, the depth where the goal can be there is C*/e, so space complexity will be O(C*/e) and time complexity is O(b^(C*/e)) where b is the branching factor. Space complexity of the fringe will be same as time complexity i.e, O(b^(C*/e)). The solution is complete solution if the minimum arc cost is positive and the best solution has finite value of the cost.

 The disadvantage is that we consider all the nodes initally in every direction to reach the goal. All the search algorithms are same except the fringes. Datastructure we use for frimge varies with the algorithm of search problem. Incase of DFS, we use stack and incase of BFS we use queue. We can use priority queue incase of uniform cost search for fringe. Efficiency of Search operation depends on how we model the world.  


Informed Search:
	Informed search: Search operations can be made better with some additional features. Search Heuristics: Heusristic is a function which estimates that how far the agent is from the goal. This function does not provide optimal solution, it just estimates the distance between goal and agent and tells which direction would be better for agent to move. Estimation depends on the parameter the function is using to calculate the closeness of the goal and agent(euclid distance or manhattan distance). Heuristic function may not produce the optimal solution. 

Greedy search: Greedy search is based on heuristic function. Initially we will have root node in our fringe, we will expand it and based on heuristic values we will expand that node which have least heuristic value and continue this process until we get the goal state and pop out from thr fringe. This solution may not be the optimal solution but it's a complete solution. Agents using greedy search do not plan, they just explore the path which have least heuristic value and do not consider about the future consequences. So these agents are reflex agents. 

A* search: A* earch makes use of the advantages in UCS and greedy search. UCS is slow but produces optimal solution but greedy is fast but may not produce optimal solution. UCS calculates the cumulative cost (cost from root node to the current node) i.e, backward cost and greedy search take care of the least heuristic value in the current scenairo i.e, forward cost. So A* function combines these two functions by summing them to make sure that it produces the optimal solution in a better possible manner. 

A* should terminate when we dequeue a goal. A* search may not produce optimal solution if the heuristic function over estimates. So if the heuristic values have lesser values than the cumulative costs, then A* search would be optimal. So thise heuristics are called admissible heuristics. Admissible heuristics underestimate the forward costs and inadmissible heuristics overestimates the forward costs. So a heuristic is admissible if the heuristic value is less than or equal to true cost to nearest goal and it should be positive.

Optimality of A* search: Let A be the optimal goal and B be the suboptimal goal. Suppose B is the fringe and let n which is the ancestor of A is in thr fringe. We can claim that n will be expanded first then B. Because, f(n)<=f(A) as heuristic function in f(n) is admissible cost of path from n to A will be estimated less than actual cost. Also f(A)<f(B) as g(A)<g(B) and both the heuristic functions will be zero. Thus n will be expanded before B. So every ancestor of A will be expanded before B.Hence A will also be expanded before B. So A* search is optimal. 

Properties of A*: On comparing A* and UCS, A* will explore less to get the optimal solution as the heuristics function will inform the way where the leat cost path is there withour cually exploring the paths. A* expands mainly in the direction of goal.  A* explores more than greedy search. Video games, Resource planning problems are some of the areas where A* search is used.Inadmissible heuristics are useful in some cases. Inadmissible heuristics are some how faster than admissible heuristics. Inadmissible heuristics are useful where we are looking for a solution which can be sub optimal also. Admissible heuristics will be solutions to relaxed problems. Relaxed problems are the problems where new actions are available with less constraints.

Sources:
1. Slides used in the class
2.Artificial Intelligence,a modern approach by Russel & Norwig


