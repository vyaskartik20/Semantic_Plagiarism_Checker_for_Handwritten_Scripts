Earlier in our topics like local search, CSP’s etc our task was to achieve a given task by our agent like in case of informed and uninformed search the task was to achieve the goal in the shortest path in the least time interval. Considering the practical situation there are also opponents for the agent which plays against the agent.  Now let's discuss adversarial search techniques which work in situations where we have an opponent agent also which work against our agent. There are many real life examples where an artificially intelligent machine has defeated the champaign players. In 1997 Deep Blue company designed an artificial intelligence  chess machine which has defeated Garry Kasparov who is a well known chess champaign. Coming back to adversarial search, we have different games in which the opponent behaves differently but the action of the opponent would be such that it affects the agent the most. There are different games in which the opponent's task is the same as that of the agent; both of them compete to achieve the goal. There may be cases where we know the information of the states like in the case of pacman we know the position of the dots i.e. the agent is aware of the environment, game layout etc. whereas there may be case where the agent is not aware of the upcoming environment like in case of card games the agent does not know about the future. So for all this game we will try to evaluate a policy or a strategy which recommends a move from one state to another i.e. what action we should take given certain things. Like in case of informed search, the agent was trying to find the best next state and depending on it, it was trying to find the best possible solution, similar we will see in case of adversarial search in which we need to keep track of 1 more agent that is the opponent. Let's discuss states and functions for deterministic games. Consider a case in which we have many states with starting state s0 and n players. So for each player we have an action(A) which depends on the previous action of the other player or the state of the player. We would have a transition function(S*A-->S), which basically would take us from one state to another after appropriate action from a given state. There is a terminal test(S->[t,f]) which tests whether the current state is the goal state or not. Thereafter we have terminal utilities, which basically tell us the goodness of the state. 


Lets see about zero sum games. In this type of game we have 2 opponents and they play against each other and their objective is the same i.e. both of them try to achieve the same task. Like in case of chess we have 2 players which have the same objective to win the game. In zero sum games, if one the player wins then the other player automatically loses the game. If we assign 1 for win, -1 for loss and 0 for tie then in any case we have a sum of 0 therefore known as zero sum game. These values are known as utilities which have a value opposite to each other. Similarly we have another type of games known as general games. The utility values are independent of each other's players. In this type of games cooperation, indifference, competition and more are all possible types of games. So now let's continue with our discussion on adversarial search. In this case 2 opponents which compete against each other and change their state depending upon their location and environment. These players change their state in an alternative manner. The Actions of each player will depend upon each other and will go on till the game does not end. Since for any game we can represent the states using a search tree. So from a given starting state depending on the game the different states may arise eg- in case of pacman with movement either left or right. We can have 2 states from the given starting state. In a recursive manner each state can be expanded and we assign a value to each of the terminal states and each value tells us the goodness of the state. So we assign these values in such a way that the optimal path state has the highest value and for other states the values are smaller than this state. The values of the states are the utility values. So for the graph all the leave notes become our terminal states and other states as non-terminal states. So for the utility values of non-terminal states we can calculate the maximum utility of the child of a node and iterate from bottom to top of the tree. Now let's look at the adversarial search tree in which we have an additional opponent agent which is against our agent. Similar to the previous case in which we have a pacman and an opponent agent which tries to kill the pacman. Similarly we create a search tree using the various moves possible between the pacman and the ghost and assign the goodness value to each leaf node. Since in this case we have an opponent therefore we take the minimum or maximum value of the child nodes of a node and compute the goddess value of a node. Depending upon the situation we need to  consider the value for the node as the main motto of the opponent agent to act against the pacman hence therefore we consider the minimum or maximum value depending upon the condition. So basically the states which are under the agents control i.e. under pacman control then we take maximum value else on the other side we take the minimum value. 


Let's consider another example of tic-tac-toe in which we have an agent and an opponent agent and first we have a turn of our agent which can have any of the 9 possible states and opponent have the next 8 possible states and the turns goes on. Since the states are large therefore the constructed graph is very big. There are basically 3 possible terminal states in which either the agent wins with utility value of -1 or the opponent wins with utility value of +1 or the game draws with utility value of 0. Since the state graph for the game is quite large to be built so in case we go upto certain depth say 3 or 4 or may be sometimes till 8 or 9 nad we calculate the goodness of the state and try to estimate the chances of winning or losing the game. There may be samy such games which are deterministic and zero sum games like chess, tic tac toe checkers etc in which one player minimizes the result while the other maximizes the result. 


Lets look at the implementation of the minimax function, so we would have 2 functions, one for maximizing the level and the other for minimizing the level. We will value the value recursively and starting from the root node. So if we are at the maximizing node then we assign the value of node to -infinity and then calculate the maximum between current and the minimum of the successor of the current node i.e. max(V,min(successor)) and return the value. Similarly if we are at the minimizing node, we initially assign the value of the node to +infinity and then calculate the minimum between current and the maximum of the successor of the current node i.e. min(V,max(successor)) and return the value. So the main function of the minimax implementation would be a function in which we check whether the state is terminal state or not, in case of terminal state we simply return the value else we calculate using the min or max function depending on the condition. Lets understand the algorithm using an example in which we have a root node which is maximining and 3 successor nodes of the root node which are minimizing each of the 3 sussor node further have 3 terminal nodes which has the value as follows from left node to right node (3,12,8,2,4,6,14,5,2). So for the first minimizing node the value for that node will be 3 and the other 2 nodes will have values as 2 and 2. While the root node which is maximizing node will have the value 3. So let's check the values of the root node using the pseudo code which we have discussed earlier. Value of root is maximum of -infinity and minimum of the child nodes and value of the child nodes are minimum of +infinity and the child nodes so the value of the child node of the root node will be 3, 2 and 2. So the value of the root node is 3. Hence our pseudo code works for this case and will also work for the other cases too. So the efficiency of the minimax search could be compared with the exhaustive case of DFS search. The time complexity of the search is b^m and space complexity is b*m. So in the case of chess we have values of b as 35 and m as 100 approxity and working on the algorithm is not feasible. So we need to do something so that we go to a certain depth and make a decision. Action of the agent will depend on the type of the opponent. If the opponent is not smart enough then the problem becomes complex for solving as for such a case we need to take all possible possibilities and hence the efficiency of the problem solving decreases. In practice as we discussed earlier, it is not possible to search till the leaf node as in case of a chess problem we would have a very deep search tree and practically calculating values of the nodes is not feasible. So for such cases we search only till a particular depth of the tree and calculate the utilities of the nodes. So for such scenarios we calculate the utility values using an evaluation function which estimates the utility value at a particular depth node and finds the utility values of the other nodes. Since in such cases we are using the evaluating function so we are not sure that always an optimal answer would be returned by the algorithm. The evaluating function which we here use uses iterative deepening for an anytime algorithm. It is so called iterative deepening for an anytime algorithm because for a given time we calculated how deeper we would go into the search tree. In general it is always helpful to search deeper because as we go deeper the probability for getting the optimal solution increases. In other cases we may use a complex evaluation function which gives us better estimates of the utilities. So we are making a tradeoff between complexity of features and complexity of computation. The solution of the problem also depends on the depth upto which we are going. There may be cases where the agent is stuck if we go less depper whereas in the same case if we go for a deeper technique then in such case we can get a solution. 


Let's look at the evaluation function, it is the function which gives us the estimated utility value of a particular non-terminal node. So in case of chess we can say that evaluation function is something polynomial of functions like Eval=w1f1(s)+w2f2(s)+w3f3(s)+.......+wnfn(s). Here wi is some weight which is assigned to the function fi(s) whereas fi(s) are some functions. The value of the evaluation function will depend upon the states of the agent and the opponent. It may happen that function value is less for some cases whereas high in some cases considering the state of the agent to be the same and opponent position changes. In the case of a pacman game we can consider that the ghost acts like an agent in which their goal is to kill the pacman and hence will try to choose a path in such a way that they reach the pacman with their evaluation function. 


Now let’s see how we will cut short the game tree with the assurance that we do not remove the optimal move.  Let’s understand this concept using an example in which we have a root node which is maximining and 3 successor nodes of the root node which are minimizing each of the 3 sussor node further have 3 terminal nodes which has the value as follows from left node to right node (3,12,8,2,4,6,14,5,2). So we will calculate the value for the 1st node which is found to be 3 as the minimum of 3,12,8 is 3. So we can ignore all the values which are smaller than 3 in the further computation for calculating the value for the root node. We will only ignore if the value encountered is less than 3 else we will compute because for values more than 3 we can't say the minimum value for the successor node of the root node. So based on these values we can be prone to some part of the search tree which will help us to go deeper into the search tree. 


Let's discuss a technique called alpha-beta pruning. In this technique we ignore the further exploration of the node, let's say we are talking about the minimizing node in this case if the node has a value less than the already available minimum value of the node then we would ignore that node further exploration. Symmetrically we can think for the maximizing node. Let's discuss the pseudo code for maximizing function. Initially we set the value to -infinity, and then compute v=max(v,value(successor,alpha, beta)) for the successor function. And if the value of v is greater or equal to beta tha we return v else we set alpha as max(alpha, v). Similarly we define the function for minimizing nodes. So this pruning has no effect on the minimax value of the computed value for the root node. Sometimes ordering of the nodes may help in reducing the computation. 


Using the brute force minimax algorithm will always give us the answer but the time consumption would be very high so for that we use alpha-beta pruning which cuts short the tree and decreases the time consumption. The effectiveness of alpha–beta pruning is highly dependent on the order in which the states are examined.This suggests that it might be worthwhile to try to examine first the successors that are likely to be best.If this can be done, 2 then it turns out that alpha–beta needs to examine only O(b^m/2) m nodes to pick the best move,√ instead of O(b ) for minimax. This means that the effective branching factor becomes b instead of b—for chess, about 6 instead of 35. Put another way, alpha–beta can solve a tree roughly twice as deep as minimax in the same amount of time. If successors are examined in random order rather than best-first, the total number of nodes examined will be roughly O(b 3m/4 ) for moderate b. For chess, a fairly simple ordering function (such as trying captures first, then threats, then forward moves, and then backward
moves) gets you to within about a factor of 2 of the best-case O(b m/2 ) result. 


So till now we have discussed conditions where situations were deterministic i.e. the agent was able to determine the move. But in cases like in a game of dice we don't know which number would turn up which are non-deterministic conditions. Now discuss our new topic i.e. uncertainty and utilities. Usually the value of utilities of successors of a node have a huge difference and hence we assign a probability value to each node and calculate the estimated cost using the product of probability of the edge and the utility value. We do not always know the result of an action by the agent. It depends on the explicit randomness of the situation like in case of rolling dice, we explicitly do not know the output. There may be chances of unpredictable opponents where we don't know how the agent would react to our move when we move from one state to another. There May be uncertainty like the agent we use may be a robot which may get distubed due to the environment like the wheel of the robot got stuck etc. hence the uncertainty comes from the environment not from the agent side. So for such scenario we would use expectimax search where we compute the average score under optimal play the nodes are same as in case of minimax search but we would have a chance node which are like the min nodes  as in the case of minimax search but the outcome is uncertain and we would calculate the expected utilities i.e. take weighted average of the children. The pseudo code for expectimax would be somewhat similar to that of the minimax search max function. Here we calculate the probability of each successor and find the value of v using v=v+p*value(successor) and return v. like we have 3 successor of node with their probability and utility values as (½,8),(½.24),(⅙,-12). The value for the root node would be (1/2)*(8)+(1/3)*(24)+(1/6)*(-12)=10. 


Now we will try to prone our tree in expectimax search. Since on pruning the expectimax search we are not aware of the value of the node therefore in general we don't prune a expectimax search tree.Expectimax requires the full search tree to be explored. There is no type of pruning that can be done, as the value of a single unexplored utility can change the expectimax value drastically. Therefore it can be slow. In case if we have a very large tree and we are not able to go to the very deep node then we make use of approximation and these values can be used to get a score here. 


As we are going to use probability in our further discussion then we let's see some basic concepts for it. In probability we use a term random variable which represents an event whose outcome is unknown. And a probability distribution is an assignment of weights to outcomes. There are some laws of probability which states that the probabilities of an event are always non-negative and probabilities over all possible outcomes sum to one. Let's revise the expected value concept. The expected value of a function of a random variable is the average weighted by the probability distribution over outcomes. Consider an example where we have 3 outcomes with probability as 0.25,0.5,0.25 with the following weighted value as 20,30,60. We simply multiply their probability value with the weighted value to get the answer on calculating the value we find the value as 20*.25+.5*30+.25*60=35. Hence the value of the node is 35. So the value of the node completely depends on the value of the probability which we assign to the branches. In practice, finding the probability is very difficult. So as of now we will assume that the probability is given to us. 


Now let's continue with our discussion on modeling assumptions. Basically our modeling assumptions will depend upon what assumption we are making in our world or environment. In case if we miss any assumption then in such scenario our agent will nor work according to our required way. The assumption which we are making we need to consider various other things like the optimism and pessimism of the algorithm. In many cases the agent may assume that the step is favorable but actually it is not favourable which results in deviation from our optimal path. So let's compare our algorithm with the type of ghost we can have. We will consider 2 types of ghost i.e. one which follows adversarial search technique and the other is the ghost which moves randomly i.e. assign value to the next move equally. Considering the pacman game example in which we consider 2 types of pacman one which follow minimax search technique and the other follow expectimax search technique. When the pacman is following minimax search technique and if the ghost is following adsarial technique then according to data we can win 5 out of 5 times with average score of 483 units while considering the ghost to be random ghost then in such scenario we would win 5/5 times and with an average score of 293 whereas if the pacman is following the expectimax technique and if the ghost is adversarial ghost then in such case we would win ⅕ times and average score would be -303 and in case if the ghost is random then in such case we would win 5/5 times with an average score of 503 units. 


Now let's look at other types of games in which we have certain uncertainty. Let's consider an example of a backgammon game. In this game dice rolls increase b: 21 possible rolls with 2 dice then the backgammon would have 20 legal moves and the depth of the tree would become 20*(21*20)^3 which is of the order of 10^9. So the chances of getting a very optimal solution would be very low.  So as the depth increases, probability of reaching a given search node shrinks so usefulness of search is diminished and limiting depth is less damaging but pruning is trickier. Consider a game of multiplayer in which each player is trying to maximize its own outcome utility value. The leaf nodes will have many values depending upon the number of players it can have. So from going from a node to successor node it may happen that the utility value of 2 players or more gets higher when we are trying to increase the value for one player. As each player tries to maximize its own utility value so for the other players we can't say anything regarding the utility values for other players. 


Let’s try to understand the utility values for the graph. Utility is basically the preference of the agent to achieve a particular thing. Let's consider an example in which we have n children and we have said that they would be given one out of the n things available to them depending on their work they achieve. The things could be anything like ice cream, chocolate etc. so each child would have different choices of preference to take one of the particular things like one would like to have ice cream preferred over chocolate. Hence these preferences over one thing could be understood as the utility values. There is something which is called the principle of maximum expected utility which states that a rational agent should choose the action that maximizes its expected utility given its knowledge. The utility of a state is relative to an agent. For example, the utility of a state in which White has checkmated Black in a game of chess is obviously high for the agent playing White,
but low for the agent playing Black. But we can’t go strictly by the scores of 1, 1/2, and 0 that are dictated by the rules of tournament chess—some players (including the authors) might be thrilled with a draw against the world champion, whereas other players (including the former world champion) might not. There is no accounting for taste or preferences: you might think that an agent who prefers jalapeño bubble-gum ice cream to chocolate chocolate chip is odd or even misguided, but you could not say the agent is irrational. A utility function can account for any set of preferences—quirky or typical, noble or perverse.  There may be many questions which may arise while learning utilities where do utilities come from? How do we know such utilities even exist? How do we know that averaging even makes sense? What if our behavior can be described by utilities? We will come to know about this as we move further. So what utilities we use. Lets consider an example in which we have a binary tree as the search state with depth 2 and values of leaf nodes as 0,40,20 and 30. Let's use a transformation function which squares the values and hence when we would consider the minimax search since the value of the solution of the transformed function would be still bigger and hence we would get the same solution but when we consider expectimax search with each nodes as equally likely then  the solution would change. So in case if we apply some monotonic function our solution changes hence for the expectimax search we make the utilities values accordingly as we have seen that the solution changes as we change the values monotonically. So utilities function from outcomes (states of the world) to real numbers that describe an agent's preferences. So there is a theorem which states that as long as our preferences are rational then we can make a function which can give us utility. So the utility tells us the preference of that state and behavior tells us how an agent is achieving that outcome. In practice we assign utility value and let the agent choose the behavior accordingly. Consider an example of getting an ice cream. For getting a single scoop of ice cream would eventually have no other possibility and consider the next node as getting two scoops of ice cream and since there are 2 possibilities of equally likely of being icecream fallen or it would get 2 scoops of ice cream.  Since we assign the values to 0 scoops of ice cream as 0 and 1 scoops of ice cream  as 100 and 2 scoops as 200 and since while traversing the tree we would get struck as the values of the nodes will become 100 and then the agent will choose the solution randomly. And in case if the utility value of the 2 scoop is increased then it would prefer 2 scoops of ice cream. 


Let's look at the preferences of the utilities and try to understand what a rational preference would mean. Let's assume that there are 2 prizes A,B. Consider the lottery system. So for getting a prize A we have probability of p and for B we have probability of 1-p. So we use the symbol > if A is prefered over B then we use notation of A>B and if the prizes are equally likely then we say that the prizes are indifference and denote it using A~B. We should have transitivity over prizes like (A>B)^(B>C)=>(A>C) i.e. if a is preferred over b and b is preferred over c then we can say that a is preferred over c. Some of the axioms of rationality as (A>B)V(B>A)=> (A~B), (A>B)^(B>C)=>(A>C), Continuity theorem as A>B>C=> there exist p[p,A;1-p,C]~B, substitutability A~B=>[p,A;1-p.\,C]~[p,B;1-p,C] and monotonicity theorem. Rational preferences imply behavior describable as maximization of expected utility. 


Now lets learn about the maximum expected utility principle(MEU). This principle says that if we are given 2 prizes A and B and U(A)>=U(B) ⇔ A>=B and U(p1,s1;....pn,sn)=summation of piU(Si) i.e. values assigned by U preserve preferences of both prizes and lotteries. Hence the maximum expected utility principle states that choose the action that maximizes expected utility. Now let's try to relate it with the human related utilities. The basic idea is simple. Consider the environments that could lead to an agent having a given percept history, and consider the different agents that we could design. If an agent acts so as to maximize a utility function that correctly reflects the performance measure, then the agent will achieve the highest possible performance score (averaged over all the possible environments). This is the central justification for the MEU principle itself. While the claim may seem tautological, it does in fact embody a very important transition from a global, external criterion of rationality—the performance measure over environment histories—to a local, internal criterion involving the maximization of a utility function applied to the next state. Note that behavior is invariant under positive linear transformations i.e. U’(x)=k1U(x)+k2 where k1>0. With deterministic prizes only ordinal utility can be determined i.e. total order on prizes. Consider an example in which entry fees of the park is 30 rupees and if a person needs to enter the park then he needs to either pay 30 rupees or he needs to rotate a wheel and if the wheel rests in a particular location then he may enter the park free. So this situation is like a lottery in which he has chances to enter the park free. The probability of winning would be very low. Money does not behave as a utility function, but we can talk about the utility of having money (or being in debt). Given a lottery in which we have two lottery X and Y with probability p and 1-p. So the expected monetary value EMV(L) is p*X+(1-p)*Y. U(L) = p*U(X)+(1-p)*U(Y). Typically we have U(L)< U(EMV(L)).


The value an agent will accept in lieu of a lottery is called the certainty equivalent of the lottery. Studies have shown that most people will accept about $400 in lieu of a gamble that gives $1000 half the time and $0 the other half—that is, the certainty equivalent of the lottery is $400, while the EMV is $500. The difference between the EMV of a lottery and its certainty equivalent is called the insurance premium. Risk aversion is the basis for the insurance industry, because it means that insurance premiums are positive. People would rather pay a small insurance premium than gamble the price of their house against the chance of a fire. From the insurance company’s point of view, the price of the house is very small compared with the firm’s total reserves. This means that the insurer’s utility curve is approximately linear over such a small region, and the gamble costs the company almost nothing. Consider an example of human rationality in which we have 2 cases A:[0.8,$4k; 0.2,$0], B:[1.0,$3k; 0,$0]. People will prefer case B as there we get a guaranteed prize of $3k with probability 1 although we are getting less but we are getting something instead of nothing. Consider another example as C:[0.2,$4k; 0.8,$0] and D:[0.25,$3k; 0.75,$0]. Here people will prefer C as there are almost the same chances of winning so the people will try to take advantage of money and will prefer more money. So people prefer B>A and C>D. and if U($0)=0 then B>A=>U($3k)>0.8U($4k) and C>D => 0.8U($4k)>U($3k). 


References -


1. http://ai.berkeley.edu/home.html.
2. Textbook referred  - Artificial Intelligence, A Modern Approach Third Edition by Stuart J. Russell and Peter Norvig.
3. Google meet lecture recordings of Artificial intelligence lecture (Course CS323 Artificial Intelligence September 2020 - November 2020) by Dr. Yashaswi Verma IIT Jodhpur.