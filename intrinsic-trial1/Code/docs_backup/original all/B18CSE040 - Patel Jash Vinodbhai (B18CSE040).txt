ARTIFICIAL INTELLIGENCE
- NOTES BY JASH PATEL (B18CSE040)

FROM: Sept 1, 2020 to Sept 11, 2020

Day1 marked with a brief discussion regarding course contents, evaluation scheme and grading criteria. 
The second part of Day1 led to introduction of AI. The terms describing AI were discussed. AI pertains to making rational decisions. Being rational means maximizing your expected utility. Applications of AI in various fields were also discussed. Some common daily-life examples include playing a game, speech processing, image recognition, robotics (humans designing humans), decision making and many more.

Day2 begins with basic differences between agents that plan. Reflex agents are the ones who choose action based on current percept + memory while, planning agents make decisions based on hypothetical future consequences. In short, planning agents must formulate a goal (test) to proceed further. The next differences to study are between optimal planning and complete planning. It can be stated that optimal planning is a subset of complete planning. At the end of lecture, search problems were defined. A search problem is defined using state space, successor function (with actions, costs), start state and goal state. A solution is a sequence of actions (a plan) which transforms the start state to a goal state. A search problem can be described as to find a path between two cities. The state space here is collection of cities, successor functions are the roads having distance as the cost and the start state and end state are the two cities.  
The second part of Day2 proceeded in the direction of search problems. Some new terms were defined. The world state includes every last detail of the environment. A pacman game example can be used to very well understand the search problem concept. Next is a state space graph which represents the problem to be solved using techniques. A state space graph represents all the states possible and thus it is not build entirely. 

Day3 brings recap of state space graphs, followed by discussing search trees. A search tree can be thought of another way of representing the search problem. It comprises of states represented as nodes, children corresponding to successors and such a what if outcomes. Again, similar to state space graphs, a state tree is not build entirely. Comparing state space graphs and search trees, analogy can be established between a subtree in search tree with a path on state space graph. Both have their own benefits and are used accordingly. For example, state space graphs having cycles would lead to infinite depth search trees. Moving on with searching in a search tree, the method is discussed. The steps include, expanding possible nodes until we find a solution (end state). Maintaining a record of partial plans saves time to a larger extent. Now the question arises of deciding which node to expand first? The answer to this caters to the knowledge of searching algorithms. Depth First Search (DFS) was the first approach to be discussed. The structure used to record partial plans would be a LIFO stack. The strategy discusses expanding the deepest node first and then building over. 
The second part of Day3 started with discussion on the properties of DFS based search algorithm. If we take b as branching factor (out degree), and m as the maximum depth, the total number of nodes can be calculated as (1+b+b^2+b^3+....b^m) leading to time complexity O(b^m). And for each expansion, the nodes at the same level are to be kept for consideration. This leads to a space complexity of O(bm). But the solution is not optimal as it gives the leftmost solution (a property of DFS). The next algorithm to be discussed is Breadth First Search (BFS). The structure used to record partial plans would be a FIFO queue. The strategy involves checking every branch for a node (adjacent nodes) and then move to the next sub node.  Taking same conventions as stated before, here complexity depends on the depth of the shallowest solution (a property of BFS). Let us assume the shallowest depth at which solution exists be s. Then, the overall time complexity would be O(b^s) and the space complexity would be O(b^s). This algorithm can be considered as optimal if all the costs of paths are considered to be same. Now, we can compare BFS vs. DFS. DFS performs well in case of space complexity and when solution is expected to exist in leftmost part. On the other hand, BFS performs well in terms of time complexity and when solution is expected to exist in shallow depths (i.e. less number of paths between start and end node). 

Day4 sets with quick revision of DFS and BFS based approaches. A brief explanation on the space complexity of DFS approach is given. Starting with start node, we expand the start nodes and store the children nodes in the stack. Next we expand the left node and again store all its nodes. And the process continues. Thus in the meantime, it can be observed for every expansion b nodes are stored. And after m expansions we reach the leaves of the tree. And now the stack can be reused for the next node. Thus it settles down at O(bm) space complexity. The next topic to be discussed was iterative deepening. The idea behind using this approach is to get the best of DFS and BFS (DFS in terms of space and BFS in terms of time). In this approach, DFS is run for iterative depths (1, 2, 3... and so on). This approach proves to be a better solution due to the fact (keeping in mind) that most solutions exist in the lowest level. There can be various techniques to implement iterative deepening.
The second part of Day4 addresses the problem of finding the least cost path. As BFS finds the shortest path, but in terms of actions (branches), we have to find a path with lowest cost (Cost Sensitive Search). Uniform Cost Search (UCS) can be thought of an idea of doing so. The algorithm focuses on expanding the cheapest (least cost) node first. The record of nodes is kept in a priority queue (cumulative cost being the priority).

Day5 revises the UCS algorithm and discusses the properties of UCS. Taking the cost of optimal solution as C* and minimum cost of each arc as E, then at each step(expansion) cost reduces by roughly E, thus to reach the solution effective depth(steps taken) would be C*/E. Based on these parameters, the time complexity comes out to be O(b^(C*/E)), analogous to DFS. And the space complexity also comes out to be O(b^(C*/E)). This solution can be considered as complete in case of C* to be achievable(finite) and positive E. The algorithm is optimal whose proof would be discussed later. Despite being complete and optimal, UCS has drawbacks. As UCS explores in directions of increasing weights, there is no smart work involved. If on the basis of direction of goal, the paths are to be explored it would be better in terms of complexities. Taking the three algorithms in consideration, all work the same except of the type of fringes (stores record of nodes). All fringes are priority queues having different priorities (cost for UCS, timestamp for BFS, DFS). The next topic to be discussed is search and models. Talking about google maps algorithm, drawbacks include the exclusion of unpaved paths or obstructions and diversions. This issue doesn�t pertain to search agents; rather it is due to absence of this situation in model.
The second part of Day5 ponders on tackling issues with previous algorithms and learning more efficient algorithms (Informed Search). We will be starting with the Pancake problem. A pancake problem is to achieve a desired configuration of pancakes from a given initial configuration. Each configuration is different sizes of pancakes kept on top of each other. At any step, you can choose a pancake and flip all the pancakes above it. So the problem statement is to find the minimum number of steps (flips) taken to achieve the desired configuration. Additionally, cost of flipping can be assigned in respect to number of pancakes flipped in the step (if at any step, you flip n pancakes, the cost of that step would be n). The problem can be considered as a variant of sorting numbers in their ascending order by prefix reversal (reverse all the numbers in the prefix). A search tree based approach was discussed.

Day6 talks about new topics like heuristics and much more. A heuristic is basically a function that describes how close a state is to a goal. Each search problem has its specific search heuristic. Heuristic doesn�t tell about possible solution or anything related; rather it just gives an estimate of how far the goal is. Heuristic guides us to the solution but does not guarantee optimal solution. The first algorithm we would be studying is Greedy Algorithm. In this algorithm, we expand the node with lower heuristic value (closeness to the goal).
The second part of Day6 continues discussion on Greedy search. It is possible that in Greedy search you don�t get a solution. Greedy search is not complete. Moreover in the worst case, it would be very much similar to the worst case of DFS. The next algorithm to discuss is A* search. A* can be thought of a combination of UCS (optimality) and Greedy (complexity). In simpler terms it can be stated that UCS follows lower backward cost search (cost/distance covered) while Greedy follows lower forward cost search (cost/distance left to cover). A* search depends linearly on both backward cost and forward cost (A* cost order = forward cost + backward cost).

Day7 continues with A* search discussion. The problem arises of when to terminate A* search. When we enqueue a plan containing the goal, we may get a solution but that may not be optimal, so we should stop when we dequeue a plan containing the goal. A* may not be prove out to be optimal in cases where heuristic dominates to a larger extent (actual bad goal cost < estimated good goal cost). So in order to keep a check, estimates must be less than actual costs. This brings out classification of heuristics into admissible and inadmissible. Admissible (optimistic) heuristics slow down bad plans but never outweigh true costs, on the other hand Inadmissible (pessimistic) heuristics break optimality by trapping good plans on the fringe. A heuristic can be said admissible if it is non-negative and less than the true cost of the nearest goal.
The second part of Day7 deepened discussion on admissible heuristics. The admissibility of heuristic depends on the arc cost. The proof of A* being optimal with use of admissible heuristics was discussed. Let us assume A to be optimal goal, n to be a ancestor of A, and B to be a suboptimal goal (goal with higher cost then A). Then it can be seen that due to admissible heuristics, even though B enters (enqueue) the fringe before A, it would leave (dequeue) the fringe after A. This is due to the facts that f(n) is less than or equal to f(A), and as f(A) is less than f(B), n expands before B leading to reach A.

Day8 focuses on A* search and its comparisons. UCS expands equally in all directions, whereas A* expands mainly towards the goal. But computation increases for A* as we need to compute the heuristic function along with UCS. The most widely used A* search finds its applications in video games, path problems, robot motion planning, machine learning, speech recognition and much more. The primary task for A* is to create admissible heuristics. Often, admissible heuristics are solutions to relaxed problems, where new actions are available. A possible way is to compute straight-line distance for using as heuristics. A straight line distance would always be less than or equal to original path.
The second part of Day8 continues the discussion with example of the 8-puzzle. A possible heuristic can be the no of tiles misplaced. The heuristic can be thought of as a relaxed problem as no restrictions on movements would be there to compute the no of tiles misplaced. Statistical comparisons show that why UCS would prove out to be a worst-complexity approach.