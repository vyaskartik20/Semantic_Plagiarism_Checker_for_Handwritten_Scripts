Now let's discuss our new topic i.e. propositional logic. We all come across the term logic in our day to day life i.e. we try to be logical. So here we will try to discuss how we can implement logic into the machine and discuss various components of logic and infer new things in a logical manner. There are different types of logic which are as follows- propositional logic, first order logic, Non-Monotonic logic, markov logic etc. but we will limit our discussion on first two logic i.e. propositional logic and first order logic. In logics there are different components which are syntax, semantics and inference procedure. Syntax of a logic is the way by which we convey our information to the system. It is similar to our programming syntax which we have. Semantics of logic means what's the meaning of our information. An example could be suppose our truth assignment v makes A and B true and C false. To evaluate (B→C)∨(A∧B) under v, note that the expression B→C comes out false and the expression A∧B comes out true. Since a disjunction “false or true” is true, the entire formula is true. So the variable is semantic of this logic. Now let's look at the interference procedure of logic which is also called reasoning which basically means that when a new situation is encountered we infer to our situation which we already know i.e. well we encounter some situation where we known what to do and when we encounter a similar situation where we don't know what to do we refer to a situation where we already know what to do. So we are trying to infer something which we already know . This logical inference is known as inference procedure. If we want the machine to think logically we, so we need to take into a language which a machine can understand, so a syntax is something representing knowledge and knowledge is something in the form of some sentence and the meaning of the sentence is semantics. So we will be using inference procedures that will help us in deriving new knowledge based on the knowledge the system already knows. 


Let's start with propositional logic. It is the basic building block of logic and simplest logic. Propositional logic is a logic which make use of prepositions i.e. sentences which use statements which are either true or false. Proposition is a statement to which we will assign a truth value i.e. a value which is either true or false in a given context. For example- I will get good marks in this notes assignment. So this statement may be true in some contexts whereas this statement may be false in some other contexts. So in any situation the sentence would be a proposition as it has its value either true or false. So for any statement for which we can assign a truth value is a preposition logic. There may be statements which are not proposition logic for example X+1=2, this is not a proposition as we can not assign truth value to it. So to denote a propositional logic we generally make use of some symbols like x,y,z etc which are similar to our variable which we use. The value of this symbol could be either true or false. We can use this symbol instead of a sentence. For example if we have a sentence that TA will give a good compliment on my notes, so we can denote this sentence by symbol A which has either value true or false. So we are using this symbol so that we can represent our logical statement into a simpler way. So as discussed each preposition is a sentence and a sentence can be divided into an atomic sentence or a complex sentence. An atomic sentence is one which has a value true or false or a symbol. A complex sentence is one which is made up of different sentences like and of 2 sentences i.e. sentence1^sentence2. We call a formula a well-formed formula(wff) if it follows the rules of forming a logical sentence. Example ~A^B is wff as it follows the grammar rules whereas ~A^=>B is not a wff as it has 2 logical operators one after the other without a symbol in between them. Consider a propositional logic PV(QVR~). Here the brackets control the order of operation i.e. first the expression inside brackets will be evaluated. So for a given expression of logic the evaluation preference will follow the precedence (highest to lowest) as ~,^,V,=>,<=>. Sometimes expressions may contain the same symbol where order of preference has not been decidable. For example A=>B=>C. Here the order of preference is a bit difficult to choose. Now lets us look at something called a truth table of a logical expression. It shows the truth value of every possible value of its constituent atomic propositions. Following is the Truth Table for A V B. 


Input A  |   input B  |  Output AVB
--------------------------------------------
T                 T                    T
T                 F                    T
F                 T                    T
F                 F                    F


An interpretation of a formula is a complete True/False assignment to all propositional symbols. So for the above example A V B we have 4 interpretations (TT,TF,FT,FF). The semantics of a sentence is the set of interpretations in which the sentence evaluates to True. So the semantics of the above example is TT,TF,FT. Model of a set of sentences is an interpretation in which all the sentences are true.


Truth table for the propositional logic-


P        Q        ~P        P^Q        PVQ        P=>Q        P<=>Q
------------------------------------------------------------------------
T        T        F        T        T        T        T
T        F        F        F        T        F        F
F        T        T        F        T        T        F
F        F        T        F        F        T        T


Some points regarding the => (implication sign). If the first symbol of P => Q is false i.e. P is false then the value of the expression would be True regardless of the value of Q. Example “5 is even implies 6 is odd” has the value true. So A => B “means” A is sufficient but not necessary to make B true. 


Let's look at some examples of propositional logic. EXP = (~P V (Q^R)) => Q.


P        Q        R        ~P        Q^R        ~P V ( Q ^ R )                EXP
-----------------------------------------------------------------------------------------
F        F        F         T          F                   T                F
F        F        T         T          F                T                F
F        T        F         T          F                T                T
F        T        T         T          T                T                T        
T        F        F         F          F                F                T        
T        F        T         F          F                F                T        
T        T        F         F          F                F                T
T        T        T         F          T                T                T


So the formula is true for certain values of P,Q,R and these values are known as satisfiable. The last 6 interpretations are true therefore the formula EXP is satisfiable. So a formula is satisfiable if the sentence of the formula is true for some interpretation. Whereas a formula is unsatisfiable if the expression has all its value of the expression as false under all interpretations.They are also known as inconsistent or contradictory.  For example A= P^~P, the value of the expression is always false irrespective of the value of P. If a sentence is always true for all interpretations. Then the sentence is known as valid or tautology. For example A=(P=>Q)V(P^~Q) is always true for all the possible values of P and Q.


P        Q        P=>Q                P^~Q                A
------------------------------------------------------------------
F        F        T                F                T
F        T        T                F                T
T        F        F                T                T
T        T        T                F                T


Now let's discuss what is converse of implication. As the same suggests for a given expression P->Q, converse of it would be Q->P. For example “If it rains today, then I will go shopping” its converse would be “I will go shopping, then it will rain today” The values of the truth table would be different as their meanings are different. Now let's see what is the inverse of implication P->Q. The inverse of P->Q would be ~P->~Q. For example “If it rains today, then I will go shopping” its inverse would be “If it does not rain today, then I will not go shopping”. Let's look at the next term contrapositive of implication. The contrapositive of P->Q would be ~Q->~P. For example “If it rains today, then I will go shopping” its inverse would be “If I will not go shopping, then it does not rain today”. Note that a conditional sentence i.e P->Q and its contrapositive i.e. ~Q->~P have the same truth values. Similarly inverse and converse have the same truth value. 


Let's look at the new term i.e. knowledge base(KB), it is basically a single sentence which is combined together to form a single sentence of multiple sentences. For example, we have two sentences in symbolic form as ~P and ( Q ^ R ). We combine this two together to form a single sentences i.e.~P ^ ( Q ^ R ). So a model of KB is an interpretation in which all sentences in KB are true. 


Now let's look at the next terminology i.e. entailment. We say that some formula F entails an interpretation if that formula F evaluates to be true under the interpretation I. we write this statement as  I |= F if F evaluates to true under I and I |≠ F if F evaluates to false under I. There is a theorem on entailment which states that I |= F if and only if I |≠ ~F. There is something called as deduction theorem which states that  A |= B if and only if A=>B is valid i.e. it is always true.  Lets understand entailment using an example. Consider a formula F: P^Q->~PV~Q. Let I1 be the interpretation such that [ P->true,Q->false ]. So the formula F comes out to be true so we can say that I1 entails F i.e. I1 |= F. Let's consider another example I2 which has an interpretation such that [ P->true,Q->true ]. So the formula F comes out to be false, so we can say that I2 does not entails F i.e. I1 |≠ F. A formula F is satisfiable iff there exists interpretation I s.t. I |= F and F is valid iff for all interpretation I, I |= F And F is unsatisfiable iff for all interpretations I, I |≠ F. F is contingent if it is satisfiable but not valid. Let's look into another concept i.e. equivalence . two formulas F1 and F2 are equivalent if they have the same truth value for every interpretation e.g. pVp and p. Some laws of equivalence- laws of double negation: ~~A ≡ A, Identity law: A^T ≡ A and AVF ≡ A, domination law AVT ≡ T and A^F ≡ F, idempotent laws: AVA ≡ A, absorption law: A^(AVB) ≡ A, commutative law A V B=B V A, distributivity law AV(B^C) ≡ (AVB)^(AVC) and De-Morgan’s laws ~(AVB) ≡ (~p^~q) and ~(A^B) ≡ (~pV~q). So when is this equivalence formula useful? These equivalence relations and laws are useful to reduce a propositional expression or to equate two propositional logic for being equivalent. For example ~(pV(~p^q)) and ~p^~q are equivalent as the first formula can be reduced to the second using the above few laws. So now let's see how to derive a propositional logic from the sentences given. Consider an example of 3 statements: if ram drives fast, he will get in an accident, ram did not get in an accident, therefore ram did not drive fast. Let f be the proposition for “ram drives fast” and t be the proposition for “ram got in an accident”. So the knowledge base would be ((f->t)^~t)->~f. So we need to prove this argument as valid i.e. it is true for all values of interpretation. So we can do it in either two ways: either use a truth table to show that this formula turns out to be true for all interpretations or we can reduce this argument into a true argument using the various laws discussed earlier. So similarly we can also use this to prove that certain statements reduce into another statement. 


Lets see other things soundness and completeness. Soundness says that any well formed formula that follows deductively from a set of axioms KB is valid i.e. true in all models and other thing to discuss is completeness which says that all valid sentences i.e. true in all models of KB can be proved from KB and hence are theorems. Lets look at our first method i.e. inference by enumeration it basically checks for inference using the truth table. So lets see an example in which we are given two KB as P: AVC and Q: BV~C and we need to check for X=AVB. So the query would be KB |= X. so all the logically distinct cases must be checked to prove that a sentence can be derived from KB. KB= PVQ so the following is the truth table for KB


A        B        C        P        Q        KB        X
--------------------------------------------------------
F        F        F        F        T        F        F
F        F        T        T        F        F        F
F        T        F        F        T        F        F
F        T        T        T        T        T        T
T        F        F        T        T        T        T
T        F        T        T        F        F        F
T        T        F        T        T        T        T
T        T        T        T        T        T        T


Rows where all of the sentences in KB are true are the models of KB. X is entailed by KB if all models of KB are models of X i.e. all rows where KB is true, X is also true.  In other words: KB=>X is valid. So we have noticed here that it enumerates all possibilities. So the computation time is of the order of exponential time and since it computes all values therefore this procedure is too slow. Now let's look at another method i.e. natural deduction using sound inference rules. So here we define a more efficient algorithm that enumerates and deduces new sentences that are true given the initial set of sentences in KB, plus uses all logical equivalences. There are rules for sound inference let's look into them modus ponens which states that given two propositions which hold i.e. A=>B and A hold then we can say that B also holds and the other rule is And-elimination where if A^B holds then A will also hold. So what we are trying to get is that we are logically following something from what is given to us. So we fill proof in a sequence of inference steps that leads from A(KB) to B(query). For example given certain KB elements (P^Q)=>R, (S^T)->Q,S,T,P and query R. we need to prove R from KB. we select S and T do a conjunction and with this conjunction we again do a conjunction from (S^T)->Q which reduces to Q and doing conjunction with P and P^Q)=>R the statement reduces to R and hence proves the query. Deriving a new sentence and adding it to KB does not affect what can be entailed from the original KB. hence we can incrementally add new true sentences that are derived in any order. Once something is proved true, it will remain true. 


Consider an example with given KB as {i}PetOfRoommateIsABird => PetOfRoommateCanFly, {ii}PetOfRoommateIsAPenguin => PetOfRoommateIsABird,{iii} PetOfRoommateIsAPenguin => (NOT)PetOfRoommateCanFly, {iv}PetOfRoommateIsABird. By taking modus ponens on iv and and ii we get {v}PetOfRoommateIsABird and by taking modus ponens on v and and i we get {vi}PetOfRoommateCanFly and by taking modus ponens on iv and and iii we get {vii}(NOT)PetOfRoommateCanFly and we can write the statement 6 equivalent to (NOT)PetOfRoommateCanFly => False. So we get a {ix}false statement after taking modus ponens on vii and viii. And since it is false we can derive anything from it. Like given a statement {x}False => he is a good boy so modus ponens on ix and x will give a statement he is a good boy. Hence we have proved anything from the false statement. So this is caused due to inconsistency in our knowledge base. So in such condition we may not make a sure comment on what we should be sure of. 


Let's look at a new rule for inference i.e. resolution rule of inference. Assume that we are given three resolutions A,B,C and given 2 sentences in our KB A V B and ~B V C and given a third sentence A V C which is inferred from the given 2 sentences. So given any two sentences(we will say them as a clause) where one contains some symbol and the other sentence contain the complement of symbol. So we merge this sentence with removing those negation symbols. So if doing so we are left with nothing then we can say that it's an empty clause or False and in case we obtain certain things then we say that it's a resolvent clause. 


Let's look at another method which is called resolution refutation which shows that a KB is entailed to A by proving that KB ^ ~A is false for all inference. So for doing this we need to convert it into special forms. We follow the following steps to do so: {i} Add negation of query to KB,{ii} pick 2 sentences that havent been used before and can be used with the resolution rule of inference,{iii} if none, halt and answer that the query is not entailed by KB, {iv} compute resolvent and add it to KB, {v} if KB is false then we halt and answer that the query is entailed by KB or if the KB is true then we goto procedure {ii} and repeat. So doing so we need to convert all statements into conjunctive normal form (CNF). It is basically a statement which contains only OR’s of the sentences. We replace all biconditional statement as follows A ⇔ B ≡ (A => B)^(B => A), and replace all implication symbol as A => B ≡ ~A V B. move all the negations invert i.e. ~(~A) ≡ A, apply destibitivity of V over ^:  A^(BVC) ≡ (A^B)V(A^C) so this are some steps we need to follow which translate or convert out sentences into a CNF formula. Lets understand using a example in which we are given a formula A ⇔ (B V C). first we reduce the biconditional statement so the formula is reduces to (A => (BVC))^((BVC) => A) then reduce the implication and the formula reduces to (~AVBVC)^(~(BVC)VA) on further reduction it reduces to (~AVBVC)^(~BVA)^(~CVA) and each elements of the CNF is called a clause. 


Let's look at some steps for resolution refutations. So given a Kb and a query B we add ~B to KB and convert the sentence into CNF. then we show that this leads to False. For example given KB as A ⇔ (BVC) and ~A and the query as ~B then we add ~B to Kb and convert to CNF: [i]~AVBVC, [ii]~BVA,[iii] ~CVA,[iv]~A,[v]B so with the help of this statement we reduce it to false. So by taking CNF of [i],[ii,][iii,][iv] the statement reduces to False. So now let's discuss the efficiency of resolution refutation algorithms. Sometimes in the worst case the run time is exponential. So to reduce the time complexity we do factoring i.e if a new clause contains duplicates of the sam symbol delete the duplicates i.e. P V R V P V T ≡ R V P V T and the other method to reduce the time complexity is that if a clause contains a symbol and its complement the clause is a tautology and is useless it can be thrown away. I.e. if two statements are (!AVBVC) and (~BVA) which reduces to BVCV~B so we can throw away these two sentences.


There are some weaknesses like We cannot represent relations like ALL, some, or none with propositional logic. Example: All the girls are intelligent and Some apples are sweet. Propositional logic has limited expressive power.In propositional logic, we cannot describe statements in terms of their properties or logical relationships. 


We can use propositional logic to validate the form of an argument that takes us from premises to a conclusion. We cannot use propositional logic to establish the truth of a proposition that isn't given as a premise, or which can't be inferred by the laws of inference. In particular, we cannot use propositional logic to reason about propositions that obey laws (such as arithmetic laws) beyond the logical inference system. Limits of Propositional Logic in Reasoning about Relations example: Consider the following "if" clause: if (a < b) OR ( (a >= b) AND (c = d) ) We would like to know if we can simplify this "if" clause as follows: if (a < b) OR (c = d) More formally, we would like to establish the following equivalence as a tautology, where we have rewritten the relation expressions in the "if" clauses as propositional variables: p1 OR (p2 AND p3) == p1 OR p3. We can establish whether or not this equivalence is a tautology by showing the truth table for the expression:
     p1  p2  p3   e1   e2   e1==e2
       0   0   0    0    0     1
       0   0   1    0    1     0
       0   1   0    0    0     1
       0   1   1    1    1     1
       1   0   0    1    1     1
       1   0   1    1    1     1
       1   1   0    1    1     1
       1   1   1    1    1     1
This is NOT a tautology, even though it appears valid to substitute the simpler boolean expression for the more complex one. The problem is that, in performing the substitution, we used facts about integers to draw inferences about the propositions (such as p1 -> NOT p2) that we could not deduce using propositional logic only.
Let's start a new topic First order logic. So as we discussed earlier about propositional logic which has some limitation over description. It would be difficult to solve when we have a large number of sentences so we will try to encode these things into a simpler way. So the expressive power of propositional logic is limited so we use first order logic which has more expressive possibilities. In propositional logic we can make an assumption like if we have a statement like if ram drives fast he will get an accident and  if sham drives fast he will get an accident. In first order logic we can write these two statements as if anyone drives fast he will meet with an accident. Let's look at some basic building blocks of First order logic(FOL) which comprises constraints, variables and predicates. Constraints refers to specific objects in the sentence like ram, sham, CS311 etc. Variables are used to subset the elements from the domain. Predicates describe properties of objects or relationships between objects. Examples ishappy, love, moves etc. predicates can be applied to both constants and variables like ishappy(George),x>3 etc. a predicate P(x) is true or false depending on whether property P holds x example ishappy(rohan) is true if roha  is happy but is false otherwise. Let’s see some examples of predicates considering Ivan jumped far. In this example, “Ivan” is the subject and “jumped far” is the predicate. Here, far is an adverb to describe how Ivan jumped. Ivan jumped higher than Andrew. In this example, “jumped higher than Andrew” is the predicate. Ivan read a book to the students. In this example, the predicate of the sentence is “read a book to the students.”


Let's look at the formulas in FOL. formulas are formed using the predicates and logica; connectives. Example even(x)Vodd(x) is a formula. Semantics of FOL. in propositional logic the truth value of the formula depends on the assignment of values to the variables, in FOL the truth value of a formula depends on the interpretation of predicate symbols and variables over some domain D. consider some formula ~P(x) which has interpretation as for some value of x we have value as false whereas for other as true. Let's look at another topic that is quantifiers. predicates are used alongside quantifiers to express the extent to which a predicate is true over a range of elements. Using quantifiers to create such propositions. Quantifiers allow us to talk about all objects or the existence of some objects. There are two types of quantification- 1. Universal Quantification- Mathematical statements sometimes assert that a property is true for all the values of a variable in a particular domain, called the domain of discourse. Such a statement is expressed using universal quantification. The universal quantification of P(X)for a particular domain is the proposition that asserts that P(X) is true for all values of X in this domain. Example: Let P(X)be the statement “X+2 > X“. What is the truth value of the statement for all x in P(X)? Answer to it would be as X+2 is greater than X for any real number, so P(X) is equivalent to true for all X. Another type of quantifier is 2. Existential Quantification- Some mathematical statements assert that there is an element with a certain property. Such statements are expressed by existential quantification. Existential quantification can be used to form a proposition that is true if and only if P(X) is true for at least one value of x in the domain. Formally, The existential quantification of P(X) is the statement  "There exists an element x in the domain such that P(X)". The notation ∃P(X) denotes the existential quantification of P(X). Here ∃ is called the existential quantifier. In FOl the domain should be non-empty. Consider an example in which we consider the domain of integers and predicates even(x) and div4(x) which represent if x is divisible by 4. Given the conditions let's try to compute the truth values of ∃x(~div4(x)^even(x)) is true for all values of x. The truth value for ∃x(~div4(x)->even(x)) will be true. 


Now let's learn how we translate english into quantified formulas. Assuming freshman(X) means “x is a freshman” and inCS311(x) “x is taking CS311” let's try to express “someone in CS311 is a freshman.” so we can write this as ∃x inCS311(x)^freshman(x). Let's look at DeMorgans law for quantifiers. For propositional logic we have seen use of De Morgan's laws to simplify the denial of ~(P⇒Q):⇔~(~P∨Q)= (~~P)∧(~Q) = P∧~Q so the denial of ~(P⇒Q) is P∧¬Q. In other words, it is not the case that P implies Q if and only if P is true and Q is false. Of course, this agrees with the truth table that we have already seen. There are other versions of De Morgan's laws for quantifiers: ~∀xP(x) = ∃x~P(x) and ~∃xP(x) = ∀x~P(x). Now lets try to express “ran in CS311 is a freshman ” as ~∃x(inCS311(x)^freshman(x)). So we can have different formula for same sentence like in propositional logic we have p->q equivalent to ~pVQ.


Let's look at other quantifiers i.e nested quantifiers. Nested quantifiers are quantifiers that occur within the scope of other quantifiers. Example: ∀x∃yP(x, y). Lets consider an example and under it more clearly. Let L(x, y) be the statement “x loves y,” where the domain for both x and y consists of all people in the world. We use quantifiers to express each of these statements. [i] Everybody loves Jerry. ≡ ∀xL(x, Jerry) [ii] Everybody loves somebody.  ≡ ∀x∃yL(x, y) [iii] There is somebody whom everybody loves.≡ ∃y∀xL(x, y) [iv] Nobody loves everybody.≡ ∀x∃y¬L(x, y) or ¬∃x∀yL(x, y) [v] Everyone loves himself or herself ≡ ∀xL(x, x). 


Now let's look at another topic i.e. well formed formula (wff). A free variable that isn't bound by a quantifier ∃y likes(x,y) here x is free and y is bound. A wff is a sentence where all the variables are quantified. So similar to propositional logic we have equivalence in FOL as well. Two formulas in FOL is said to be equivalent if F1 <-> F2 is valid i.e. it is always true. In propositional logic we proved two formulas as equivalent using the truth table but in the case of FOL it is not possible to make a full table so we try to rewrite one formula into another. Modus ponens applicable to both propositional logic and first-order logic. P implies Q and P is asserted to be true, therefore Q must be True. The second rule for inference is modus tollens. If we have A->B and ~B is equal to ~A. Example Q(x) and ~P(X)->~Q(X). There is another rule i.e. Hypothetical Syllogism(HS) which says that implication is transitive i.e.A->B and B->C then A->C. 


Let's look into another concept i.e. or introduction and elimination. Or introduction states as follows given A then it implies that AVB. Example: Ram is a man therefore either ram is a man or there are red elephants on the moon. Or elimination states that given AVB and ~B it implies A. example: it is either a dog or a cat. It is not a dog. Therefore it must be a cat. And introduction and elimination. Given A and B hold then  will also hold A^B. Elimination could be given A^B hold then A will hold. Next discussion is on a resolution rule which states that given a truth statement as AVB and ~AVC it implies that BVC. to see why this is correct is to observe A is either true or false. Then suppose A is true then ~A will  be false. Therefore by 2nd hypothesis we say that C must be true. Suppose A is false then by 1st hypothesis we say that B must be true. Therefore in any case B or C must be true which implies BVC. Encoding is similar to propositional logic. Additional inference rules for quantified formulas are as follows- these come in pairs for each quantifier. Ine is called generalization and the other is called instantiation. If we know that something is true for all members of a group we can conclude it is also true for a specific member of this group. This is known as universal instantiation. Example: No humans can fly. John Doe is human. Therefore John Doe can not fly. Nest is universal generalization which states that suppose we can prove a claim for an arbitrary element in the domain. Since we have made no assumption about this element proof should apply to all elements in the domain. I.e. if P(c) holds for any arbitrary element c of the universe, then we can conclude that ∀x P(x). Example: For every number x if x > 1, then x - 1 > 0. Also for every number x, x > 1. Hence we conclude that for every number x, x - 1 >0. In case of universal generalization we need to keep track that the given predicate should hold for all values of x. The next is existential instantiation. This rule says that if P holds for some element of the universe, then we can give that element a name such as c (or x, y, a etc). When selecting symbols, one must select them one at a time and must not use a symbol that has already been selected within the same reasoning/proof. Example: Consider the following argument: If you get 95 on the final exam for CS 398, then you get an A for the course. Someone, call him/her say c, gets 95 on the final exam. Therefore c gets an A for CS398. This argument uses Existential Instantiation. The next rule for inference of quantifiers is existential generalization which states that  if there is some element c in the universe that has the property P, then we can say that there exists something in the universe that has the property P. For example the statement "if everyone is happy then someone is happy" can be proven correct using this existential generalization rule. 


Till now we have seen rules for inference of FOl now we will look into them into further details. Every instantiation of a universally quantified sentence is entailed by it. Consider the previous example "No humans can fly. John Doe is human. Therefore John Doe can not fly." First let us express this using the following notation: F(x) means "x can fly." H(x) means "x is human." d is a symbol representing John Doe. Then the argument is [∀x [H(x) -> ~F(x)] ^ H(d) ] -> ~ F(d). So we can also convert formulas of first order logic into propositional interference by getting rid of the quantifications by adding sentences in our KB. lets look into Unification concept.  Unification is a process of making two different logical atomic expressions identical by finding a substitution. Unification depends on the substitution process. It takes two literals as input and makes them identical using substitution. Let A = King(x), B = King(John), Substitution θ = {John/x} is a unifier for these atoms and applying this substitution, and both expressions will be identical. The UNIFY algorithm is used for unification, which takes two atomic sentences and returns a unifier for those sentences (If any exist). Unification is a key component of all first-order inference algorithms. It returns fail if the expressions do not match with each other. The substitution variables are called Unifier. Let's look at the example of a knowledge base where we are given that “The law says that it is a crime for an american to sell weapons to hostile nations. The country Nono an enemy of America as some missiles and all of its missiles were sold to it by colonel west who is american” now we need to prove that colonel west is criminal. So we can say that it is a crime for an American to sell weapons to hostile nations: American(x) ∧ Weapon(y) ∧ Sells(x,y,z) ∧ Hostile(z) => Criminal(x).  The country Nono an enemy of America as some missiles, i.e., ∃x Owns(Nono,x) ∧ Missile(x): Owns(Nono,M1) and Missile(M1). all of its missiles were sold to it by colonel west who is american i.e. Missile(x) ∧ Owns(Nono,x) ⇒ Sells(West,x,Nono) Missiles are weapons: Missile(x) ⇒ Weapon(x) An enemy of America counts as "hostile“: Enemy(x,America) ⇒ Hostile(x) West, who is American i.e. American(West) The country Nono, an enemy of America … Enemy(Nono,America). Coming to forward chaining. Something we need to look into is that we always start with the formula which has single predicates with constant symbols and we look into our knowledge base and identify appropriate sentences that would have some relation with predicates. Like American(Robert). At the second step, we will see those facts which infer from available facts and with satisfied premises. Like American (p) does not satisfy premises, so it will not be added in the first iteration. Owns(A, T1) and Missile(T1) are added as they infer. Similarly we add other elements also similarly. Then we check if the substitution {p/Robert, q/T1, r/A} is satisfied , so we can add Criminal(Robert) which infers all the available facts. And hence we reached our goal statement. Similar to forward chaining we have another method i.e. backward chaining. In this we start from the goal state and move in the backward direction. Backward-chaining is based on modus ponens inference rule and the goal is broken into sub-goal or sub-goals to prove the facts true. At the first step, we will take the goal fact. And from the goal fact, we will infer other facts, and at last, we will prove those facts true. So our goal fact is "Robert is Criminal," so the following is the predicate of it. At the second step, we will infer other facts from the goal fact which satisfies the rules, the goal predicate Criminal (Robert) is present with substitution {Robert/P}. So we will add all the conjunctive facts below the first level and will replace p with Robert. Now we move to further steps i.e. we will extract further fact Missile(q) which infer from Weapon(q) and further we can infer facts Missile(T1) and Owns(A, T1) form Sells(Robert, T1, r) and we can infer the fact Enemy(A, America) from Hostile(A) and hence all the statements are proved true using backward chaining. Now let's look at another term i.e. resolution. Suppose given 2 formulas such one has some symbol and the other formula has the negation of this two and we resolve this two we remove this symbol. So by applying the unification formula we resolve the two formulas. For example: given two statements as ~rich(x) and unhappy(x) and rich(ken) after solution we will get unhappy(ken). Now let's try to convert statements into a CNF. Consider an example: Everyone who loves all animals is loved by someone ∀x [∀y Animal(y) => Loves(x,y)] => [∃y Loves(y,x)]. First we Eliminate biconditionals and implications and the given expression reduces to ∀x [~∀y ~Animal(y) ∨ Loves(x,y)] ∨ [∃y Loves(y,x)]. Now move the negation ~ inwards i.e. ∀x [∃y ~(~Animal(y) ∨ Loves(x,y))] ∨ [∃y Loves(y,x)] which further reduces to ∀x [∃y ~~Animal(y) ∧ ~Loves(x,y)] ∨ [∃y Loves(y,x)] which further reduces to ∀x [∃y Animal(y) ∧ ~Loves(x,y)] ∨ [∃y Loves(y,x)]. Now we Standardize variables: each quantifier should use a different one ∀x [∃y Animal(y) ^ ~Loves(x,y)] V [∃z Loves(z,x)]. Now we eliminate existential quantifiers and this process is known as skolemize. So for eliminating existential quantifiers each existential variable is replaced by a Skolem function of the enclosing universally quantified variables: ∀x [Animal(F(x)) ^ ~Loves(x,F(x))] ∨ Loves(G(x),x). Now drop universal quantifiers:  [Animal(F(x)) ^ ~Loves(x,F(x))] V Loves(G(x),x). And the final step is to distribute ^ over V:  [Animal(F(x)) V Loves(G(x),x)] ^ [~Loves(x,F(x)) V Loves(G(x),x)]. 




References -


1. http://ai.berkeley.edu/home.html.
2. Textbook referred  - Artificial Intelligence, A Modern Approach Third Edition by Stuart J. Russell and Peter Norvig.
3. Google meet lecture recordings of Artificial intelligence lecture (Course CS323 Artificial Intelligence September 2020 - November 2020) by Dr. Yashaswi Verma IIT Jodhpur.