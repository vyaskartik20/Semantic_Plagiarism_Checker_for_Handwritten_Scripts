B18CSE014
Ekagra Das
ONLY the lecture recordings was used for reference for these notes, with timestamps referring to relevant places in the recordings.

Lecture-01-2:
[00:00] What is unusual about futuristic Sci-Fi movies?
They are fictional, far from reality, the presence of self conscious AI in them.
Though we stil like them cause we want it to happen someday, and we usually try to escape from reality.
The main objective of AI is to make machines that can think like humans, act like humans and even be better than humans.
[05:46] In this course, being rational would be in the context of achieving a goal, and how it will affect the environment and accounting for relevant obstructions in the process. In the second half of the course, we will see how we can express it in some sort of value, ie in terms of utility. Utility here is a measure of the outcome.
[13:10] Imperfection of the brain - Though the human mind can make rational decisions, it isn't perfect. We can't break it down in terms of algorithms, but can only take inspiration from how it works, to create such algorithms, just how we took inspiration from birds to create airplanes.
[15:17] In the present times, basically AI can perform technical/objective tasks which can be performed using a set of instructions, but not tasks that makes use of emotions, or information that is very locally available.
[19:08] Some examples where machines are good - Speech related tasks like audio recognition or Text-to-speech synthesis, Language and image processing - tasks based on dialogues, or object and face recognition.
And coming to Robotics, physical tasks like walking, driving, painting and such are possible though we have to remember that reality is more difficult to control than in simulations.
[21:30] AI evolves with the help of logic and methods like Deduction, Constraint satisfaction and Satisfiability solvers.
And how does applied AI make decisions? By scheduling, route planning, medical diagnosis, web search engines, spam classifiers, automated help desks, fraud detection etc.
[22:54] What is a Rational Agent? It's an entity that perceives and acts, to maximize its expected utility.

Lecture-02-1
[05:00] Agents that Plan - Should be careful about their capabilities and how their surroundings will act in response to their actions.
Reflex Agents - These agents act based on what they see right now and they don't consider the consequences of their actions. They may or may not have 'memory' ie the ability to store data, like a past state.
[09:00] In Pac-Man, the ideal objective is to eat all the points as quickly as possible. So we start randomly and eat the dots, and keep on exploring the environment. Or we can explore the environment as much as we can and on the basis of that plan the path.
So basically these agents try to understand the consequences of the resulting actions, and decide and act on their path.
[13:20] Optimal vs Complete planning - Optimal is the best plan, and complete will make sure you reach the goal although it may not be the best plan, like in Pac-Man, going directly to end location, may fulfill the goal, but not collect the max possible dots, hence it's not the best.
[19:17] Search Problem - We define it using a state space(possible states), a successor function(contains primarily the actions and costs, talks about how the agent moves in a given state space), a start state and a goal test(when all the dots have been eaten - multiple states possible in this context).

Lecture-02-2
[01:11] In Search Problems, we try to model out the environment to discretize different values, to plan and test upon.
Example of Travelling in Romania - For the shortest path, the solution is simply the smallest path between start state and goal test.
[05:32] Pac-Man has two problems in it, one is a Pathing problem and the other is trying to Eat all the dots problem. Both the problems, will have different State space, Actions, Successor function and Goal test.
Pathing Problem - States:(x,y) coordinate, Actions:NSEW, Successor function:Update location, Goal Test: x,y coordinates = END
Eating all the Dots Problem - States:(x,y) coordinate and a dot boolean, Actions:NSEW, Successor function: Update location and the dot boolean, Goal Test: Dots are all false.
We can see that the Goal Test may not always correspond to a state or a condition, hence its a test and around it, the solution and objective revolves.
[13:22] To find the total number of World States in the given example - (Agent Positions) x (2(if dot eaten or not)^Food count) x (Ghost positions^2(if ghost there or not)) x DirectionTheAgentIsFacing(4 in this case-NSEW)
In the pathing problem, we are only concerned with the position of the agent, hence the total states in that context is 120, that is the total number of positions of the agent.
In the dot eating problem, we are concerned with the position of agent aswell as if the dots are eaten or not, hence the total number number of states is TotalNumberOfPositionsofAgent x 2(if eaten or not)^Food count.
[26:11] Problem - Eating all dots + keeping the ghosts scared - The state space will have to keep in account, the total agent positions, for each dot if its eaten or not, for each power pellet if its eaten or not, and the remaining scared time of the ghosts.
[29:00] State space graph - We can basically represent the whole state space in the form of a graph, where each node(state) may branch into further nodes(states), cause of the actions they perform(NSEW), and each state will occur only once in the graph. Upon considering all the possible states we will finally get a graph, but it will require a lot of memory which isn't feasible.

Lecture-03-1
[03:11] A Search Tree is different from a Search graph, it may not contain all the states, but it contains the paths for a sequence of states, where each child node corresponds to the succeeding state from a node. The starting state is the root node here and usually, we don't build the whole tree, but only search upto a few levels, cause it's not feasible memory-wise.
[04:35] In State space graphs, we show all the possible states and also show the transitions between them and in Search tree, we show the paths, between any two particular states and repitions are possible, and with this we can trace the path easily between two states. In State space graph, if we have to check for a path, we have to put conditions so that we don't get stuck in loops, whereas in the tree, we just have to check that we don't repeat any vertix.
[09:08] When we search with a Search tree, we can see the locations where we can go from our starting location/node, and these locations will become the child nodes of this starting root node. And we further expand these nodes, and which node is to be expanded is decided by the algorithm that we use. We also keep track of possible paths to consider, using a fringe, which is a queue containing the child nodes, from which the next node to be expanded is chosen by the algorithm, and the already expanded parent node is moved from the fringe node to an older queue containing all the expanded parent nodes, and is replaced by its child nodes in the fringe. Basically a fringe is a queue that is visible to the algorithm, but not yet expanded.
[15:00] General Tree Search(PseudoCode) - Its basically a function which we perform with the knowledge of what the problem is, and what algorithm/strategy we will be using. It will either return us the path or tell us that no such path that satisfy it exist. We start it by using the initial state of the problem, and then perform a loop where we check for available candidates(nodes) to expand, if none exist, then no solution. After that we choose a leaf node from the Fringe according to our algorithm/strategy. If we reach the goal test(state), we return the solution, else we further expand the node and add the resulting child nodes to the tree, and then repeat the loop.
[21:30] Depth-First-Search - We perform DFS on the tree, and the path that we come up with is indeed a solution, hence its a complete path. But we can't comment if its the optimal path or not.

Lecture-03-2
[00:39] Search Algorithm Properties - For a tree of 'b' as branching factor and 'm' as the maximum depth
In DFS
Time complexity will be O(b^m), which will be the worst case scenario (1,b^2,b^3,...,b^m).
Space complexity will be O(bm) because in worst case scenario, the longest path will go upto 'm', and for each node that we expand, it will have 'b-1' sibling nodes that are yet to be explored, which we have to keep an account of, so that the algorithm can know what nodes are next to check or been checked, hence O(m+(b-1)m) = O(bm).
[22:36]In BFS
The Time complexity will be O(b^s) where 's' is the level of the goal.
The Space complexity will be O(b^s) because while scanning a specific level we have to keep all the nodes in our memory to expand later.
And its a complete search since s must be finite, for the solution to exist.
And it's optimal in the scenario when the costs are 1 and by cost it means the cost of moving from one node to the next node.
[25:46]When to prefer DFS or BFS? BFS when the depth is shallow, and DFS when the depth of goal is deep.

Lecutre-04-1
[00:00]General tree structure - First child, next sibling links
[08:59]DFS step by step -
Start from root node, then look at children, and we have to keep track of these children. So we push S(parent node)->d, s->e, s->p in the fringe stack.
Say we pick d from this stack, so we take out s->d from the fringe stack. Now from D, we can goto b,c,e, so all these 3 parts we need to push in the stack, so now our stack will contain 3 more entries, ie s->d->b, s->d->c, s->d->e. From this we pick s->d->b, which we will say is on the top of the stack.
Since b has 1 child node a, we push s->d->b->a in this stack. Now we are done with this part, but still goal not reached, but since there are more options left to explore, we take out the next option s-d-c, and similarly, s-d-c-a. Now since we still havent reached the goal, we pop out the next option from our fringe stack. 
Now we have 2 children from e, so we need to push both of them to the fringe stack, ie s-d-e-h and s-d-e-r, in our stack. Now we pop the first one giving us two options. Now we put both of them too in the stack, and pop the first one and finally get s-e-h-p-q but still goal not reached. Similarly we keep expanding based on the options in our fringe.
Eventually we get s-d-e-r-f-g. 
When we get this, we put it into the fringe stack, but DONT STOP here. 
We only stop, when we push on the fringe and the last node of that corresponds to the goal.
[22:39]Iterative Deepening - Basically we use the DFS's space advantage alongwith BFS's time advantage.
We run DFS with depth limit from 1, and increase it 1 by 1. And it is pretty useful because most of the solutions are usually found in the shallow levels.

Lecture-04-2
[00:00] Cost Sensitive Search - 
BFS doesn't find the least cost path, but the shortest path in number of actions.
Uniform Cost Search - In this we keep a track of the cost too.
We first start with the choice between s-d (3), s-e (9), s-p (1). We choose s-p cause of the least cost, then we further check s-p-q giving value 16 in total.
Then check between s-d (3), s-e (9), s-p-q (16). We choose s-d (3).
Then between s-d-b (4), s-d-c (11), s-d-e (5), s-e(9), s-p-q (16), we choose s-d-b(4).
After choosing it, we go to the next node from b, ie a giving s-d-b-a(6).
Now in the fringe we choose from sdba 6, sdc 11, sdeh 13, sder 7, se 9, spq 16.
We choose the least one that is, sdba 6, and realize that a is not our goal, so we drop it.
Then we check the next least, sder 7, we get sderf 8, and upon expanding that since its still the least, we get sderfc 11 and sderfg 10.
We find that sderfg 10, has G that is our goal, but we will NOT STOP here, and continue to explore. 
After this we check and find that se 9, is the least one, giving two more options seh 17, ser 11. Now after checking all options in the fringe, we find that sderfg 10, is the least cost, so we take it out of the fringe and it becomes our solution.
Basically we don't immediately stop after encountering G, cause there is a possibility of another path having lesser value we can encounter later upon exploring.

Lecture-05-1
[00:47] Uniform Cost Search Properties -
Let cumulitive cost of optimal solution is C*, and the minimum cost of each arc is let E. Then at each step we would be reducing the cost by a factor of E and can say that we'll require C*/E steps or depth in the search tree.
Time complexity is O(b^(C*/E)) same as in BFS.
Space complexity is O(b^(C*/E)) same as in BFS.
For it to be complete search, the solution should have a finite cost and the minimum arc cost should be positive.
This algorithm looks at all possible paths, and since we explored all possible paths we can be sure it's optimal.
[14:20] The solution that we get is optimal as we explore on the basis of increasing costs. But the disadvantage is that we explore in every direction. Like if we are at a position and we have to reach another position, we simulate all possible moves in all directions to move from current position to goal, and we don't use any possible information we have about the goal in doing this. Hence this blind search is an uninformed search algorithm.
[17:10] The One Queue - In all these searches we use some form of a queue. Whenever we expand a subplan, we get new subplans and put them into a fringe. And how we operate on this queue, depends on the algorithm we are using.
In case of DFS, we use the LIFO method and in BFS, we use FIFO. In practice, we implement a generalized version of priority queue and we assign priority on the base of the algorithm being used. In case of UCS, priority corresponds to cumulitive cost. In case of BFS and DFS, we assign a timestamp with each subplan. In DFS - LIFO, pick the subplan of the least timestamp value and for BFS, the maximum timestamp value.
[20:00] The agents, create a virtual model of the world, and run their simulations and tests in it, instead of the actual world. The search operations occur in these models, and are as useful as the more detailed the model is. Like for example, in google maps, it may contain a path traversing through private property which leads to trespassing, if we use them. But the agent may choose this path as it doesnt have necessary information and things may go wrong.

Lecture-05-2
[07:10] Pancake problem - Can only flip a specific number of pancakes from the top, not from middle.
We can consider each configuration as a state in the search graph, and start from the start state, and each flip will act as an action that changes the state.
And for cost, it can simply be the number of pancakes flipped in that specific action.
We can see this problem, like N integers, and we have to arrange these in ascending order by flipping them. Another thing to note is that the same configuration/state can be repeated by flipping the same way.
[19:30] The Tree Search function to be used here is the same as Lecture 03-1 General Tree search function.
The Videos of Demo Counters UCS shows how searching in all directions is a waste, instead of searching in a specific direction with the knowledge of location of the goal.

Lecture-06-1
[10:00] Informed Search -
Heuristic Function - Takes a state as an input and returns a value that shows how far we are from a goal/goal state in Manhattan distance, Euclidean distance etc. Unlike the generic search tree/ search graph used to search , in Heuristic approach, a single Heuristic may not work every time.
[18:39] In the map, the Heuristic value provides guidance towards the goal, it may not provide the optimal path, but just shows a value of how close you are to the goal, to help us choose which place to go first based on which one has a lower Heuristic value, although it's not optimal.
In the Pancake problem, the size of the largest pancake that is out of place could be the Heuristic value.
[25:20] Greedy Search - We try to get the best that we can get out of a current state, but we may not get the optimal solution in the long run. We keep choosing the child node, with the least Heuristic value greedily, till we reach the Goal state.

Lecture-06-2
[02:00] The strategy in Greedy Search is to keep expanding the node that we think is closest to the goal state with the help of the Euristic Value.
Although most of the times we can notice that the best first greedy step, may lead us to the wrong path/goal.
The Worst Case scenario, will be a badly guided path, where we may end up doing more work than in BFS or DFS
In the PacMan small maze, the agent, will choose the direction with the least Euristic value, then as it reaches a dead end, it will go for the second least possible value path, when that also reaches a dead end, it will double backtrack and go for the third least possible value path, and eventually reach the goal state. This is an example of a Reflex agent.
[21:30] A* Search - Combining UCS and Greedy
A function g(n) would be showing the Uniform cost on the basis of path cost.
A function h(n) would be showing the Greedy value on the basis of proximity to goal.
A* Search orders by the sum of g(n)+h(n)=f(n)

Lecture-07-1
[03:40] When should we stop A*? When we dequeue a goal.
In the example, the starting position is S, and we have to decide between A and B to goto, we choose B, cause it has a lower h value and both have same g value, so B's h+g value is lower hence we chose it. After that we goto G from B, hence A-B-G, making up 5 cost value.
Then we dequeue the goal and stop. This however isn't optimal as SAG has 4 cost value.
[14:05] In this value, at S, we have two choices between SA and SG, SA has a cost value of 1+6=7, and SG has a cost value of 5+0=5, so we traverse via SG, then dequeue the goal and end it. But this isn't optimal as SAG, has a total cost of 4. Hence A* is not optimal in this case.
In both the examples the Actual 'bad' goal cost < estimated 'good' goal cost.
[24:00] There are two types of Heuristics, Inadmissible(pessimistic) and Admissible(optimistic).
Admissible heuristics never overestimate the cost of reaching the goal and Inadmissible heuristics try to break the optimality, and traps good plans in the fringe.
Admissible heuristic - 0<= h(n) <= h*(n), 
Where h*(n) is the true cost to a nearest goal.

Lecture-07-2
In the Pancake problem, the Heuristic value, is the number of the largest pancake, which is still out of their place.
We'll the call the Heuristic function as admissible Heuristic if the Heuristic value is less than or equal to the true minimum cost.
[21:00] Optimality of A* Tree Search -
Let a tree, where A is the optimal goal node and B is the suboptimal goal node, and h is an admissible Heuristic.
Since B is suboptimal the cost of RootNode-B is more than RootNode-A.
In order to claim that A* search is optimal, we have to prove that plan leading to A will come out of the fringe before the plan leading to B
Proof - 
Let path leading to B be in the fringe
Let Some ancestor N, of A is in the fringe aswell (A could be too)
The claim is that N will be expanded, before we expand B.
(1) F(N) is less or equal to F(A)
That is F(N) = G(N) + H(N) [definition of the F cost], and F(N)<=G(A) [ because H is admissible Heuristic ]
And G(n)+H(n) <=G(A), and since H=0, cause it's at the goal
G(A)=F(A)
(2) F(A) < F(B)
We know that G(A) < G(B), cause B is a suboptimal goal.
And F(A) < F(B), since Euristic value H=0 at the goal.
[27:00](3) N expands before B - because of (1) and (2), and when N and B are both in the fringe and we compare their F() values, the N will have less value and will come out of the fringe before B.
From here we can deduce that path to A will expand before path to B, hence we can conclude that the A* search here is optimal.

Lecture-08-1
[00:45] Properties of A* Search -
We compare UCS and A*, we find that A* is expected to search less as compared to UCS.
In case of UCS we don't make use of a Heuristic function and simply rely on search, and in A* we make use of Heuristic function, and if the Heuristic is admissible, it will show Optimality, however it increases computation because we are doing additional compution in respect to a given node.
[06:30]UCS vs A* Countours -
UCS expands, in all the directions equally, whereas A* only expands towards the goal, ensuring optimality.
A Comparison between Greedy-UCS-A* in PacMan - we see that in Greedy, the agent rushes in the direction with the least distance, and the solution isn't optimal, whereas in UCS, it searches almost everything, leading to a nonoptimal solution and in A* it doesn't search everything as in UCS, but not tunnel vision as much as in Greedy, and give us an optimal solution.
[13:40] A* Applications - 
Plenty of applications, like in games, pathing, routing, planning, motion planning, language analysis, machine translation and speech recognition.
[22:00] Creating Admissible Heuristics -
Let's say the Euristic that is in use is very fast, but is inadmissible, and the admissible Euristic performs alot of computation and is slow. Inadmissible Euristic is mostly useful when we want a solution regardless if its optimal or not, but we want it fast. 
Admissible heuristics, are often solutions to 'relaxed' problems. These are where new actions are available, and we drop constraints.

Lecture-08-2
[05:20] Example - 8 Puzzle -
In this 3x3 board, there is a Start State, the action is simply moving a the empty block NSEW, the goal state, and each possible configuration belongs in the state space.
For the given Start state, we can see that there are 4 successor states of it, but for a different configuration we can see that it can have 2 successor states.
A reasonable Euristic for this puzzle, would be the total number of tiles that are misplaced in each configuration from the goal state. This is a relaxed problem, as we check the Heuristic value without taking the constraints in consideration. Hence it's an admissible Heuristic.










# Tree Search: Extra work!
-> If the algorithm we are using failed to detect the repeated states or nodes then the search tree will end up performing steps no longer needed.
   For example, consider a graph of 4 states, from each state there are two edges directly connected to second state, from second two edges to forth state and so on.
   If we construct tree for this graph then each node will have two same child and so on. So agent will end up exploring same node more than once. So we should keep track of expanded 
   nodes.

# Graph search
-> 1) We should never expand a state or node twice. For that keep track of visited nodes.
   2) Implementation :-
      2.1) With tree search also create a set of expanded states("closed set") and update it after each expansion of states.
      2.2) After taking out a node from the fringe, expand this node if it has not expanded before or this state is not in the set.
      2.3) If node is in set, skip it, else expand it.
   3) Store the closed set as a 'set' and not a 'list' because search time of set is less than that of a list.  
   4) Optimality :- We may not get the optimal solution but we will get a sub-optimal solution because of above constraints.
                    For example, lets perform A* graph search on given state space graph,
           
                            h=4     
                   --1----->(A)---1-------                                              S(0+2)
                  |                      |                                             /      \
                  |                      v                                            /        \
              h=2(S)                 h=1(C)----3---->(G)h=0                       A(1+4)       B(1+1)                
                  |                      ^                                          |            |
                  |                      |                                          |            |
                   ---1---->(B)---1-------                                        C(2+1)       C(3+1)
                            h=1                                                     |            |
                                                                                    |            |
                     Fig. State space graph                                       G(5+0)       G(6+0)
                  
                                                                                     Fig. Search tree {node(g(n)+h(n))}
           
                If we look at the search tree for above state space graph and if we search the tree with closed set, then start from S, least f(n) value is of B so expand B, then expand 
                C and then we will reach to Goal state(G).
                But this is a sub-optimal solution whereas optimal solution is when we expand S, then A, then C, and reached to G.

# Consistency of Heuristics
-> 1) Estimated heuristic costs <= actual costs
      1.1) In  case of admissibility:-
                Heuristic cost  <= actual cost to goal
                For example, consider 3 nodes, 
                                             A---1----> C ---3---->G
                                         the heuristic value of A is equal to 4.
                                     =>  h(A) <= actual cost from A to G

      1.2) In case of consistency :-
                Here, we consider heuristic arc cost and actual cost for each arc, consider arc between the path from source to goal node.
               
                             Heuristic arc cost <= actual cost for each arc
                
              Difference between heuristics of two states in the path <= cost of arcs between that two states
              For example, A------> C -------> G

                      h(A) - h(C) <= cost(A to C)
  
      1.3) Consequences of consistency:-
             The f value (f(n) = g(n) + h(n)) along a path never decreases when we move towards the goal,
                       h(A) <= cost(A to C) + h(C) 

             With consistent heuristic A* graph search is optimal.

# Optimality of A* graph search
-> 1) In tree search, A* graph search algorithm expands nodes which are in increasing f value.
   2) For any state, lets say state s, the nodes that reach s optimally are given more priority and expanded before nodes that reach s suboptimally.
   3) Because of above two statements, we can say that A* search is optimal.

# Summary of Optimality
-> In case of tree search:
                          - A* search is optimal when heuristic is admissible i.e., heuristic cost <= actual cost
                          - When heuristic value is equal to zero, UCS becomes a special case.

   In case of graph search:
                           - A* search is optimal when heuristic is consistent i.e., h(A) - h(C) <= cost(A to C) 
                           - As h=0 is always consistent therefore, UCS is optimal.

   If heuristic is consistent it also implies that it is admissibile.
   
# Planning: It is sequences of actions
           - In this, the path to the goal matters and different paths have various cost and depths
           - With the help of heuristic we can get problem specific guidance

  Identification: It is assignments to variables
           - In this the priority is the goal state and not path.
           - For some formulations all paths are at the same depth
           - Problem which are specialized for identification problems are CSPs(Constraint Satisfaction Problems)

# Standard Search Problems
-> - In past search problems learned, these problems can be solved by searching in a space of states.
   - In these, each state is like a black box(arbitrary data structure) with no internal strtucture.
   - In these, the goal test can be any function over states, agent use this function to reach to goal state. 
   - Successor function depends on the algorithm we use.

# Constraint Satisfaction Problems(CSPs)
-> - It is a special subset of search problems.
   - CSP consists of three components
      - Xi is a set of variables
      - D is a set of domains consist of allowable values, one for each variable. State is defined by Xi and D.
      - Goal test is a set of constraints which specify allowable combinations of values for subsets of variables.

   - Solving CSP:
                 - Define a state space and notion of a solution. Each state in CSP is defined by an assignment of values to some or possibly all variables.
                 - Consistent assignment is one that does not voilate any constraint.
                 - If every variable is assigned it is called complete assignment.
                 - Partial assignment assigns values to only some of the variables.
   
   - Example in which CSP is used is map coloring problem, N-Queens problem. 

# Map Coloring Problem
-> Given a map with different blocks for each area. Our task is to color the map in such a way that no two adjacent block have same color(Adjacent block means blocks sharing same boundary),
   this is also the constraint to this problem.

   Consider the map with 6 blocks or areas,
                     ___________________________
                    /      |  NT  |            /
                    |      |______|     Q     |
                    |      |      |___________/
                    |  WA  |      |   NSW    |
                    |      |  SA  |__________/    _____
                    |      |      |    V     |   |  T  |
                    |______|______/__________|   |_____|

   - Variables: WA, NT, Q, NSW, V, SA, T (Here, variables are different blocks which we need to color, T is an independent block)
   - Domains: D = {red, green, blue} (Domain is the set of colors which we can give to blocks)
   - Constraints: Constraint is that the adjacent blocks should not have same color.
                  Consider adjacent blocks WA and NT,
                  Implicit WA cannot be equal to NT.
                  Explicit is that (WA, NT) can be assigned {(red, green), (red, blue), (blue,green),.........}
   - Solutions are the assignments which satisfy all constraints.
     For example, {WA = red, NT = green, SA = blue, Q = red, NSW = green, V = red, T = any color}

# N-Queens Problem
-> Given a N x N chess board, place N queens in such a way that no two queens can attack each other(Queen can move forward, backward, up, down and diagonally).
   
   Formulation 1:-

   Variables: X(i,j), X is the position on the chess board, 1 <= i,j <= N
   Domains: {0,1} which represent value of variable X, if X(i,j)=1 there is a queen on this position and if X=0 then no queen at X(i,j).
   Constraints: 
               For all i,j,k, such that coordinate (a,b) should be (1,1) <=(a,b) <= (N,N). 
               1) (X(i,j), X(i,k)) belongs to {(0,0), (0,1), (1,0)}, in a row there cannot be more than one queen.
               2) (X(i,j), X(k,j)) belongs to {(0,0), (0,1), (1,0)}, in a column there cannot be more than one queen.
               3) (X(i,j), X(i+k, j+k)) belongs to {(0,0), (0,1), (1,0)}, in a diagonal(/) there cannot be more than one queen.
               4) X(i,j), X(i+k, j-k)) belongs to {(0,0), (0,1), (1,0)}, in a diagonal(\) there cannot be more than one queen.
               5) Summation of all X(i,j) = N, which means that total N queens have been placed at its respective position.

   Formulation 2:-

   Variables: Qk, represent N variables one for each queen of a row, 1 <= k <= N
   Domains: {1, 2, 3,...., N}, it represent column in which queen can be placed
   Constraints: 
               Implicit: No queens should be in attacking position to another
                         For all i,j, non-threatning(Qi, Qj)
               Explicit: For all (Q1,Q2) possible positions can be {(1,3), (1,4),...}
                         (1,3) means Q1 queen of first row is at column 1 and Q2 queen of second row is at column 3.

# Constraint Graphs
-> Consider the map coloring problem discussed above,
   We can make graph for this map with different blocks as vertex(node) and the common boundaries as edges connecting two blocks.
   - In this graph the constraint is between pairs of nodes can be called binary constraint graph and the corresponding problem is binary CSP(every constraint has two variables). Binary 
     constraint relates two variables e.g., SA not equal to NSW.
   - In binary constraint graph, nodes represent variables and arcs represent contraints.
   - With the help of graph structure,many general-purpose CSP algorithms use it to speed up the search. For example, in map coloring, block T is an independent sub-problem.

# Cryptarithmetic
-> It is a CSP, where each letter stands for a distinct digit and the aim is to substitute each digit with a letter such that the result is arithmetically correct.
   For example, consider a cryptatihmetic problem
 
                 T  W  O 
              +  T  W  O
	      ___________
              F  O  U  R   

   '+' denotes standard addition symbol 

   Variables: letters used {F, T, W, O, U, R} and the carry for each place {X1, X2, X3}
              Therefore, variables are  F, T, W, O, U, R, X1, X2, X3
   Domains: Considering it as base ten summation, domain is {0,1,2,3,4,5,6,7,8,9}
   Constraints: 
               alldiff(F, T, U, R, W, O) i.e., different variables stands for different digits.
                
               O + O = R + 10.X1,  adding ones places and getting carry as X1
               X1 + W + W = U + 10.X2,  adding carry X1 and tens places and getting carry X2
               X2 + T + T = O + 10.X3,  adding carry X2 and hundreds places and getting carry X3
               10.X3 = F,

   Constraint graph for cryptarithmetic-
                                        Edges denotes variables connected.
                                             __
                                            |__|--------------------->alldiff (all different constraint)
                                       /  / /  \  \  \
                                      /  / /    \  \  \
                                     F  T  U     W  R  O---------
                                     |  |   \__  |   \  |       |
                                    _|_  \___  \_|_   \_|_      |
                                   |___| |___|  |__|  |___|     |   This four boxes denotes column addition constraints
                                      \  / | \  /   \   /       |
                                       \/  |  \/     \ /        |
                                       X1  |  X2      X3        |    X1, X2, X3 represent the carry digits for three columns
                                           |____________________|

              This is not a binary constraint graph.

# Suduko
-> Another example of CSP, consider a 9 x 9 suduko involving a grid of 81 squares. The grid is divided into nine blocks each containing nine squares. Some entries filled among 81 squares.
   
   Variables: variables are each empty square
   Domains: {1,2,3,4,5,6,7,8,9} for a 9 x 9 suduko
   Constraints:
               - 9-way alldiff for each column (Each column should have all different digits from 1 to 9)
               - 9-way alldiff for each row (Each row should have all different digits from 1 to 9)
               - 9-way alldiff for 9 blocks of 9 squares (Each block should have all different digits from 1 to 9)

# The Waltz algorithm
-> The waltz algorithm is used for interpreting line drawings of solid polyhedra as 3D.
   It is a kind of CSP for AI computation.
   Variables:- Intersections in the drawing
   Domains:- Consider a cube with one small cube cutted out from one of its vertex. Now we can say that, domains are inward intersection(it is of part cutted out) and outward intersection.
   Constraints:- The adjacent intersections are constraints to each other.
   In this, solutions are physically realizable 3D interpretations.

# Varieties of CSPs
-> There are two types of CSPs in terms of variables:-
   1) Discrete variables
    - It consists of both finite and infinite domains.
    - In finite domain, suppose there are n values then domain size d implies O(d^n) complete assignments(i.e., every variable is assigned).
    - Infinite domains consists of integers, strings, etc. Linear constraints are solvable and non-linear constraints are uindecidable.
    - Boolean CSPs, include boolean satisfiability(NP-complete) are examples of CSPs with finite domain.
    - Job scheduling problem is an infinite domains discrete variable CSP. In this variables are start or end time for each job.

   2) Continuous variables
    - It is a kind of time stamp(time of occurrence of a particular event). 
    - Example is start/end times for Hubble Telescope observations.
    - Linear constraints(eg. aX + bY + c) are solvable in polynomial time by LP(Linear Programming) methods. LP methods are used to get best outcome in a mathematical model when there are 
      linear relationships.

# Varieties of Constraints
-> Unary Constraint:
                    It is a constraint including single variable. It is equivalent to reducing domains.
                    For example, consider map problem, area SA not equal to green. 
               
   Binary Constraint:
                    It is a constraint including pairs of variables.
                    For example, in map problem SA not equal to WA.

   Higher-order constraints:
                            This constraints involve 3 or more than three variables.
                            Cryptarithmetic column constraints is an example of higher-order constraint. 

# Real-World examples of CSPs
-> 1) Assignments problems: e.g., who will teach which class?
   2) Timetabling problems: there are some courses and batches, schedule the courses so that students selected courses time do not overlap.
   3) Hardware configuration: Consider the hardware board, place the transistors, capacitors, etc. in such a way that maximum of them can be placed on hardware to maximize hardware's 
      utility.
   4) Transportation scheduling: Scheduling of trains, flights, buses and many other vehicles. This problem consists of variables, domains and constraints.
   5) Factory scheduling: Managing raw materials, intermediates and production capacity are allocated to meet demand. This is also a kind of CSP.
   6) Circuit layout
   7) Fault diagnosis

# Standard Search Formulation
-> States defined by the values:
   - Initial state: Initial state is empty assignments i.e., none of the variables have assigned the value from domain.
   - Successor function: As moving ahead in search space, start assigning value to an unassigned variable.
   - Goal test: The current assignment have satisfied all the constraints and are complete is our solution. Here also, the solution is the goal and not the path.

# Search Methods
-> Consider the map coloring problem graph with edges as sharing boundary between two regions.
   
   Through BFS:-
   Start state: State which have all variables and none of them is assigned. In this map, there will be 7 variables. Therefore, at each keep track of each variables and color assigned 
   to it or variables unassigned. 
   At start state, all the variables are unassigned. At next level all 7 variables were assigned and there will be multiple assignments also. As here are three values so for start state
   we have 21 children. Picking any node and assigning one of the three values.
   In BFS, we perform one assignment at each level and move to another level. Beacause of branching factor tree will be huge. Our goal is at the end.
   Depth of the search tree is number of variables.
   Disadvantage of BFS is that we have to traverse the whole tree.

   Through DFS:
   At start state no variables are assigned any value. At next level assign a color to an unassigned variable. Now move to next level rather than moving to the same level of the node and 
   assign value to an unassigned variable.
   There are chances of getting a solution in 7 steps in case of the above map coloring graph. Thus there are advantages through DFS than that of BFS in case of CSPs.

   Problems with naive search:-
   We have to perform to go through lot of search in the tree.

# Backtracking Search
-> In DFS we continue with non-satisfying assignments and backtrack when achieved complete assignment. We can imporve it by backtracking whenever dfs chooses values for one variable at a 
   time and backtracks when a variable has no satisfying value to assign.
   Backtracking search is the basic uninformed algorithm for solving CSPs because we do not have information other than constraints for goal.
   Process to achieve the target:-
   - Assigning order to the variables. Choosing next unassigned variable in an order. 
     For example, consider the above map coloring problem, after assigning WA value equal to red and green to NT, now we should go for SA for which only one value possible which is blue 
     rather than assigning Q.
     i.e., we only need to consider assignments to a single variable at each step.

   - After assigning a value to a variable check that it is not voilating constraints with other assignments. If that happens then backtrack and go for other possiblities.
     For these there might be some more computation to check the constraint.

   By improving above two methods in DFS, it will be the backtracking search problem.

   Pseudo-code for backtracking search:- 

                |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
                ||                                                                                 ||
                ||         function BACKTRACKING-SEARCH(csp) returns a solution, or failure        ||
                ||            return BACKTRACK({ }, csp)                                           ||
                ||                                                                                 ||
                ||         function BACKTRACK(assignment, csp) returns a solution, or failure      ||
                ||            if assignment is complete then return assignment                     ||
                ||            var â† SELECT-UNASSIGNED-VARIABLE(csp)            <-------------------------------- Choosing next unassigned variable
                ||            for each value in ORDER-DOMAIN-VALUES(var , assignment, csp) do      ||
                ||                if value is consistent with assignment then                      ||
                ||                    add {var = value} to assignment                              ||
                ||                    inferences â† INFERENCE(csp, var , value)                     ||
                ||                    if inferences (not equal to) failure then                    ||
                ||                       add inferences to assignment                              ||
                ||                       result â† BACKTRACK(assignment, csp)                       ||
                ||                       if result (not equal to) failure then                     ||
                ||                          return result                                          ||
                ||                remove {var = value} and inferences from assignment              ||
                ||            return failure                                                       ||
                ||                                                                                 ||       
                |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||

                Backtracking = DFS + variable-ordering + fail-no-violation

# Improving backtracking
-> Methods to improve backtracking:-
   1) Filtering:- 
                 Keep track of variables which are unassigned and remove the values from domain for this variable with which constraints are definitely not satisfied.
                 Forward checking:- It is a kind of filtering in which we shrink the domain of variables by removing values which do not satisfy constraints.
                                    For example, consider map of above map coloring problem with its domain.
                                    
                                    Variable           Domain              Value assigned
                                      WA          {red, green, blue}           NIL
                                      NT          {red, green, blue}           NIL
               (i)                    Q           {red, green, blue}           NIL
                                      NSW         {red, green, blue}           NIL
                                      V           {red, green, blue}           NIL
                                      SA          {red, green, blue}           NIL

                                   Lets assign WA = red, now shrinking domain of other variables that do not satisfy constraint.
                                     
                                    Variable           Domain              Value assigned
                                      WA          {red, green, blue}           Red
                                      NT          {green, blue}                NIL        <-------------- WA and NT are neighbous therefore red is removed form domain of NT
               (ii)                   Q           {red, green, blue}           NIL
                                      NSW         {red, green, blue}           NIL
                                      V           {red, green, blue}           NIL
                                      SA          {green, blue}                NIL        <-------------- WA and SA are neighbous therefore red is removed form domain of SA

                                   Lets assign Q = green, new domains are
      
                                    Variable           Domain              Value assigned
                                      WA          {red, green, blue}           Red
                                      NT          {blue}                       NIL        <-------------- Q and NT are neighbous therefore green is removed form domain of NT
               (iii)                  Q           {red, green, blue}           Green
                                      NSW         {red, blue}                  NIL        <-------------- WA and NT are neighbous therefore green is removed form domain of NSW
                                      V           {red, green, blue}           NIL
                                      SA          {blue}                       NIL        <-------------- WA and NT are neighbous therefore green is removed form domain of SA

                                   Lets assign V = blue, new domains are
 
                                    Variable           Domain              Value assigned
                                      WA          {red, green, blue}           Red
                                      NT          {blue}                       NIL       
               (iv)                   Q           {red, green, blue}           Green
                                      NSW         {red}                        NIL        <-------------- WA and NT are neighbous therefore blue is removed form domain of NSW
                                      V           {red, green, blue}           Blue
                                      SA          {}                           NIL        <-------------- WA and NT are neighbous therefore blueis removed form domain of SA

                                   As domain of SA becomes empty we will backtrack form here and try new possiblilties.
                                   With forward checking we can not predict future constraint which will be violated.
            
                    Constraint Propagation:-
                                    Through constraint propagation we will detect failure before assigning value to any assignment.
                                    For example, consider the (iii) table above, as domain of NT and SA are {blue} but as they are neighbours so we cannot assign same value to them. So 
                                    here we detect the failure and we backtrack from here instead of assigning Q any value.

# Consistency of a Single arc
-> An arc X -> Y is consistent if and only if for every x in the tail there is some y in the head which could be assigned a value without violating a constriant.
   Consider the above map's constraint graph with directed arcs, lets check its consistency,
   WA -> NT, WA is head and NT is tail. Domain of NT is {red, blue, green}.
   Assigning WA = red. Assigning blue or green to NT does not violate any constraint. If we assign red to NT constraints are violated. So to make it consistent remove red value from domain
   of tail node i.e., NT.
   Always remove values from domain of tail node only.
   Consider WA -> Q, we can assign any value to Q as they are not neighbours and we do not violate any constraint. So constraints are satisfied with any value.
   Every time shrinking domain of any tail node also check for other arcs also. In this map, there are 36 arcs.

# Arc Consistency of an Entire CSP
-> Through simple propagation we can make all arcs consistent.

   Consider the example of filtering forward checking part (iii).
   
                                      Variable           Domain              Value assigned
                                      WA          {red, green, blue}           Red
                                      NT          {blue}                       NIL        
                                      Q           {red, green, blue}           Green
                                      NSW         {red, blue}                  NIL         
                                      V           {red, green, blue}           NIL
                                      SA          {blue}                       NIL        

          Consider arcs,
                       WA  -> NT   consistent
                       NT  -> WA   consistent
                       Q   -> WA   consistent
                       SA  -> NT   inconsistent (We can only assign blue to SA and domain of NT does not have value other than blue. So it is violating constraints and therefore 
                                                 inconsistent. So we backtrack from here.)
                       V   -> NSW  consistent (For any value we assign to V there are other values for NSW so that constraints are not violated.)
                       SA  -> NSW  consistent (Assigning blue to SA we can assign red to NSW)
                       NSW -> SA   inconsistent (Assigning blue to NSW, we cannot assign any other value to SA. To make it consistent remove blue from domain of NSW and check for other
                                                 arcs also)
           
          Important is to check for neighbours of X whenever removing value from tail.
          Arc consistency helps in detecting early failures.
          
    Pseudo code for above is 

           ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
           ||                                                                                                      ||
           ||      function AC-3(csp) return the CSP, possibly with reduced domains                                ||
           ||         inputs: csp, a binary CSP with variables {X1, X2, X3, ....Xn}                                ||
           ||         local variables: queue, a queue of arcs, initially all the arcs in csp                       ||
           ||                                                                                                      ||
           ||         while queue is not empty do                                                                  ||
           ||            (X1, X2) <-- REMOVE-FIRST(queue)                                                          ||
           ||            if REMOVE-INCONSISTENT-VALUES(Xi, Xj) then                                                ||
           ||               for each Xi in NEIGHBOURS[Xi] do                                                       ||
           ||                  add (Xk, Xi) to queue                                                               ||
           || _____________________________________________________________________________________________________||
           ||                                                                                                      ||
           ||      function REMOVE-INCONSISTENT-VALUES(Xi, Xj) returns true iff succeds                            ||
           ||         remove <-- false                                                                             ||
           ||         for each x in DOMAIN[Xi] do                                                                  ||
           ||            if no value y in DOMAIN[Xj] allows (x, y) to satisfy the constriant Xi <--> Xj	           ||
           ||               then delete x from DOMAIN[Xi];  removed <-- true                                       ||
           ||         return removed                                                                               ||
           ||                                                                                                      ||
           ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
  
           Runtime: O((n^2).(d^3)), can be reduced to O((n^2).(d^2)), where n is number of variables and d is size of domain.
                    We are checking for n x n arcs, comparing each value from domain of tail node with other nodes which gives d x d, whenever we remove any value again run the same process
                    which gives d x d x d.

# Limitations of Arc Consistency
-> After enforcing arc consistency we can have three possiblities:
   1) We have one solution left.(In table (iii), NSW -> NT)
   2) We can have multiple solutions left.(In table (iii), WA -> Q)
   3) We can have no solutions left.(In table (iii), SA -> NT) 

   Arc conistency runs inside backtracking search.

# Ordering: Minimum remaining values
-> Choose the variable with least domain size after first assignment.
   In above map, after assigning red to WA least domain size will be of NT and SA.
                 Choose NT and assign green to it.
                 Choose SA and assign blue to it. After that least domain size will be of Q which is one.
                 Assign red to Q.

   Always choosing minimum domain rather than maximum size domain so not to violate constraints.
   It is also called "most constrained variable"

# Ordering: Least Constrianing value
-> Choose the variable such that the domain of its neighbours shrink by minimum amount.
   We may perform more computation then in filtering.
   This makes feasible even 1000 queens problem.

## LOCAL SEARCH AND OPTIMIZATION

# Local search
-> In local search, the agent just keep track of the current state(rather than multiple paths) in which a little memory is used and can only move to the neighbouring states. In local 
   search, we does not care about path taken.
   There are many chances in which we can get reasonable solutions in large state spaces for which systematic algorithms are unsuitable.

# Pure Optimization problems
-> Local search algorithms are useful in solving pure optimization problems in which the aim is to find the best solution according to an objective function(Like heuristic function, 
   objective function is a function which gives the maximum or minimum value as per the problem for each state).
   The goal is to find the state in which value of objective function is maximum or minimum depending on problem.
  
# Trivial Algorithms
-> In local search, when we generate states randomly it is called random sampling.
   When we randomly pick a neighbour of the current state it is random walk.
   
# Hill-climbing
-> It is a kind of greed local search because it selects best neighbour irresponsible about where to go next.
   
   Consider below the pseudo code for hill climbing problem:-

              function HILL-CLIMBING(problem) returns a state that is a local maximum       (Local maximum is a peak that is higher than each of its neighboring states but lower than global
                                                                                             the global maximum)
                 input: problem, a problem                 
                 local variables: current, a node.
                                  neighbour, a node.

                 current <-- MAKE-NODE(INITIAL-STATE[PROBLEM])
                 loop do
                      neighbor <-- a highest-valued successor of current
                      if VALUE[neighbor] <= VALUE[current] then return STATE[current]
                      current <-- neighbour

   Given a problem and variables or states.
   Start with any state, from current state look for neighbours and move to the neighbour with higher value than current state otherwise current state is best in locally(among neighbouring states).
   Above pseudo code is for maximum inequality.

   - there is a loop that continously moves in direction of increasing value and terminates when reaches a peak i.e., where no neighbour has greater value.
   - In this, value can be either objective function or heuristic function.
   - As it is local search we do not look ahead of the immediate neighbours.
   - If there are more than one neighbours with equal objective value we can choose randomly.

# N-Queens problem
-> Place N queens on a N x N chessboard such that no one attacks others.
   Converting it into an optimization problem.
   Objective function will be the number of pairs of queens attacking each other.
   Aim is to minimize the objective function to zero.

   State space - configurations of all N queens on the board.
   Successor function - Can only move each queen to another square in the same column.
   Heuristic function - number of pairs of queens attacking each other. We have to minimize heuristic function.

#  Hill climbing on 8-queens
-> Consider a random start states of 8-queens, around 14% of the time we come up with the solution and 86% of the time we get stuck at local minimum. 
   Whenever it come up with the solution only 4 steps on average are required.
   When it stuck 3 steps on average.

# Drawbacks of Hill Climbing
-> 1) Local maxima :- Through hill climbing algorithm it reaches around local maxima then drawn upward towards the peak and got stuck with no where else to go.
   2) Plateaus :- Plateau is a region of flat area of state space landscape. It is flat local maxima from where no greater peak exist. We stop at flat area.
   3) Diagonal ridges :- It is the sequence of local maxima that is very difficult for algorithm to find.

# Escaping Shoulders: Sideways move
-> Sideways movement helps in escaping the plateau. If there are no downhill or uphill moves possible, it allows sideways moves so that algorithm can escape. To avoid infinite loops we need to put a 
   limit on the possible number of sideways moves.

   For 8-Queens:-
                 Allowing sideways moves and keeping its limit of 100. There is percentage raise in solving problem instances from 14 to 94%.
                 Drawback is that number of steps have increased. 

# Tabu search
-> - Tabu search prevent from returning quickly to same state. Create a queue of fixed length and add most recent state visited to queue and pop out old states.
   - As the size of tabu list increases, hill-climbing asymptotically become non-redundant i.e., never visit same state twice.
   - A reasonable sized tabu list improves performance of hill climbing.

# Hill climbing: stochastic variations
-> In standard hill climbing, we got stuck in local minima. So to avoid that we insert some random walk and random restart in hill climbing.
   - Random walk implies that with some probability p we move to best neighbouring state and with probability (1-p) choose neighbouring state randomly. It will take us out of local minima.
   - In random restart, we start from different states in many iterations and move to best neighbouring state. So there are chances that starting at some iterations we can reach to global maximum 
     state. It ensures complete solution if performed many iterations.
   - Different variations in random restart:
     - For each restart we can terminate it on getting solution or run it for a fixed time.
     - Run a fixed number restarts or run indefinitely.
   - Applying hill climbing approach without sideways moves on 8-queens problem, 14% of the time we get the solution and 86% of the time we get stuck.
     Applyling hill climbing with random restart on 8-queens get us the solution. Approximately 7 restarts are required for getting a complete solution.
   Standard hill climbing is not complete as it stucks in local minima but with random walk it ensures asymptotically complete solution.

   - Combining random walk and random restart:
     Start from any state, choose best neighbour state then choose that neighbour with probabiliy p or choose other random neighbouring state with probability 1-p. Terminate at some point or restart 
     and start from any other state.

# Simulated Annealing
-> It is also one of local search approach.
   - Suppose we have to get to the global minimum.
   - If we just roll the ball from any uphill it definitely get stuck in some local minima.
   - If we can provide some kind of disturbance or shake it such that it can get out from local minima.
   - The intensity of shake should not be greater such that it goes away from global minimum. 
   - It picks the random move instead of best move.
   - Let the change in objective function is $, if $ is better than previous value, move to that state otherwise move to this state with probability propotional to $.
   - Through simulated annealing, there is always a chance of escaping from local minima(in case of minimization).

   Pseudo code of the algorithm:
   
              function SIMULATED-ANNEALING(problem, schedule) returns a solution state
                 inputs: problem, a problem
                         schedule, a mapping from time to "temperature"
                 local variables: current, a node
                                  next, a node
                                  T, a "temperature" controlling the probability of downward steps

                 current <-- MAKE-NODE(INITIAL-STATE[problem])
                 for t = 1 to infinity do
                      T <-- schedule[t]
                      if T = 0 then return current
                      next <-- a randomly selected successor of current
                      $E <-- VALUE[next] â€“ VALUE[current]
                      if $E > 0 then current <-- next
                      else current <-- next only with probability e^($E/T)


     The inner loop is same as that of hill climbing with a bit difference that here instead of picking best move it picks a random move.
     If the move improves the situation it is accepted. Otherewise it accept with some probability. Probability decreases with a bad move i.e., evaluation get worst by amount $E.

     Some of the application in which simulated annealing is practiced:-
     - Solving VLSI layout problem, traveling salesman, graph partitioning, graph coloring, factory scheduling etc.

# Local beam search
-> It is another variant of hill climbing search.
   The idea here is to keep just one node in memory which seems to be an extreme reation to the memory problems.
   - Local beam search keeps track of k states, some randomly generated k states instead of only one state.
   - At each k states, generate successors of each states.
   - Among this successors of k states, there is a goal and the algorithm stops.
   - If there is no goal it selects k best successors among generated successors and again repeat the above three processes.

# Genetic algorithms
-> It is a varaint of stochastic beam search.
   Consider a pair of states or two parent states, now by cobmining these two parents successor states are generated rather than modifying a single state.
   This algorithm begins with a set of k randomly generated states and these set is called population.
   In this each state is represented as a string over a finite alphabet such as a string of 0's and 1's.
   With each state there is an objective function or a fitness function. A fitness function is a function which return higher values for better states.
   Consider the 8-queens problem, here state is the position of 8-queens each in a column. The fitness function will be the function which returns number of non-attacking pairs for queens.
   Some additional notions in genetic algorithm are:
   - Random selection: In a population of k states, the pairs are selected randomly based on there value of fitness. Then we take genetic of each organism from these pair and produce new successors.
   - Crossover: Two individuals are selected from random selection process and crossover sites are selected randomly. At these crossover sites genes are exchanged to form completely a new individual or say offspring.
   - Random mutation:  Randomly selected any point in any state after crossover and insert another genes in it.

   Consider a representation of 8-queens problem with each column has only one queen,

                              
                             8   0 0 0 0 0 0 1 0
                             7   0 0 0 0 1 0 0 0
                             6   0 1 0 0 0 0 0 0          String representation of this is
                             5   0 0 0 1 0 0 0 0                 16257483
                             4   0 0 0 0 0 1 0 0
                             3   0 0 0 0 0 0 0 1
                             2   0 0 1 0 0 0 0 0
                             1   1 0 0 0 0 0 0 0 


                  Illustration of genetic algorithm on 8-queens problem.


                                                           Crossover point/site                                              1 is mutated in place of 5    
                                                                     |                                                           |
                                                                     v                                                           v         
                       2 4 7 4 8 5 5 2 (24)(31%)___           __3 2 7|5 2 4 1 1___      ___3 2 7|4 8 5 5 2 ----------> 3 2 7 4 8 1 5 2
                                                   \_________/       |            \____/        |
                       3 2 7 5 2 4 1 1 (23)(29%)___/\___________2 4 7|4 8 5 5 2___/    \___2 4 7|5 2 4 1 1 ----------> 2 4 7 5 2 4 1 1   <-- No mutation
                                                   \                                                                       v------------------2 is mutated in place of 7
    It is not          2 4 4 1 5 1 2 4 (20)(26%)__  \___________3 2 7 5 2|4 1 1___      ___3 2 7 5 2|1 2 4 ----------> 3 2 2 5 2 1 2 4
   selected beacause                              \                      |        \____/            |
    of least    -----> 3 2 5 4 3 2 1 3 (11)(14%)   \____________2 4 4 1 5|1 2 4___/    \___2 4 4 1 5|4 1 1 ----------> 2 4 4 1 5 4 1 7
  fitness value                          ^                                                                                            ^--------- 7 is mutated at place of 1
                                         |                        Selection                  Crossover                   Mutation
                      4 random states    |                                                
                                       Fitness value                                        New states after
                                      (No. of nonattacking                                    crossover
                                        pairs)
                       More the fitness value more the chances of survival.
                       In selection process, two states are randomly selected based on fitness value. 

      
    Advantage of crossover is that it allows us to jump from one state to another state which cannot be possible even after applying some moves in the start state.

    Positive points of genetic algorithms:
    - Through random exploration it becomes possible to find solutions that are not possible through local search.

    Negative points:-
    - As we are selecting random states therefore there is lack of uncertainity. In selection process we select random states based on fitness value.

# Genetic algorithm for Travelling Salesman Problem
-> Given the set of cities and goal is to travel all the cities atmost once and return back to starting city in minimum cost.
   We will start with a representation of cities. For example consider there are 10 cities to visit, so its representation will be
                                                  [9 3 4 0 1 2 5 7 6 8]

   States are cities and fitness function will return the length of the path. So lesser the value more the chances of survival.
   Techniques for mutation:
   - Randomly choose any two cities and swap them only if swapping reduces path length or cost.
   - Another way is of considering all pairs and swapping the pair which reduces its cost.
   Method for crossover:-
   - Randomly select two parents.
   - Select first city from any one of the parent.
   - Compare the neighbouring cities paths in both parents and select the one with least fitness value.   
   - If a city in one parent is visited leave it and go for city of another parent. 
   - If cities in both parents are already appeared, randomly select another non-selected city and repeat the above process.                       

# Optimization of continuous functions
->  Hill climbing uses discretization as there are discrete states.
    Gradient space is same as hill climbing but it is in a continous space. For example, consider a 3-D space.

# Gradient Descent
-> Consider a continous function of n-variables: f(X1, X2, X3,......, Xn).
   - For computing the gradients, partial differentiation of the function with respect to each variable. This differentiation will provide us the direction of tangent which helps us for next move. In case of minimization, move in 
     downward direction of tangent.
   - In gradient direction take steps downhill depending on the value of lambda,
                    Xi <-- Xi - lamda*(gradients),
     If lambda is small, take small steps and if big take bigger steps.
     Choosing lambda:
                    Either start from small value and successively increase it until f increases or start from large values and successively decrease it.
   - Repeat the above steps.

     In case of downhill, we have to minimize the continuous variables.
 
     Another method is Newton-Raphson method applied to function minimization:
           In this to minimize the function f, we need roots of the equation f'(x)=0. Here we use single and double derivative of f.
                           x <-- x - f'(x)/f"(x)
           In case of x is a vector,
                          -x <-- x - f'(x)/f"(x)    
  
 
# References
-> 1) Slides
   2) Artificial intelligence A modern approach by Stuart Russell and Peter Norvig
   3) https://www.quora.com/What-is-the-different-between-discrete-variables-and-continuous-in-CSP-And-the-different-between-finite-and-infinite-domin-inside-discrete-variable
   4) https://www.planettogether.com/blog/what-is-factory-scheduling#:~:text=Factory%20Scheduling%20is%20the%20production,time%20communication%20of%20schedule%20changes.
   5) https://www.tutorialandexample.com/local-search-algorithms-and-optimization-problem/#:~:text=A%20pure%20optimization%20problem%20is,state%20from%20the%20current%20state.
   6) https://www.geeksforgeeks.org/genetic-algorithms/             