Artificial Intelligence - Lecture Notes (Sept 1 to Sept 11)
Manul Goyal (B18CSE031)
Instructor: Dr. Yashaswi Verma

Introduction to AI
Artificial intelligence (AI) is a recent and exciting field in science and engineering which aims to build intelligent systems that think and/or act like humans. This field is relevant to all the fields which require automation of intellectual tasks. In an era with an exponentially increasing population and incessantly increasing demands for products and services, there is a dire need for automation of manual tasks because robots are much more efficient than humans in terms of speed and capacity. AI aims to extend this automation from straightforward mechanical tasks to creative and intellectual tasks. There are four main approaches in AI, visually, building machines that think as a human, act as a human, think rationally or act rationally. The 'thinking as a human' approach is based on cognitive science, which studies the actual inner workings of the human brain. The 'acting as a human' approach is defined by the Turing Test, in which a human interrogator poses some questions to a machine, and if the human interrogator cannot tell whether the responses are generated by a human or a computer, the system is considered to be intelligent. The 'thinking rationally' approach is based on the field of logic. Problems are reframed in the form of formal logical statements and arguments are constructed by inferring valid conclusions from a set of premises. But such a reframing of problems into formal statements isn't always possible. The 'acting rationally' approach is more general as it also includes forms of rationality other than logical inferences, such as reflexes. An agent is something which acts, but it is also expected to operate autonomously, perceive its environment, adapt to changing environment and create and pursue goals. We are going to study the design of intelligent agents that act rationally under a given situation. The term 'rational' here means 'in the best possible manner'. Rationality depends on the decisions made by an agent while solving a given problem, and not on the thought process behind them. A goal-based agent starts by formulating a goal based on the utility of the outcomes (the utility is just a numeric value assigned to each outcome which describes how desirable the outcome is). Acting rationally then corresponds to maximising the expected utility. AI is inspired by the working of the human brain. But instead of trying to mimic the human brain, it aims to understand the underlying principles of intelligence itself. The brain teaches us that memory and simulation are key to decision making, which motivates many approaches in AI. There are many tasks for which there exist AI models which far outperform human experts. For example, the most recent chess engines are able to convincingly defeat world's best human chess players. Similarly, object and face recognition, image classification, speech recognition, language translation, and self-driving cars are fields in which AI proves to be quite effective. On the other hand, there are tasks in which we have still not been able to effectively apply AI, such as having long conversations with a human and generating intentionally funny stories. Both these tasks require some form of creativity, and defining 'funny' formally in the latter case, which is quite difficult to do. This course is about the design of rational agents which can find a solution of any general problem defined in terms of an initial state, transition model, action space, a set of world states and a cost function.

Types of Agents and Planning
There are various types of agents. Two such types are reflex agents and planning agents. Reflex agents are agents which do not consider the future consequences of their actions, i.e., they do not plan ahead. They adopt a greedy approach, i.e., they consider only the current percept or state they are in and choose the best action possible based on the current situation (and possibly the past states the agent has encountered). A reflex agent may or may not have memory. If it does have memory, then it would make decisions based on the history as well as the current world state. A reflex agent may or may not be rational as a greedy approach may not always lead to the optimal solution. For example, in a navigation problem, the agent may end up in a dead end because it just kept on going closer to the destination in terms of displacement from it, but did not consider whether there actually exists a road to the destination. The second type of agent, called planning agent, considers the future consequences of its actions and makes decisions based on these hypothesized consequences of its actions. In order to do this, the agent must have some model of the world, including a transition model, which determines which actions are possible in a given state and how the state changes in response to a given action. The agent also formulates a goal test, which is a test that takes a state as input and determines whether it is a goal or not. The agent then plans ahead about how to reach the goal using the model. A planning may be optimal or complete. A complete planning guarantees that it will find a solution if one exists, but it may not be optimal. An optimal planning guarantees that it will find the optimal solution. Therefore, every optimal planning is also complete, but not vice-versa.

Search Problems
A search problem consists of an initial or start state, a successor function, a cost function and a goal test. The successor function takes a state and an action which is permitted in this state as its inputs, and outputs the state resulting from the action applied on the input state. The set of actions and the successor function implicitly define the state space of the problem. The goal test determines whether a given state is a goal state. A search problem can be modelled as a directed graph with nodes representing states and edges representing actions. A solution to the problem is a sequence of actions or a plan (a path in the directed graph) which transforms the initial state to a goal state. The path cost function takes a path (a sequence of actions) as input and outputs the cost associated with it. An optimal solution is a solution which minimizes the cost of the path. States can be of two types, world states and search states. A world state contains information about each and every detail of the environment, while a search state only contains the information relevant to the problem, i.e., it abstracts away the unnecessary details of the environment. The information contained in a search state depends upon the problem. For example, if the goal is to reach a given city from a starting city (pathing problem), the states would be all the possible cities the agent could be in. Now, if the goal is changed to visiting the maximum possible tourist attractions while visiting each city at most once, the states would be all the possible cities along with the number of tourist attractions in each city. The goal test may correspond to a fixed set of states as in the first example, while it may be just a condition not relating to a particular state as in the second example.

State Space Graphs and Search Trees
A state space graph is a directed graph which is used as an abstract model of a search problem. Each node corresponds to a world state and the arcs (edges) represent actions which lead to successor states (nodes). Each state appears only once in the graph. The goal test corresponds to one or more goal nodes. A search tree consists of nodes where each node represents not only a state but a complete plan to reach that state, i.e., each node in the search tree corresponds to a path in the state space graph. The root of the tree corresponds to the start state and there is a unique path between the root and each node, which corresponds to the specific plan to reach that node. A state may appear multiple times in a search tree, as there can be more than one path to reach the state from the start state. In practice, both the state space graph and the search tree may be quite large and therefore infeasible to fully construct in memory. So, they are constructed incrementally, nodes are added when they are required. 

Searching in a Search Tree
The basic strategy for searching is to maintain a fringe consisting of tree nodes which are potential candidates for expansion in the future. Initially, the fringe contains the root node (initial state). Since each tree node represents a partial plan, the fringe is essentially a set of partial plans which we may expand in the future. By expansion, we mean that we remove the node from the fringe and insert all possible successor nodes of this node into the fringe. The major question is how do we decide the priority of nodes in the fringe, i.e., which node (plan) do we pick next for expansion. This choice directly affects how quickly are we able to find a solution, and therefore, how efficient our algorithm is. The general search algorithm proceeds as follows:

function TreeSearch
Inputs: problem, strategy
Output: a solution or failure
1. Initialise the search tree with the start state as the root node
2. Repeat the following:
3.		If there are no nodes left to expand, return failure
4.		Choose a leaf node from the tree according to strategy
5.		If this node corresponds to a goal state, return the corresponding plan (solution)
6.		Expand this node and insert the resulting successor nodes into the search tree

The tree is represented by a fringe which contains the candidate nodes for expansion. The strategy here determines the priority scheme of the nodes in the fringe, i.e., which node to explore next. Loops in the state space graph can be handled by ensuring that there are no repeated nodes in any given path from root to a node in the search tree. If a node encountered is already present in the path, we do not expand it again.

Uninformed Search Algorithms
Such algorithms don't have any prior information or indication about the position of the goal node(s) in the search tree. These algorithms blindly explore the search tree and have no idea whether their current strategy would eventually lead to a goal node or not. The goal state is only determined once the goal test is performed on the state. DFS, BFS, iterative deepening and UCS come under this category, and are described next.

Depth-First Search (DFS)
This algorithm works by expanding the deepest node in the fringe first. The fringe is a last in first out (LIFO) stack in this case. The last node pushed into the fringe is the first one to be expanded. This effectively results in searching an entire path from the root to a leaf and then proceeding to the next path. This strategy is complete if we are able to take care of cycles, i.e., given a solution exists, it is guaranteed to find it; but the solution found may not be optimal. The reason for this is that as soon as we first encounter (pop from fringe) a goal node, we stop and return the solution, and thus never check more optimal (low cost) paths that may exist. Let the branching factor of the search tree be b and the maximum depth be m. In the worst case, the goal node may be present in the last (deepest) level in the tree. In this case, we are pushing and popping each node in the tree once. The total number of nodes in the tree are b^0 + b^1 + ... + b^m = O(b^m). Thus, the worst-case running time of this algorithm is O(b^m). Now, at any given moment, if we are expanding some node k, the fringe would contain k and its siblings as well as all the ancestors of k up to the root and all the siblings of each ancestor. Thus, the fringe stores at most b nodes at each level, so the space complexity of this algorithm is O(b*m), since the maximum depth is m. Note that we don't need to store the whole tree or graph in memory because we are generating the nodes only when they are first needed. Also, in order to prevent m becoming infinite, we need to take care of cycles. This can be achieved by ensuring that no node occurs more than once in any path from root to a node. If we ensure this, the algorithm would be complete, but still may not be optimal because it returns the leftmost solution.

Breadth-First Search (BFS)
This algorithm works by expand the shallowest node in the fringe first. The fringe is a first in first out (FIFO) queue, which results in a level-order traversal of the search tree. It returns the shallowest solution it finds, i.e., the goal node at the smallest depth is found and its solution is returned. Assuming that the shallowest goal node is at a depth s, the number of nodes explored before reaching the goal is b^0 + b^1 + ... + b^s = O(b^s), so the running time would be O(b^s). Also, in the worst case, when the goal is present in the last level, we would end up exploring the whole tree, giving worst case complexity of O(b^m). Since the fringe may need to store all the nodes at a given depth, the space complexity is O(b^s) (in worst case, s = m). Also, if a solution exists, s must be finite, and thus, this strategy is complete. But we still need to take care of cycles since the algorithm would never terminate if no solution exists and cycles are not accounted for. It gives the optimal solution when the cost of each edge (action) is 1, but may or may not be optimal in other cases.

BFS vs DFS
Although in the worst case both algorithms would end up exploring the whole tree leading to the same asymptotical time complexity, DFS would outperform BFS in terms of space complexity. In some specific scenarios, however one may outperform the other in terms of running time. If some solution exists in the left part of the tree and at a deeper level, DFS would find it faster, while if some solution exists at a shallower level and towards the right part of the tree, BFS would find it faster. If we could approximate the position of the solution in the tree by using some heuristic, we could employ the more suitable strategy.

Iterative Deepening
This algorithm combines the space advantage of DFS with the time advantage of BFS for shallower solutions. It works by first restricting the maximum depth to explore to 1 and then applying DFS, then incrementing it to 2, then again applying DFS, and so on. We are effectively incrementing the search area level by level, until a solution is found. Although it results in redundant computations as we repeatedly explore the top part of the tree, it may be efficient for relatively shallow solutions due to its level-by-level exploration strategy, while still using lesser memory as compared to BFS. Moreover, the major work occurs at the lowest level permitted during some iteration, the redundant work is relatively smaller compared to the new work.

Uniform Cost Search (UCS)
BFS guarantees a solution which consists of the minimum number of steps or hops to reach a goal state from the start state. But it may not provide the optimal solution in the case when each action has some non-negative cost associated with it and our optimal solution is required to minimize the sum of the action costs in the solution. In this case, UCS guarantees an optimal solution. It works by maintaining a fringe implemented by a priority queue, which prioritizes the nodes with smaller cumulative costs. The cumulative cost of a node in the search tree is the total cost of the path from root to this node in the tree (i.e., the cost of the plan to reach this node). The invariant maintained is that whenever we pop a node from the fringe for expanding, all the nodes with smaller cumulative costs than this node have already been explored. No node with a smaller cumulative cost can occur after this node since all the costs are non-negative. Therefore, the first time we pop out a goal node from the fringe, we are sure that the corresponding solution obtained is optimal since all subsequent encounters with a goal state would have a greater cumulative cost. So, we can stop when we first pop out a goal node.
Now, suppose that the cost of the optimal solution is C, and the cost of an edge is at least e. As described above, we would explore all nodes with cumulative costs less than C before reaching the goal node. The goal node would be present at a maximum depth of C/e. Thus, in the worst case, we may end up exploring all the nodes in the levels 0, 1, .., C/e, since every node deeper than the level C/e surely has a cumulative cost greater than or equal to C. So, the worst-case time complexity is O(b^(C/e)). The space complexity is also O(b^(C/e)) as the level C/e contains at most b^(C/e) nodes. Given that the optimal solution has a finite cost and all action costs are non-negative, this algorithm is both complete and optimal.
Since UCS is an uninformed search, it blindly explores all possible directions without any prior information about which direction the goal may be present in.

A note about the fringe
The fringe, in general, is a priority queue, with different ways of assigning priorities for different strategies. A single implementation could suffice for all above algorithms, with the only change being in the definition of priority. For example, in DFS, nodes with later timestamps have higher priority than those with smaller (earlier) timestamps. But in the case of DFS and BFS, the additional log(n) factor of a priority queue can be avoided by using a stack and a queue respectively.

It's just a simulation!
The agent doesn't actually try out all the different strategies in the real world, rather it simulates the above strategies on models (approximations) of the real world. Once a solution is obtained via simulation, it can execute the solution steps (action sequence) in the real world. The planning is carried out using models, thus, the plan obtained is only as good as the model used. The more accurate the model, the more practical the plan for execution in the real world. For example, a map is a model of the street layout of a city in the real world. A map may not account for roads which pass through someone's private property, and hence are restricted. In this case, the search algorithm may include restricted roads in the solution. But it is not the shortcoming of the algorithm, but rather of the model itself. 

Informed Search Algorithms
These algorithms make use of some information about the goal to guide the decision-making process (i.e., which action to choose in a particular state). The agent makes use of some heuristic which it can use to gain some insight about where the goal node might be in the search tree. Here, we define a search heuristic as a function that takes a state as an input and outputs an estimate of how close it is to a goal state. In contrast to the search algorithms described above, which are general algorithms which work for any search problem, the search heuristic varies depending on the particular search problem at hand. Different heuristics could be useful for different situations. For example, for pathing problems, some useful heuristics are Euclidean distance and Manhattan distance. These heuristics measure the actual physical distance between some state and the goal state; hence, they are useful for path-finding problems. If the heuristic value reduces due to some action (moving from one location to another), it indicates that the agent is getting closer to the solution (destination). However, the agent may not necessarily be guided in the right direction by blindly following the heuristic. Next, we see some informed search algorithms based on search heuristics.

Greedy Search 
This algorithm, as the name suggests, works by choosing the node from the fringe which is estimated to be the nearest to a goal node. Continuing the above pathing problem, the greedy algorithm will choose the node with the least value of the heuristic (the node which is nearest to the destination) from the fringe for expansion. Thus, in this algorithm, the priority is assigned to a node based on the value of the heuristic corresponding to that node. This algorithm is complete because it will eventually end up searching the entire search tree, and therefore, if a solution exists, it will be found eventually. It is a common case that the initial guidance provided by the heuristic may lead the agent to end up at a dead end. Once these values of the heuristic are eliminated, other paths would be searched by the algorithm. In the worst case, when there are many paths which come very close to a solution but do not lead to the solution, this algorithm may end up exploring a much greater part of the search tree than even the uninformed search algorithms. Hence, this algorithm may or may not be more efficient than an uninformed algorithm, depending on the particular problem, the environment and the heuristic used. Since this algorithm chooses the best possible actions based on only the current state and doesn't account for the future consequences of its decisions, it is a kind of reflex agent. Due to its greedy nature, the solution obtained may not be optimal. 

A* Search: Combining UCS and Greedy Search
This algorithm tries to combine the optimality property of UCS and the less computation required (efficiency) in Greedy search. The UCS algorithm prioritizes nodes by the path cost or cumulative cost of a node n from the root, also called backward cost g(n). On the other hand, the greedy algorithm prioritizes nodes by the estimated goal proximity of the nodes (based on some heuristic), also called the forward cost h(n). A* search uses the function f(n) = g(n) + h(n) to prioritize the nodes in the fringe. f(n) is an estimate of the sum of how much distance we have already covered and how much distance is approximately left to be covered to reach a goal node. 
Here again, we observe that we stop only when we dequeue a goal node from the fringe, not when we enqueue it. The A* and greedy search only differ in the way they assign priorities to the nodes and can be implemented in the same manner as the uninformed algorithms. The A* search is complete but may not always give the optimal solution, which we see next.

Optimality of A* Search
The A* search may not give optimal solution in cases where the heuristic function overestimates the forward distance of a node from a goal node, i.e., it estimates the forward distance of a node to be larger than the actual distance (path cost) from the goal. In such cases, the algorithm may end up preferring another plan with lower value of f(n) but higher value of the actual path cost. The optimal plan was not chosen because its estimated cost was greater than the estimate for the non-optimal plan, leading to a non-optimal solution. In order to resolve this problem, we need to impose a constraint on the heuristic that it needs to be less than the actual forward cost (i.e., the actual path cost between a node and a goal). In other words, the heuristic should be optimistic instead of pessimistic, it may underestimate the forward cost, but not overestimate it. This constraint ensures optimality in the A* search algorithm.

Inadmissible (Pessimistic) and Admissible (Optimistic) heuristics
Inadmissible heuristics may break the optimality of a search algorithm by overestimating the actual cost of plan, which may lead to the optimal plan not getting chosen for expansion from the fringe. On the other hand, admissible heuristics have the effect of slowing down bad plans but never trap good plans in the fringe because they never overestimate the true costs.

Definition of Admissible Heuristics
A heuristic h is said to be admissible (optimistic) if 0 <= h(n) <= h*(n), for all nodes n, where h*(n) is the actual shortest path cost from the node n to the nearest goal state. For example, in the pacman pathing problem, Manhattan distance is an example of an admissible heuristic, because the minimum actual path cost to the destination can't be less than the Manhattan distance. Similarly, since Euclidean distance is always less than or equal to the Manhattan distance, it is also an admissible heuristic for the above problem. Finding an admissible heuristic is a major step in applying the A* search to a problem.

Proof of Optimality of A* Search
Let's assume that A is an optimal goal node, B is a suboptimal goal node, and h is an admissible heuristic. We claim that A will exit the fringe before B.
Proof:
Let's assume that B is in the fringe and some ancestor n of A (maybe A itself) is also in the fringe (we can safely assume this because the root node is also an ancestor of A and it is present in the fringe at the beginning). We claim that n will be expanded before B. To prove this, we first show that f(n) <= f(A) [1]. We know that f(n) = g(n) + h(n) and f(n) <= g(A) (since h is admissible). Also, g(A) = f(A) since h(A) = 0. Thus, f(n) <= f(A). Secondly, we observe that f(A) < f(B) [2], because g(A) < g(B) (since B is suboptimal) and h(A) = h(B) = 0 (since h for goal node is 0). From [1] and [2], we conclude that f(n) < f(B), and hence n will be expanded before B. Now, since n was chosen arbitrarily, it implies that all ancestors of A would be expanded before B. Once the parent of A is expanded and A enters the fringe, it will be expanded (popped out) of the fringe before B since f(A) < f(B). Hence, the claim is proved.

From the above claim, we can conclude that the A* search is optimal, given that the heuristic used is admissible.

Properties of A* Search
The A* search when used along with an admissible heuristic function, is not only optimal, but it also needs to search less than UCS in order to reach a solution. A heuristic is relatively better if it approximates the shortest actual cost to a goal state more closely than some other heuristic. The calculation of such a heuristic for each node increases the amount of computation required for each node, but reduces the search area in the search tree. Compared to the greedy search, A* explores more nodes in the search tree, but ensures an optimal solution, because it also considers the backward distance.
In practice, an inadmissible heuristic may be preferred over an admissible one when computing the former is considerably faster than the latter, and non-optimal solutions may be acceptable.

Creating Admissible Heuristics
It often involves relaxing the problem at hand by introducing new actions and ensuring their cost is less than the actual path costs. For example, in a pathing problem in a city, we may relax the constraint to only travel through roads and allow flying straight to the destination. This would provide the Euclidean distance as an admissible heuristic for this problem.

References
1. Russell, Stuart J. (Stuart Jonathan). Artificial Intelligence : a Modern Approach. Upper Saddle River, N.J. :Prentice Hall, 2010
2. Lecture Slides