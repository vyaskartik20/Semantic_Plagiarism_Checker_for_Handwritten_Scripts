Artificial intelligence
Understanding and building Intelligence


#Defining AI
All the definitions of AI generally are looked at from the perspective of two dimensions. Rationality and thought process/behaviour. This leads to 4 different approaches people usually take to make an AI model.
      1. Thinking like human (Cognitive Modeling)
      2. Thinking Rationally (“Law of thought approach)
      3. Acting like human (The Turing Test Approach)
      4. Acting Rationally (Rational agent approach)
#Intelligent Agents
Let's use the example of terminator to explain the concept of Intelligent Agents. He came to interact with earth and used this eye-camera to perceive his environment and his machine gun hand to shoot anything he doesn't like.
In this case, terminator is an agent because it is perceiving the environment(earth) through his sensors(eye-camera) and acts upon that environment through the actuators(his machine-gun hand).
A percept would be an agent's input from the sensors at a given instant. (The current input from the eye-camera of the terminator). A percept sequence of an agent would be the complete history of everything it has perceived.


#Reflex Agent
The actions selected by a reflex agent are based on the current precept (which could be saved in a memory) and not any previous or future percepts. A precondition for reflex agents to work properly is that the environment should be fully observable.


#Planning Agents
The decisions hypothesized by this kind of agent are based on the consequences of the actions, towards attaining the desired goal.


#Defining a problem
There are 5 main components of a well defined problem: Initial State, action function returning set of action available at a particular state, transaction model returning the state after a particular action on a state, a goal test checking if a state is a goal state, and a path cost returning a cost to each transaction.
A solution is a sequence of transactions from initial state to a goal state, and an optimal solution would be the one with lowest path cost.
Generally a problem comes under either Toy Problem (with concise and exact description) or Real-world Problem (general problem formulation with no exact description).
Let's take the example of a 3x3 sliding puzzle to define the problem. A state would represent the location of all 8 tiles and the blank void. Initial state would be the one we start with, and actions would consist of moving the void to left,right,up or down(depending on the position of the void). The transaction model would return the state after an action on a state. Goal test would be the state of final arrangement and path cost would be the ratio of hardness to do an action at a state.


#State Space Graph
The initial state, actions and transaction model make up the state space of a problem. Each node in the graph represents a state linked together by edges representing the actions. A path would be a sequence of states connected by actions.


#Search Trees
A search tree would be an action sequence tree starting from initial state as root. The child nodes of a state would represent the successor of the actions of the parent state. Unlike the state space graph, in the search tree, each node/state also represents the action sequence it takes to reach that state (the path).
Theoretically, we try to compute both the state space graph and Search tree on demand and as less as possible.


#Searching a tree
In general, a search problem involves looping from an initial state, choosing a leaf node according to the exploration strategy and check it. if not the goal state, expand the leaf nodes to the tree. The three important things here are to decide the strategy, the method of expansion and the fringe we choose. Fringe is a (ordered) set of unexplored nodes we want to visit next.
Certain properties to consider in a search problem are, the algorithms completeness, optimality, and the space and time complexities.


##Depth-First Search
We go through the child node first (strategy) and have a LIFO stack for the fringe. This method can be complete if we avoid the cycles in the state space graph, but it does not find an optimal solution. If the states are finite, the time complexity would be O(total states), given no loops, and fringe’s space complexity would be O(branching factor*maximum depth).


##Breadth-First Search
We go through the sibling states first (strategy) and have a FIFO queue for the fringe. This method is complete ,but it only finds an optimal solution, if all actions have the same cost. The time complexity would be O(branching factor^goal state depth), and fringe’s space complexity would be O(total states).


##Uniform Cost Search
We go to the cheapest neighbouring state first (strategy) and have a priority queue according to the total cost for fringe. This method is complete if costs are not negative, and it will always find the optimal solution. The time complexity and the fringe space complexity  is O(branching factor^(solution costs/least arc cost)). Though it is optimal and complete, it searches for every direction and has no idea about the goal location.


If the solution is expected to be found in the greater depth of the tree, we could have an iterative DFS search with depth limit, to leverage the BFSs time complexity and DFSs space complexity.
Also, all the fringe strategies used in the above search algorithm can be implemented by a priority queue with different priority.
#Informed Search


##Heuristic Function
Heuristic function estimates the cost of the cheapest path from current state to the goal state.
NOTE: The output of the function only depends on the state of the input node, and not other nodes except the goal state.


##Greedy Search
Greedy search leads you to nodes, which seems closest to the goal, first. The fringe priority would therefore depend on the heuristic function (or the straight line distance). This method is not complete for the search tree version if we find any loops, but in case of a state space graph, it is complete (given the no. of nodes are finite). The space complexity for the tree version is O(branching factor^maximum depth) (but with a good heuristic function, it can be reduced)


##A-Star Search
The cost function is the summation of the heuristic function (forward cost) and the cost to reach the node (backward cost). This method is both optimal and complete, given the heuristic function is admissible.
An admissible heuristic function never overestimates the cost to reach the goal, hence not getting stuck in a loop of good plans. Or h(x)<h*(x)[true cost to a nearest goal].
An admissible heuristic function would always lead to an optimal solution because, even if a parent node of the optimal solution has the sub-optimal goal state as a child, optimal solution will expand before any other goal state since cost to optimal solution and its ancestors will be always less than other path leading to other goal states.
Since the cost function for A* is non-decreasing, we can have counters for each state. In this case we expand to states with cost <= cost of optimal path, unlike UCS where we expand in all directions equally.
A-star search can be helpful in many applications like path finding games(pac-man) and robots, language analysis and speech recognition and much more.


##Relaxed Problems
A relaxed problem is a superset of a problem with less restrictions. It’s state graph has the same state nodes but more edges (actions). Therefore, the optimal solution for a relaxed problem is the same as the original problem, unless an edge provides a shortcut. Hence, the cost function for a relaxed problem is the admissible heuristic function for the original problem.


#Reference
 Artificial intelligence : a modern approach/ Stuart Russell, Peter Norvig.
AI which is short form of Artificial Intelligence is a technology being developed and used in day to day life. It is used to acheive specific goals in best possible way. We will be using the term rational here in a technical way. We can say rational as acheiving predefined goals maximally and it concerns what decisions are made and not the actual thought process behind it. Also goals are expressed in terms of the utility of the outcomes. Like for example i want to go from City A to City B in "minimum possible time" . So goal is to reach to City B from City A in minimum possible time. So methods like ,Shortest path from place A to place B , or a path with least possible traffic or best road condition and calculating the expected time for each method and giving the route which is give help us reach in minimum time.

Human brains are said to make very good rational decisions but not perfect. So AI basically tries to function like a human brain, but the structure may be different. For example, for an airplane, the basic idea of wings came from birds, but the design of wings of planes were different.
Various examples of what AI can do are
1. Play a decent tennis game
2. Give us best possible route from place A to place B.
3. Make a phone call based on speech recognition
4. Language conversion etc
5. Drive a vehicle
What it cannot do yet.
1. Converse with a person for an hour
2. think of new ideas like a human brain does.
All of the above examples are from broad terms like decision making,Game playing,Logic,Robotics,Vision,Natural Language,etc.

This was a broad summary of Artificial Intelligence.

AI mostly comprises of agents which help complete the predefined goals. For examples there is a machine which has to get an apple from the tree so the agent will do a precalculation or preplanning of how its going to jump with what velocity etc to get the apple.
There are some agents like reflex agents,Planning agents etc.
Reflex agents are the agents which take action on the basis of the current situation and dont consider the aftermath of the action. So this means that it is not necessary that the reflex agents are rational.
Planning agents are the agents which takes action after considering the consequences of the action which it is going to take. So it also must have a model in which the space/environment evolves on the basis of the actions taken. Important thing to note is the consequences of the action considered are based on the computational power and other factors.
Search Problems
A search problem consists of a state space, a successor function which contains the actions, costs etc and finally start state and goal state. So a solution is a sequence of the actions which transforms a start state to a goal test.
But what does a state space have? So essentially a state space keeps only the data/information needed for planning. So it depends on the objective. If we want to find the shortest path then it will have the information about the data about the current state like co-ordinates.A Successor function, which updates the current state and moves to next state and try to move towards the goal test. So in shortest path problem successor function will update the current state. Goal test can be if (x,y) == END;
A world state contains all possible information about the environment.
If we consider all possible states and link them corresponding to their next states, we will eventually get a graph or a tree but we can rarely build a full graph because it would not fit in the memory.
SEARCH SPACE GRAPHS.
A search space graph is a mathematical representation of a search problem.In a state space graph , the nodes represent the different configuration of world of problem. An edge denotes the action taken and leading to which node. And a goal test is represented by node(may more than 1). Important thing to note is that each space in state space graph occurs only once.


SEARCH TREES
These are different from a search graph. These may not contain all the states, but may contain several possible paths. Also there may be repeatition of states. We donot make a full state space tree because it might exceed memory. In case if there is a cycle in the graph then space tree would be infinite. Thus we dont calculate the whole space tree. Each node doesnot represnt a value or a place, but a path.
Also we take some steps so that we donot go into an infinite cycle or exceed memory.

So the formation of search tree depends on the algorithm with which we are expanding the nodes. For example if we are using breadth first search method, then all the neighbouring nodes of the nodes will be checked first and then proceed to next nodes where as in depth first search , we will take a node and expand till we reach the leaf node or the end point of search and then expand the different child of the root node.
There is a term called fringe which tells us what are the nodes that are yet to be expanded. 
Using DFS in a search graph to make a tree will give us a solution if it exists but may not be the most optimal solution. 
The general definition of Tree Search is :
function TreeSearch(problem,strategy) returns a solution, or a failure initialise the search tree using initial state of problem.
loop do
      if there are no candidates for expansion then return failure.
      choose a leaf node for expansion according to strategy
      if the node contains a goal state return the corresponding solution
      else expand the node and add the corresponding nodes to the search tree.
end
The leaf nodes correspond to the search tree's leaf nodes that are yet to be expanded(these are maintained by the fringe)	 

Search Algorithm Properties 
There are a few search algorithm properties like complete,optimal etc. An algorithm is said to be complete if it guarantees to find a solution. An algorithm is said to be optimal if it is guaranteed to find the least cost path.
So if a tree has height m and branching factor b at each node , then there are total of b^m nodes in the tree. 

So if we check the algorithm properties of DFS algorithm , taking the fact into consideration that we are taking care of cycle and no negative nodes are there,
Complete - Yes DFS algorithm will be complete. 
Optimal , not necessarily.
maximum time complexity = O(b^m)  and space complexity = O(m) because if we use recursive approach the maximum height which can be reached is m. so only m nodes will be stored.
We also have to keep a track of each element if it is being repeated again so that there is no cycle. Thus it will require extra space.  Also this extra space will be less than the total nodes because if we are exploring a single child of a node, then we dont have to keep track of other child nodes of the node which we are exploring. So the space complexity will be O(bm). It would have been O(m) if there were no cycles. but since we are taking care of cycles too so in total we will need O(bm) space.

Now if we check the algorithm properties of BFS algorithm, then 
complete- YES 
optimal - Not neccesarily.
So BFS expands nodes which are only above the shallowest solution and let its height be equal to s. So the search time will be O(b^s);
The space taken by fringe will be O(b^s)

On average DFS has better space complexity than BFS. 
So if the solution is close to the root node(in upper layers) then BFS will outperform DFS in most cases 
else if the solution is deep down the tree, then  DFS will outperform BFS.

So we can calculate the search state tree using a parent first child tree and maintain fringe using LIFO stack. So whenever we traverse a graph we push all the child nodes in the fringe stack and then explore the top element of the stack and repeat the same push and explore the node until we find the final answer. Note that we will not stop the process when we push the node, but we will stop the process when we are popping the node from the fringe. This has important meaning covered later.

There is a combination of both dfs and bfs which can be used to have dfs's advantage and bfs's advantage at the same time. 
Set the initial height as 1 and then run a dfs till that height. if we find the node then close the program. if not then increase the height by 1, and again run dfs till that height. 
Repeat this process till we find the solution node. This is wastefully redundant but will result in solution earlier on average.

We used to the above traversal methods to find the goal node with minimum number of actions, not least cost path. Now we will do a cost sensitive search.
Uniform cost search which will expand the nodes on the basis of the cumulative cost. Here comes the part that why we stop the program when we pop from the stack. This is because here, if we pop it as soon as we get to the node we might miss the other path which has lesser cost than the present cost of reaching here .Thus we terminate the program when we pop the goal node from the stack.

Algorithm properties of uniform cost search algorithm
lets say the cheapest cost is C* . and the cost of least edge is e. so effectively there will be C*/e edges in between. So depth till which we will be calculating will be C*/e. 
Thus the time complexity will be O(b^(C*/e)) and space complexity will be O(b^(C*/e)) 
If the cost if finite and the edge cost is positive then yes it is a complete solution.  
And yes it is the optimal solution. But if the edges were negative too, then we might get a solution which may or may not be optimal.  The disadvantage of the uniform cost solution is that we have to search in all the directions and does not give information about the goal location .

Conceptually if we see, then the fringes are basically priority queues. So the priority for every method is different . Like in Uniform cost search , the priority was minimum cost . In DFS and BFS we can assign time stamp for the nodes and remove it from the fringe based on minimum and maximum timestamp for DFS and BFS respectively. This can be done using a single priority queue. 

Sometimes there can be different errors too. Sometimes the model may have insufficient information. for example google maps gives shortest distance from place A to place B. But it might give a solution through the private property of a person and thats not allowed in real life. So thats wrong.

An example of how we can convert some problem into a Search State Tree. Lets say we have a pile of pancakes of uneven sizes. We can flip only a top x number of pancakes. So we have to flip those pancakes until all of the pancakes are sorted from top to bottom. We have to find the minimum number of pancake flips to do that. So lets say we assign cost as the number of pancakes flipped in a single go. So that way we have n-1 possible states given n were total number of pancakes. Thus we have State Space Graph with weights of edges as the cost of flipping the pancakes. Then we can do Uniform cost search to find the optimal solution.

Most of the above search algorithms and methods were uninformed searches because we didnt knew where we were going. Just trying to get the best solution. 
Now Moving towards the Informed Search in which we will be having the direction of the goal state and proceed towards it.
Heuristics :
A heuristics is a function that estimates how close we are to a goal. A heuristic doesnot tell us anything about the solution. It only gives us estimate about how close we are to our goal. So following a heuristics function is like a greedy method, which may not lead to the goal state.
So in pancake problem , we can assign heuristic value as the the largest pancake which is out of place. But in certain states we cannot decide if we are moving towards the goal or away from the goal as there can be two different states in which 3rd pancake is out of place.
Idea behind this method is based on greedy approach. We expand the node which seems closest. Rest of the approach is the same of State tree traversal. Just the successor function is changed. It may happen that we may end up doing more work than a normal bfs or dfs. So worst case possible scenario occurs when there is a badly guided dfs. For example if we use the greedy method and reach very close to the solution but there is no further path there to explore then we will eventually end up doing more work. We can say the greedy method is a reflex agent which does not take account of the consequences of the actions taken.

We can combine the advantages of both greedy and Uniform cost search algorithms. For example we assign two costs in it. The heuristic cost denoted by h(n) and path cost which can be denoted by g(n). So we will move foward by selecting the minimum sum of g(n) and h(n) i;e g(n)+h(n) should be minimum. This algorithm is called A* search algorithm. The rest is the same , just we have used the Queue method as we discussed earlier. Also we dont stop the program/algorithm when we enqueue the goal node. We stop it when we dequeue it else we would miss the optimal solution. In different methods the value givben by g(n) and h(n) will be different. The disadvantages may be that the agent might over estimate the cost to reach the goal node and it will be biased towards one function, hence giving us the wrong solution. So a solution to this might be that we impose a constraint on the heuristic function ,that its value should not exceed the value of actual cost(value of an edge or something like that). In other words the value of h should be optimistic . Such a heuristic function is called admissible heuristic function.
A definition would look like 0<=h(n)<=h*(n) where h*(n) is the true cost to the nearest goal. 

In case of pacman problem, we define the heuristic function as the euclidian distance or the manhatten distance . In case of pancake problem we can define the heuristic function as the largest pancake which is out of order.

Optimality of A* search. 
Imagine B is on the fringe, and A is also on the fringe. Also given that B is suboptimal solution and A is optimal solution. Now we consider an ancestor n of A. So n has already been expanded. Thus n expands before B. Thus we can say f(n) = g(n)+h(n), and f(n)<=f(A) and since B is suboptimal solution, then f(A)<f(B). thus we can say that f(n)<f(B) . and hence A will expand before B. hence we will get the optimal solution before any other suboptimal solution.
We can say that the amount of search done by A* search will be less than the uniform cost search. This is because, in A* we know the direction in which we might get the optimal solution. But in Uniform cost search we dont know the direction of the search, so we end up exploring more values than the A* approach.
Examples of A* applications can be video games, pathing /routing problems , resource planning ,machine translation, speech recognition etc.  

Creating Admissible Heuristics 
In most of the difficult search problems ,solving them optimally requires finding admissible heuristic function. Like it may be a case where our heuristic function is admissible, but takes a lot of computational time, or may not be admissible but takes less computation time. So if our objective is just to find a solution in less time, then we can do a A* search with an inadmissible heuristic function provided that heuristic function takes less computational time.
Often the admissible heuristics are solutions to the relaxed version of actual problem. Lets say we had to go from a place Z to place C. In the actual problem we are supposed to go there through some of the given nodes while the admissible heuristic might calculate the euclidian distance of place Z to place C. So if we relax the constraint of moving through the nodes and allow the agent to directly move from place Z to place C., then it will give the optimal solution. 

Looking at another problem which is the 8 puzzle. In this, we have a grid of 3X3 and have 8 numbers from 1-8 and one empty slot. Now we can move a number to an empty slot to change the configuration of the grid. So given a state we have to move to a state in which we have empty slot as the first position and rest of the numbers are sorted in ascending order. In this case we may define the heuristic function as the number of slides(or numbers) which are displaced from its desired location, but there can be many states in which the heuristic values are the same. This heuristic function is admissible if this a relaxed problem , like the constraint of sliding the slide to the empty location is lifted and we can directly move a slide to another. A statistic provided by Andrew Moore states the number of nodes expanded on certain number of steps to move. In UCS(uniform cost search) for 4 , 8 and 12 steps we had to expand 112,6300,3.6*10^6 nodes respectively. But if we use A* search algorithm and use this heuristic, then for 4,8 and 12 steps we had to expand 13, 39 and 227 nodes respectively. This is significant change in optimization. 

The resources used for the preparation of the nodes are :
video lectures uploaded on the google classroom taught by Dr. Yashasvi Verma ,faculty at Computer Science Department IIT Jodhpur. 
Notes are from 1st september to 11th september. 