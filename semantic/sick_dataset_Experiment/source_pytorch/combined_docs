-----------------------------------------------------------------------
Course Name: Artificial Intelligence
Course Code: CS323
Course Instructor: Dr. Yashaswi Verma
-----------------------------------------------------------------------
Submitted by -
Name: Devin Garg
Roll Number: B18CSE011
-----------------------------------------------------------------------
Class Notes Set-1
Dates Covered: 01 Sep. 2020 - 11 Sep. 2020
-----------------------------------------------------------------------

------------------------BEGIN------------------------------------------

This course of Artificial Intelligence is based on the motivation of creating machines that are 'intelligent' and in some sense can make 'rational' decisions on their own when faced with a problem.

MOTIVATION:
The motivation behind studying this field is manifold. Be it movies, books, or research & development in tech giants, we see a lot of work happening in this direction. The idea in itself, of machines 'thinking' to solve a problem by themselves, is quite fascinating. This in turn makes us think what intelligence actually means in the said context. Humans use the brain to make decisions which is a complex organ and understanding its working to be able to mimic (to some extent) and maybe improve upon it in some tasks is a goal to aim for.

USE CASES:
Over the years the various areas where Artificial Intelligence has found use and made an improvement in human life are as follows - 
1. Natural Language - artificial intelligence has made it possible for an individual's speech to be recognized and text-to-speech synthesis work.
2. Vision - Object and person recognition be it from pictures or videos has been made possible by artificial intelligence. Combined with natural language processing, it has improved the lives of visually challenged individuals with the help of various technologies like scene description, question-answer based on visual data etc.
3. Robotics & Game Playing - Both vision & natural language processing when combined and used in robotics, give startling results. Along with automated robots and cars, there have been advances in computers playing and winning against world champions at multiple games such as chess and GO.

INTRODUCTION:
There is a need to quantify what it is we mean when we talk about responding like humans to situations. When we think about this, we need to make it clear as to what is 'rationally correct' to do in a situation. An interesting point to note here is that rational thinking and how humans react to situations are two distinct things. Therefore, there has been a divided opinion over how machines are to be made 'intelligent'. Are they supposed to act like humans? Or are they supposed to take the 'ideally rational' path? We'll discuss the two one by one.

Making machines more like humans - 
The great computer scientist and mathematician Alan Turing devised a system for measuring how close a machine is to responding like humans. According to the Turing Test, a machine can be said to be capable of thinking like human beings if, on interaction with a human, it is able to respond in a manner that is indistinguishable to how a human might have responded to the same set of situations/questions. However, even though this test remains relevant after all these years, it is believed that studying the underlying principles that lead to the way humans think is more important than just trying to duplicate the process.

Making rational machines - 
To be succinct, rational thinking simply means maximizing expected utility. And making machines rational in a sense is a more objective goal since there is a fixed definition that can be agreed upon. Categorizing a decision as rational depends on 4 basic factors:
The way we define what success for a task means, what the decision making entity knows about the environment, the possible paths or actions that can be taken by the said entity, what all input has been received by the decision making entity till now, so that it can base, it's decisions upon those.

Based on what the outcome of a particular action/decision is, we can express a 'goal' as that outcome which has maximum utility for us based on the measure that we use for 'success'.

Agents and Environments:
On a basic level, we consider an agent to be an entity that receives inputs or 'percepts' from the environment with the help of sensors and acts upon the said environment.
Note that an agent's chosen action can depend on all the inputs that it has received so far but not on anything that it hasn't perceived.
Also, a world state encodes all the information about the environment in any given situation.

Rationality of an Agent: 
Rational agents are those that tend to make decisions that end up maximizing the expected utility as described above. The selection of such rational actions is dictated by the environment, the inputs received by the agent, and the possible set of actions available to the agent. Note that rational agents optimize expected utility which is different from perfection which maximizes actual utility.

Types of Agents:
We learn about two types of agents - reflex agents and planning agents.

Reflex agents tend to work in a greedy manner and take a decision based on the current input and don't 'think' as to what might be the consequences of their actions in the future. They do not take into consideration the effect their actions would have on the 'world state' they are in. Also, reflex agents can be rational in those scenarios where the problem posed to them has the inherent property that just reacting to the situation according to current inputs is sufficient to lead to an outcome of maximized utility.

Planning agents on the other hand tend to look into the possibilities as to what might happen should they take a certain decision. This gives rise to the concept of planning in an optimal or complete way. Complete planning would mean a certain plan being followed by an agent, which is certain to lead to a solution if it exists. Optimal planning on the other hand is a kind of complete planning (which means that it inherently includes reaching to the goal) which 'costs' the least. For this, we need to introduce the idea of how we define 'cost' in a certain world, but in general, we tend to want to solve a problem in as little expenditure as possible. Also, in order to be able to figure out what would happen when a set of actions is taken, the agent needs to know how the environment actually works. What state change happens when a certain step is taken. Without sufficient information in this regard, a planning agent would not be able to perform as expected. Another requirement is fixing a goal. A goal is necessary for a planning agent to be able to terminate it's analysis of possible actions i.e. when a certain set of actions cause the world to reach a goal state, the work of the agent can be said to be done.

Planning vs. Replanning: A planning agent can work in either of two possible ways. It could either work out all the possibilities of actions that can be taken, decide a path (set of steps) that would lead to the goal state optimally, and only then start working on it. On the other hand, it could also start working sooner than that and just keep on replanning it's route until it ends up in one of the goal states. So, it basically divides the entire problem of reaching the goal state into smaller problems, decides an optimal path for them, and ultimately ends up solving the overall problem if a solution exists.

Search Problems:
Reaching the goal state can be modeled as a search problem in which we try to find the path (optimal if possible) to the goal state.
Such a search problem has the following components, in general - 
1. A state space (the states the world can be in)
2. A successor function (mapping of current state to the next state according to available actions) - an encoding of how the world works
3. A start state and a goal state.
A goal test is generally created for the agent to be able to check if the current state is a goal state or not. Note that it is important to consider a successor function that is in some sense compatible with the state space chosen to model the problem.

"Modelling" as a search problem:
As mentioned above, we simply model a real-world problem as a search problem, so the models can vary. The level of abstraction that we deem right for our model is a crucial point of decision.

Search states:
They are DIFFERENT from the world states. A search state only keeps the details that are needed for planning a path for the problem under consideration. So depending upon the problem (goal), the amount of information contained in a search state varies.
E.g. In a game of PacMan, for the 'eat-all-dots' problem we need to include booleans corresponding to all the dots and not just the number of uneaten dots left because to be able to plan optimally the agent would need to know where should the agent head next.

Representations of paths to goal states in a problem:

State Space Graph - A mathematical representation of a search problem.
Nodes are the world configurations that concern us for a given problem. Some of these nodes would satisfy the 'goal test' and would be called goal nodes. (Stressing that they may be more than one in number). Note that since (some information of) states are being represented using nodes, all the states occur only once. (Unlike search trees). Edges represent the information obtained from the successor function defining how state to state transitions take place according to the different actions taken by the agent. Due to the extremely large combinatorial rise in the number of states as the world becomes more and more complex, it is not usually possible to be able to build such graphs in practice.

Search Tree - a tree that contains all possible sequences of actions and the corresponding consequences with the start state as the root node.
It is generally much bigger than even the state space graph as there may be multiple possible ways to reach the same state and there would exist a branch in the search tree for all such possible paths. Note that any node in a search tree represents a complete path in the corresponding state space graph (starting from the root of the search tree and going till the said node). While finding out the optimum path we try to construct the search tree on-demand and as little at a time as possible.

Building a search tree incrementally: General Tree Search
Using the following algorithm we try to explore as little as possible of the giant search tree and still come up with a solution (path).
The algorithm:
1. Initialize the search tree with just the initial state of the problem.
2. Now begin a loop:
   2a. If there are no candidates for expansion left, simply return failure. (At any point of time, the candidates for expansion in a search tree are just the leaves of the tree. 
   2b. Choose any of the leaf nodes for expansion (according to the chosen strategy)
   2c. If the node contains the information of a goal state, return the corresponding solution (i.e. starting from the root node till this node (goal node)).
   2d. Else expand the node and add the resulting nodes to the search tree
3. End Loop

Two crucial things in the above algorithm are:
The Fringe - It stores all the candidates for expansion at any given time. Depending upon the implementation we may have all the predecessors of each element in the fringe so as to be able to trace the path using which we were able to reach that particular node.
Second is the Exploration strategy - This is the strategy that we depend upon to pick a node from the fringe in step 2b. Possible strategies include DFS, BFS, UCS, etc.

Various exploration strategies:
In general, there are four properties that we would want to analyze for a given search algorithm - completeness, optimality, time complexity, and space complexity. 
To be able to quantify these properties let's consider a search tree that has 'c' children for all the nodes. Let 'd' be the maximum depth of the search tree. Note that solutions can exist at various depths in this tree.

Depth First Search - As the name suggests we expand the deepest node first, from the candidates for expansion. Implementing this strategy involves using a stack to store the fringe nodes. In this way, with the help of the last in first out strategy, we will go deeper and deeper along one path, until it terminates and then after popping all those nodes from the stack, we start exploring alternative paths (provided we don't already have a solution). In this algorithm, we may come across multiple situations where the nodes under comparison are at equal depth, in which case we would need to break ties. How we break such ties can also cause a drastic change in the run time of our algorithm.
Properties of DFS:
Completeness - if we can somehow keep track of already occurred nodes and prevent loops, we would be able to say that this approach would lead to a 'complete planning'.
Optimality - This algorithm is not optimal in the sense that since it goes as deep as possible starting from the left, we would just end up getting the leftmost solution irrespective of the cost (say the cost is in the form of the number of steps taken from the start state).
Time Complexity - In the worst case, the goal node could be present at the bottom right-most end of the search tree in which case we might have to traverse the entire tree i.e. O(c^d) complexity.
Space Complexity - As mentioned above, we are using a stack to store the fringe, which means as we go deeper we keep on adding children for all the nodes that come up on the path, but we expand along only one of them. Hence, for 'd' depth (in the worst case) and 'c' children for all the nodes on that path, the space complexity would be O(dc).
 
Breadth-First Search - As the name suggests we expand the shallowest node first, from the candidates for expansion. Implementation of the fringe in this algorithm is using a queue. In this way, utilizing the first in first out strategy, this algorithm first explores all the nodes closest to the current node and then goes on to the next level of the tree.
Properties of BFS:
Completeness - This algorithm is complete in nature i.e. if a solution exists, this algorithm will provide us with it.
Optimality - In general it is not said to be optimal. Even though it will lead to a solution with the minimum number of steps from the root, the cost of each edge is not always one and hence as the edge cost varies, an optimal solution using this approach is not guaranteed.
Time complexity - Again in the worst case the goal node may be in the bottom right corner of the tree in which case we would end up exploring all the nodes with a complexity of O(c^d).
Space complexity - We are using a queue to keep track of the fringe nodes here, so at any given time we store the nodes at the current level that have not yet been expanded along with the children of the nodes that are at the same level as the current node but have been expanded. This would roughly be O(c^d) in the worst case.

BFS vs DFS - a major advantage for the BFS approach if the solution is shallow and to the right of the search tree because via DFS, we might take a very long time to reach the goal.
However, if the goal is at the bottom left, DFS would greatly outperform BFS, as BFS would traverse through all the nodes above the goal before reaching it, whereas DFS would just traverse the goal's ancestors.
If there are memory constraints, DFS would be preferred since we might run out of memory in the case of BFS, before we can actually reach the goal.
In general, if the cost is defined by just the number of actions and it is to be minimized, we would definitely prefer BFS over DFS.

Iterative Deepening - Exploiting the advantageous properties of both BFS (shallow solution quicker) and DFS (no memory problem).
In this algorithm, we run DFS but we cap the maximum depth that it goes to, to some constant. If we are unable to find a solution, we incrementally increase the depth cap and try to find a solution, performing DFS at every turn. This may seem to be very wasteful as we end up doing the computations for the shallow layers again and again. But it turns out, it still would be better because if we look at the work being done, typically the number of nodes in a layer is the sum of the number of nodes in all the previous layers. So, a lot of the work is being done in the last layer anyway.

Uniform Cost Search - This algorithm is useful in those scenarios where we care about the cost of the actions and it is not just about minimizing the number of actions but the overall cost.
Following this strategy, we implement the fringe using a priority queue where the priority of an added node is decided using the cumulative cost from the root to that node. We use this priority queue in a BFS fashion. This means that at any given point of time, we pick that node from the fringe that has the lowest cumulative cost till then. If we implement such a priority queue using a heap, we just have to take the root of the heap and expand it in the search tree. Whenever we find the solution it is bound to be the optimal one because we would have already gone through any paths that cost less than the final one we choose. If any other path were to lead to another goal, it would definitely cost more.
Properties of UCS:
Completeness - Since we are concerned with the cost as well and we choose the minimum one at every step, this algorithm is bound to lead us to the solution if there is one. (Assumptions being that the best solution has a finite cost and the edges all have positive costs).
Optimality - It will give us an optimal path as discussed above in the description.
Time complexity - Say the cheapest solution ends up being the one with cost 'C'. Let the edge costs be at least 'e'. Then the effective depth at which we must have found our solution would be C/e. This means it would be exponential in this effective depth with the base being the number of children 'c'. Hence, O(c^(C/e)) would be the time complexity, as we would have explored all the paths with cost less than the optimal one.
Space Complexity - Here the fringe is implemented using a priority queue which at a time would include the nodes at the end of the boundary of the effective depth. This would lead to a space complexity of O(c^(C/e)) (= all nodes in the boundary of the 'effective depth' level).
Drawbacks: It has no information to use, about the goal state. This causes it to just explore in all directions wherever it can find the smallest cost not caring whether it is actually approaching the goal with a step in that direction or not. This causes some useless computation which will be rectified using A* search below. There we would look at promising leads and not just basing our next step on the 'cheapest so far' approach.

DFS, BFS, and UCS - What unifies them?
The only difference between the above algorithms is the fringe exploration strategy i.e. in what manner do we choose the next node to expand. The unifying factor there could be the implementation of these methods. We could just use a priority queue for all of the above approaches. Just the priority would be different in the three cases, deepest first, shallowest first, minimum cumulative cost first being the priorities to be used respectively. (There will however be an implementational overhead if we use a queue in the case of DFS and BFS, where using a stack and a queue would be cheaper space-wise.)

Where do searches go wrong?
We need to always keep in mind that the problems we are solving using the above approaches are just models of some real-world problems and it is not necessary that there will be compatibility all the time. Any inconsistency with the real-world problem could lead to incorrect results from the algorithm. Ultimately, the search will only be as good as the modeling of the problem.

Search Heuristics:
We came to the understanding that even though UCS gives us the optimal path, it does a lot of wasteful computation since it has no information or guidance as to in what direction should it go on checking first. To this extent, we use search heuristics to give the algorithm an idea or guidance as to where should it look for the goal. 
Formally, a heuristic is a function that estimates how close a state is to a goal. Note that a heuristic is a problem-specific function and cannot be generalized in nature.

Once we have the heuristic function defined for a problem we can look into two algorithms that utilize it and improve performance - Greedy and A*.

Greedy Search - This algorithm works on the principle of expanding the node that seems closest to the goal state at any instant (as it comes to know with the help of the heuristic).
What could go wrong here? 
We could end up choosing a path that looks good at the moment but ultimately leads us to a non-optimal solution. The reason this might happen is that we are not taking into consideration the cost that has already been incurred, and are simply looking at the heuristic to decide the next step. 
In the worst case, this algorithm might work like a DFS that is guided away from the goal. In a way, it would explore all other paths, coming to the goal in the end.

Uniform Cost Search vs Greedy Search:
UCS is slow in the sense that it tries ALL the paths that cost less than the optimal one before coming to a conclusion. Greedy on the other hand works faster as it is guided at every instant to go towards the goal, but it might not work that well as it doesn't take into account the cost already encountered. Now, we could get the best of both by combining ideas from the two.

A* search - expands according to the minimum value of the sum of cumulative cost so far and the heuristic value at the node. In this way, it takes into account the forward (distance from the goal) as well as backward (cumulative cost incurred so far) costs in choosing the next node to be expanded from the fringe. Note that the point of termination of the algorithm is NOT when we enqueue the goal state but when it comes to dequeuing (expanding) a state and it turns out to be a goal state.
Optimality - In general, it does not give an optimal solution if the heuristic is poorly chosen. This means that the heuristic does not estimate the distance from the goal correctly, say it overestimates the distance from the goal. This is called pessimistic behavior of the heuristic.

Admissible Heuristics:
Those heuristics that slow down the bad plans but never outweigh true costs. Pessimistic or inadmissible heuristics on the other hand cause good plans to stay on the fringe for too long and lose optimality in the process.
Formally, 
A heuristic 'h' is admissible (optimistic) if for every node the heuristic value <= the exact cheapest way to get to the goal from that node.

Optimality of A*: If the heuristic is admissible,  A* search would be optimal. 
This can be proved by considering two nodes A and B on a search tree. Say A is an optimal goal node whereas B is a suboptimal goal node. At any point of time, say B is on the fringe along with some ancestor of A (or A itself). This is the case because if B weren't on the fringe, we wouldn't have anything to worry about anyway. Also, it isn't possible that none of A's ancestors (or A itself) is on the fringe. This is because it would mean that it has already been processed and again we wouldn't be in this situation. Now, B will be blocked on the fringe until A is expanded which proves that A* search will always yield an optimal solution in case of an admissible heuristic. The above claim of B being blocked till A is cleared can be verified by simply using the condition of admissible heuristic which needs to be optimistic in estimating the distance from the goal and the fact that at a goal, the heuristic value is zero.

E.g. 8 Puzzle: A problem which involves a 3x3 board with tiles numbered from 1 to 8 placed in it, leaving one empty state. The goal state is arranging the tiles in some fashion, say increasing order from left to right and top to bottom. An admissible heuristic in this case could be the number of tiles that are not in the correct place. This heuristic is admissible because if a tile is not in the position it is supposed to be, it has to be moved at least once.

-------------------------END-------------------------------------------

References: Class discussion of CS323 - Artificial Intelligence.
The slides & video lectures of UC Berkeley CS188.
Russel & Norvig : A modern approach to AI (3rd Edition).
I have not taken any help from any other sources. The above notes are based on my understanding of the content.

--------------------------------------------------------------------------
ï»¿Artificial Intelligence Assignment (I referred the slide and explained in my own words)

What is Constraint satisfaction problem?

We always define the problem as mathematical models. There are many variables associated with the problem.A CSP consists of finite set of variables X1, X2, ..... xn, Nonempty domain of possible values for each variable.All variables
must satisfy various conditions. So, these problems are called Constraint satisfaction problems where all variables must satisfy some constraints.
These are a special subset of search problems.The optimal solution should satisfy all constraints.


There are many different Features of Constraint Satisfaction Problem:
There are various states in the Constraint satisfaction Problem. a state is defined by variables Xi with values from domain D(it may happen that sometimes D depends on i as well).
the path from the start state to the goal state is such that all the variables in the path satisfy all constraints of the problem. All the values in the domain D can't be assigned to any variable.
Otherwise, the constraints may not be satisfied.

Let's take an example of coloring problem.In this problem we are given a map of various cities and we want to assign some colour to each city from the domain D such that no two
adjacent cities have the same colour. there are many different algorithms to assign colour to each city.
we assign variables to each city and our domain D also has some colour in it.
One simple algorithm is Breadth-first search.we start from some start state and assign one colour to start state and while exploring the neighbours of current state, 
we check if this neighbour has already been visited or not. If already visited, we check if the colour assigned to this state is same as the colour assigned to current state or not. If same, we backtrack and assign some different colour to current state and proceed further.
if the neighbour is not already visited, we assign some different colour to neighbour and proceed further.after the end of the algorithm, we will have all the cities assigned some different colour to all cities.

Another example of constraint satisfaction problem is N queens problem.
consider a N * N matrix of cells with each cell either black or white.we want to place N queens on this board(one in each row) such that no two queens are attacking each other.
we assign variables Xij which represent the jth column of ith row of the matrix.The domain is a set containing two numbers 0 and 1. 0 means that no queen is present at that particular cell and 1 means that a queen is present at that particular cell.
Two queens are attacking each other if they are in the same column or in the same row or either they may be attacking diagonally as well.
We use backtracking to solve the problem.we place the queen in some cell if there is no other queen in the same column or in the same row or diagonally as well.we keeep checking each cell whether it is satisfying the constraints or not.
if it is satisfying the constraints, we place the queen in that cell otherwise we move ahead.
after the algorithm ends, we will have the matrix as a binary matrix where each cell is either 1 or 0.

Now , let's see what are Constraint Graph ?
As known graph is a set of vertices and edges where the edges represent the relation between two end vertices.
In this graph, each vertex satisfies the constraint given in the problem.  
There is a problem category called Binary CSP where each constraint relates (at most)two variables.
and binary constraints graph is the graph where nodes are variables and arcs show constraints.
general purpose CSP algorithm use the graph structure to speed up the search.

Another example of CSP is Cryptoarithmetic.
These are nothing but mathematical puzzles in which instead of digits, their values are represented by the letters of the alphabet.
and perform the operation on the letters of the alphabet instead of digits.
suppose for example if we were to add two numbers then we would have to take care of sum and carry.
we would do both of these operations here as well.The only difference being that instead of numbers, we
will see encrypted letters.so, here we will define some variables which will represent the letters of words.
The constraints of the problem is same as in case of digits where we will have sum and carry in terms of words.
say     T W O
      + T W O
      -----------
      F O U R 
Here in this example, we are simply adding two numbers 2 and 2 but using cryptoarithmetic.
the result is also encrypted.

One more example of constraint satisfaction problem is sudoku. we want to fill a 9 * 9 matrix with numbers from 1 to 9 such that no two numbers are same in any row as well as column.also the 3 * 3 submatrix will be filled with numbers from 1 to 9 such that each number
occur exactly once.The domain of the problem is numbers from 1 to 9.we can solve this problem using backtracking.we can start filling the matrix with numbers.we try to fill one cell with one particular number. after filling,we check
whether the constraints are voilated or not. if voilated we pick another number and again try to fill until the constraints are satisfied.we backtrack when the current cell can't be filled with any number and change the
previous cell which was filled.

Now let's discuss about Waltz algorithm.
Waltz algorithm is simply for interpreting line drawing of solid polyhedra as 3d objects.An early example of an AI computation was posed as a CSP.
The approach is very simple.Each intersection is a variable.Adjacent intersection impose constraints on each other.Solution
are physically realizable 3D interpretations.

Now There are various varieties of Constraint satisfaction Problem.
The first one is problem having Discretr variables.They have finite domains. Example of this may be Boolean CSP, including boolean satisfiability which is NP complete.
Another case in discrete variable only is the case of Infinite domain like Integer,string, etc.
example of this may include The very popular Job scheduling, in which the variables are the start and end times for each job.
Linear constraints are solvable while nonlinear are undecidable.

Now let's talk about the Continuous variables.Example of this include start and end times for the Hubble telescope observations.
In this case the linear constraints are solvable in polynomial times by LP method.
There are some varieties of constraints as well.so Unary constraints involve a single variable(which can be thought of to be equivalent to reducing domain).
and in case of binary constraints , they involve pairs of variables.and finally Higher-order constraints involve
3 or more variables.So these are some varieties of constraints.

Now if we see some example of Real world constraints satisfaction problem,
they are Assignment problem(who teaches what class),timetabling problem(which class is ordered when and where),Hardware configuration,Transportation scheduling etc.
Many of these real world problems involve real valued variables.

Now let's see the standard search Formulations of Constraint satisfaction problem.
In this the state are defined by the values assigned so far in.The initial state would be of course empty assignment,{}.
The successor function will assign a value to an unassigned variable.and the goal test is that the current assignment is complete and satisfies all constraints.

So, there are various search methods like BFS, DFS as seen before also.
we will use backtracking search which is the basic uniformed algorithm for solving CSP's.
The basic Idea is to take one variable at a time.since it is cumulative, fix the ordering.
We only need to consider assignments to a single variable at each step.
we also keep checking constraints as we move further.we only consider values which do not conflict.
Depth search technique with these two improvements is called the backtracking search(still not the best.)
It can solve N queens for n nearly equal to 25.

The backtracking example could be our city colouring problem in which we start with 
some city and keep assigning colour to all the neighbours of current city such that the constraint is satisfied. In case constraint is not satisfied,simply backtrack and go back.
The backtracking algorithm works something similar to this:
select unassigned variables and for each variable in order domain values, if value is consistent with assignment given constraint then add variable to assignment
and store the result in recursive backtracking. if result is not same as failure then return result otherwise remove value from the assignment and return failure.

Now let's see how we can improve backtracking.
general purpose Idea give huge gains speed.

we do ordering of the variables.it depends on which variable should be assigned next.and also in what order its values be tried.

another important question is can we detect inevitable failure ? can we exploit the problem structure ?
we use filtering which is simply forward checking. Filtering can be seen as to keep track of domains for unassigned variables and cross of bad options.Simply cross off values that violate the constraints when added.
Forward checking propagates information from assigned to unassigned variables, but does not provide early detection for all failures.

let's define something called as Consistency of a single arc. So, an Arc X -> Y is consistent iff for every x in the tail there is some y in the head which could be assigned without violating the constraints.
The forward checking here is enforcing consistency of the arcs pointing to each new assignment.
a simple form of propagation makes sure that all arcs are consistent.
The imprtant point is that if X loses a value, neighbours of X need to be rechecked.
arc consistency detects failures earlier than forward checking.It can be run as a preprocessor or after each assignment.
we always need to remember that we always delete from the tail not the head.

How to enforce arc consistency in a Constraint satisfaction problem ?
The input is a binary CSP with variable {X1,X2,.....Xn}.
we take a queue of arcs, initially all the arcs are in CSP.
while the queue is not empty:
remove the first node from the queue , then check if the value removed makes the constraints inconsistent, we again add the value to each neighbours from where we took the value out.

There are some limitations of Arc consistency as well.after enforcing arc consistency,it is possible that only one solution
is left.Multiple solution may also be left. and also can have no solution left(and we do not even known it).
Arc consistency still runs inside the backtracking search.

Now let's what is Ordering ?
Variable ordering simply means minimum remaining values.It chooses the variable with the fewest legal left values in its domain.
It is also called most constrained variable.
value ordering is least constraining value.Given a choice of a variable,choose the least constraining value, i.e, the one that rules out the fewest values in the remaining variable.Note that it may take some computation to determine this.(Example is rerunning the filtering).
One interesting fact to notice is that combining these ordering ideas makes even 1000 queens feasible.

What is Arc consistency of an Entire CSP ?
it is a simple form of propagation that makes sure all arcs are simultaneously consistent.Arc consistency detects failure earlier than forward checking.The important
point is that if X loses a value, neighbours of X need to be rechecked.It must be rerun after each assignment.

let's see what this local search is ?
Tree search keeps the unexplored alternatives on the fringe(ensures completeness.)local search improves the single options
until you can't make it better(no fringe is the best).
It is generally much faster and more memory-efficient(but at the same time incomplete and suboptimal.)

Now let's look at what is Hill climbing algorithm ?
It's a greedy algorithm where at each step we explore all the available options
and choose the best one if it better than that of current , otherwise the algorithm simply stops.
it move to the best neighbouring state.One disadvantage of Hill climbing is that it can get stuck in local maximum.
whenever it reaches the local maximum it thinks that it reached the optimal state but it may happen that the current state may not be global optimal
state and so it may not give the optimal answer. 

Let's see what is Simulated Annealing ? 
The idea here is very simple to escape the local maxima by allowing the downhill moves.The more downhill
steps you need to escape a local optimum, the less likely you are to ever make them all in a row.
People think hard about ridge operators which let you jump around the space in better ways.

Let's see what is Genetic Algorithms ?
genetic algorithm is a search heuristic that reflects the process of natural selection where the fittest individuals
are selected for reproduction in order to produce offspring of the next generation.
it keeps the best N hypothesis at each step based on the fitness function.it also has pairwise crossover operators,with optimal mutation to give variety.
It is possibly the most misunderstood, misapplied technique around.

Let's finally see what Beam search is ?
It is also like greedy hill climbing only ,but instead of keeping the only best state, it keeps  K states at all times.
It is the best choice in many practical settings.