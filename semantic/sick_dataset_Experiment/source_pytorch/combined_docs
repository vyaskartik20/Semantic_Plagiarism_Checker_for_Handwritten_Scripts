| Introduction to AI |
We start with the first lecture where we discuss and try to answer the questions what is AI and what it is capable of? We tried to relate our current understanding of AI with the concepts of Sci-Fi movies. We tried to answer the questions: Why do we like such Sci-Fi movies with AI concepts? And what is common between all such movies? We concluded with the answer that we like such movies because this gives us an idea of how things will be in the near or far future and thus gives us a feeling of experiencing the future virtually. Something common between all such movies is that we try to give AI a human-like appearance. This describes the main goal of AI: making machines or programs which are as smart as humans or smarter than humans. After this, we discussed the rationality of the decisions and tried to answer what are rational decisions? Rationality is not associated with the decision that has been made and not the thought process behind that decision meaning it doesn't matter whatever you thought about the problem, how you analysed it, before you came to the decision or how the program or machine has analysed the problem before it made that decision. In scope of rationality, goals are associated with some utility which should be maximized. An example of utility: Utility can be defined as the time spent in reaching the goal subtracted from the time spent in reaching any goal in the worst possible case. Being rational by maximizing this utility means reducing the time taken to achieve the goal and reducing the computational cost. For, this course we adopt the term: Computational Rationality. Then, we discussed the rationality of human brains. Although human brains are not perfect, they can be considered very good at making rational decisions. But, brains are not as easy as a computer program to read, study, reverse-engineer and learn. However, many researches are going on trying to learn about the human brain on how it makes decisions, how it learns any activity, etc. So, that these can be later incorporated in the AI programs to make them better. Then, we discussed the remark,"Brains are to intelligence as wings are to flight." One may take inspiration from the brain to make rational decisions, increase utility of an AI program but such approaches may not always be fruitful. The inspiration of making an airplane came from birds' wings but the airplane is not designed like a bird. In a similar manner brains may act as an inspiration for AI but they cannot be directly implemented feasibly. The two main inspirations that the brain can provide us in terms of making rational decisions are: Memory and Simulation. We need to properly think about a problem if we want to make a rational decision and while thinking about such problems, we're also required to associate them with our past experiences or knowledge(memories). Then, we discussed the tasks which currently AI are capable of doing. AI may be able to buy groceries weekly on the web, but it can't do the same in a grocery store. Although it can answer the questions asked, AI would still fail in successfully conversing with another person. And since humour and art is subjective, we haven't been able to teach AI how to intentionally write a funny story. We also discussed some tasks which AI can do efficiently. Speech technologies, like Google Assistant in our phones, can be considered as a good example where the AI can recognise speech, process it for a response, and convert the text response to a speech. Many examples are there where AI uses vision to learn. In robotics, image processing is used to learn from humans and then train the robot on how to move. Infact, in robotics there are many other AI examples. There are robo-soccer games, self-driving vehicles, etc. AI has also been used in Mathematics and Logic. It is used to develop logical systems which can perform various tasks like proving theorems, question-answering, etc. AI are also capable of defeating humans in games basically because it's easier for them to do high computations. We also discussed examples of AI which are being extensively used around us, like spam emails classified, fraud detection, route-planning (Google maps), search engines, etc.
After being introduced to AI and it's capabilities, we get ourselves into how AI is designed. We start with discussing about agents and designing rational agents based on our understanding of Computational Rationality and utility. An agent can be described as an entity which interprets the situations, processes for what should be done, and then does it. A rational agent is one which always maximizes the utility by the decision it makes. For making rational decisions, the agent should be capable of properly interpreting the environment. Then we discussed how Pac-Man, a game, resembles an agent as it perceives the environment on how it's surroundings are, processes what is the best that can be done and then does it.

| Uninformed Search |
After this, we discussed "Search". We discussed how a normal problem can be analysed as a search problem, what is uninformed search and what are different methods of uninformed search. But, before getting into it we needed to understand more about agents and what are different types of agents. An agent interprets the environment, and taking that as a base thinks of an approach which should be suitable to achieve the goal optimally, and then act accordingly. Based on how they select approaches, agents can be classified into reflex agents and planning agents. Reflex agents are the one which make decisions according to the current environment without considering how your consequent actions will affect the environment. Such decisions can be made on the basis of memory, meaning how it has performed in the past, or it can be made on the basis of defined rules for any particular given environment. This gives rise to the question, "Can a reflex agent be rational?" It would totally depend on the environment and current state of the environment whether a reflex agent acts rationally or not. So, a reflex agent may or may not  be rational. Planning agents are those which make decisions according to the current state of environment as well as the changes that will be made after the action of the agent. So, such a decision is based on a defined set of rules or models on how the environment will change when the agent will make a particular action. For this you also require a well defined goal. So, when it comes to planning agents, two types of plannings are defined: Optimal and Complete planning. A complete planning is the one in which it is ensured that if a goal is achievable, it will be achieved. There can be many ways of achieving the goal, also there may be multiple goals, the planning which achieves the goal with the maximum utility is called an Optimal planning. A planning can be either complete, or both optimal and complete, or even none of them. Some Agents may come up with a whole plan at ones while some may require replanning at some points. These can be termed as planning and replanning agents respectively. Then, we come to "Search". So to convert a problem into a search problem, we must understand what is a state. An environment can be different at different points. All these possible environments are called states. To define a search problem, we need a few things: A state space, which contains possible defined states; A successor function, which defines how a function progresses in the state space(actions); A start state; A goal test which checks if we've reached the goal. A solution can be defined as the possible actions or sequence of actions which lead the problem from start state to goal state. Then we discuss that search problems are just models of the real problem. We can't actually extract htee whole real problem because that would be very difficult to solve, so we define a simpler search problem which is solvable. We understand this using an example of "Travelling in Romania''. The real world problem would be how to go from one city to another, but we can define a simpler search problem to solve the actual real problem. We consider Romania as weighted graph data structure with cities as vertex and distances as the cost of edges connecting cities. Now, looking at the problem, the state space would include the cities. The successor function would include how we move to adjacent cities with the cost(distance) associated. We define a particular city as a start state and we keep a goal test, i.e. if we've reached the city we wanted to. We'll soon come to the solution of this problem. Meanwhile, we take a deeper look into the state space. So, the world state is the state space which includes all the details of the states. And we define a search state which always contains the details which are required or the planning. Considering the example of pathing problem, search state would include states, the defined actions, the successor functions and the goal test. We then define state space sizes. State space size is the no. of states possible for a given problem. Taking example of a Pac-Man game, where a pac-man has to eat all the dots while two ghosts move to shoot at it in one given direction. For this, we define world state, which could include the no. of agent positions, the no. of dots, the no. of ghost positions, and since the pac-man can have different possible directions, the no. of possible directions. We can use permutations and combinations to find out the no. of possible states accordingly which would give us the state space size. We also try to find state space and state space sizes of other examples to understand it better. Now, we come to how these search problems are solved efficiently. For this, we'll have to convert the state space graph, provided by the problem, to search trees. We try to look at the search problem from a mathematical approach and try to view it as a state space graph. In this, nodes can be defined as the states, the edges link a node to its consequent successors or action results, the weight of an edge can be linked to some parameter of the real problem(example - time taken), and there is a goal test which includes the goal nodes. In the state search graph, every node will be unique, stats won't occur more than once. Then we try to view these state search graphs as search trees. This search tree comprises the plans and their outcomes. The root node shows start state, every node shows a state and corresponds to the path or plan that led it there, children are successors or the outcomes of actions. It is not actually feasible to build the complete search tree for most problems. In some cases, the search tree may even be of infinite size, e.g. search tree corresponding to a state search graph containing cycles. 
Now, we explore searching in a search tree. Searching in a search tree includes some steps: expanding the plans, maintaining a fringe of partial plans. We should try to expand as less as possible. There are many methods of searching in a tree. A general search tree initializes the tree with start state, and terminates when there are no more candidates for expansion. The search is based on three important ideas: Fringe, Expansion, Exploration Strategy. The Exploration Strategy decides which nodes are to be explored. We will be discussing three types of search: Depth First Search - DFS, Breadth First Search - BFS, Uniform Cost Search - UCS. First, we'll look into DFS. The idea behind DFS is to completely explore all the paths leading from a particular node before starting to explore its neighbour. DFS is a complete planning, when we avoid cycles, because it will explore the whole tree before it terminates the search. But, DFS isn't an optimal planning. DFS will end when it finds the first goal state. It may occur that there may be a shorter unexplored path to a goal state which never gets explored by DFS because it has already found a goal state while it was looking in depth of a node, i.e it lies on the right side of the found goal state. DFS always settles for the leftmost solution. We can define time complexity and space complexity and no. of nodes in terms of b - the branching factor and m - the maximum depth of the tree. No. of nodes are O(b^m). DFS has a high time complexity O(b^m) but a good space complexity (fringe) equal to O(b*m). 
In BFS, the idea is to explore all the nodes in one level before going to the deeper level. BFS is a complete as well as optimal planning as long as the cost of all edges is 1 or same. The time and space complexity can be defined in terms of branching factor b and depth of shallowest solution s. It has a good time complexity O(b^s) given the solution is shallow. But, it has a very high space complexity(fringe) O(b^s). DFS is a better option when the goal state is to be found very deep. And BFS is a better option when the goal state can be found in shallow levels.
We know that DFS has a good space complexity and BFS has good time complexity for shallow solutions. To combine the advantages of DFS and BFS, iterative deepening is used. The idea of iterative deepening is to limit the depth for DFS and iteratively executing it with a constantly increasing depth limit. This involves a lot of redundant search, i.e. it will explore the explored nodes again and again. And that's why it is suggested only for shallow levels. This is useful because BFS has a very space complexity which this approach can overcome.
BFS is not a cost sensitive search, i.e. it finds the shortest path to goal state in terms of no. of actions, not in terms of costs. Hence, Uniform Cost Search is used when there are costs associated with actions. The idea of UCS is to expand the node which has the least cost. So, we'll add paths to fringe and expand the path which is cheapest and add the new paths to fringe. We then expand the cheapest path at that point. In this, fringe is implemented using priority que with the priority as cumulative cost. Till the time UCS finds a solution, it has explored all the paths cheaper than the cheapest solution. Effective depth can be represented as (C*)/e where C8 is the cost of solution, and e is the minimum cost of the edges. UCS is a complete and optimal planning given that the cheapest solution has a finite cost. The space complexity(fringe) is given by O(b^((C*)/e)). UCS explores the increasing cost contours. For all these uninformed searches we learned about, they have two setbacks: they don't have prior information of the goal location; And UCS, BFS and iterative deepening explore in all the directions and not specifically towards the goals while DFS explores in a particular direction but regardless of goal location. We should also note that the agent doesn't try to explore all the plans actually in the real world, it just simulates. And the search is only as good as the model. Any fault in the model can lead to faulty search, e.g. if a road isn't in the model of Google maps, it will never show that path even if it is shortest.

| Informed Search |
We've seen different Informed Searches. Now, we'll look at the informed searches: Greedy Search and A* Search. Before that, let's brush up our understanding of search. A search problem comprises of states, action and costs, successor function, start state and goal test. For the search problem, we construct a search tree in which nodes represent plans for reaching those states and the plans have a cost, which is sum of the cost of the actions comprising that plan. A search algorithm builds that search tree, defines its way of maintaining the fringe, and aims to find the solution. Some algorithms are successfully able to find the optimal solution. The main setback of the uninformed searches is that they don't have an information of the goal location. We try to overcome this in Informed Search. For an informed search we need an entity that can tell us or guide us regarding the goal location. We define such an entity heuristic as a function that estimates that how far the current state is from the goal state. Every search problem has a different heuristic. To understand heuristic function, we look at the example of a simple pac-man game which has only one dot at some defined coordinates in the 2d space, and there are maze-like structures around. To have an estimate of how far we are from the goal state, we define heuristic as the Euclidean distance between the pacman and the dot. We can also use Manhattan distance as a heuristic here. This will help the agent to avoid searching in directions away from the goal. We look at the example of "Travelling in Romania" problem discussed before. In this we have an estimate of distances of cities from the goal city as a heuristic function. This will help the agent in not searching the paths through the cities which have longer paths unless required. We take a look at an example "Pancake Problem". There are 4 different sized pancakes lying in a column manner. You've to arrange the pancakes in increasing order of size top to bottom. You can flip a complete stack of first i pancakes, 1<=i<=4. And the cost of every action is the no. of pancakes in the stack you flip. Here a heuristic can be used equal to the number of the largest pancake that is still out of place. It would guide the agent.
Now, we come to our first Informed Search, Greedy Search. As it goes by the name, the idea of the greedy search is to lead the path which seems the best at this point. Talking in terms of search trees, we'll expand a node and then expand the path which has the least heuristic value and not look at the previous node's neighbours. We're basically expanding the node that seems closest to the goal. Usually, it is a very good approach and directly leads us to the goal state(may not be optimal). But, in the worst case, it may act like a badly guided DFS and explore the whole tree before exploring the goal state. This may happen due to the poor choices of heuristics. The Greedy Search may lead to suboptimal goals. 
Now, we'll take a look at our second informed search, A* Search. The idea is to overcome the setbacks of UCS and Greedy Search by combining them. UCS expands nodes on basis on backward cumulative costs g(n) and Greedy Search expands nodes on basis of forward estimated costs h(n),i.e. heuristic. In A* we expand just like a greedy algorithm but instead of looking at the h(n) we compare the (h(n) + g(n)) and then expand. This will lead us to optimal solutions. But, the issue is that it may lead us to a suboptimal solution before that optimal solution, so we need to take care that we don't stop the search when we encounter the goal, but stop it when we dequeue a goal. This is because A* search may encounter a suboptimal plan leading to goal before the optimal goal but the algorithm will always dequeue the optimal goal first.
Then we look at an example, where A* search fails. We have a directed graph of three nodes, S- start, G- goal and A- another node. Costs defined as S->A=1, A->G=3, S->G=5. Heuristics are: h(S)=7,h(A)=6,h(G)=0. We can clearly see that S->A->G has a cost 4 and S->G has a cost 5, so the former is optimal. But, when we perform A* search on this, we will get S->G as the result. This happens because of our faulty heuristics. We should have actual bad goal cost< estimated good goal cost to avoid such failures of A* search. So, we need estimates to be less than or equal to actual costs. Here, we introduce the concept of admissibility of the heuristics. An admissible heuristic should may slow down bad plans, but it should never outweigh the true costs. On the other hand, inadmissible heuristics are those which may break optimality bi trapping good plans on the fringe. We define an admissible heuristic h(n) as the one which is always less than or equal to the true cost. 0<=h(n)<=h*(n) where h*(n) is the true cost to a nearest goal. Manhattan and Euclidean distances in Pac-Man example and the largest out of position pancake heuristic in "Pancake Problem '' are examples of admissible heuristics. In the Pac-Man example, Pac-Man has to travel at least the Manhattan distance for reaching the goal. Similarly, the least you have to do to bring the pancakes in order is flipping equivalent of that cost. Finding out admissible heuristics is a major task while using A* search in practice. A* search will give us optimal solutions given that the heuristics are admissible. It can be proved that A* search will give optimal solutions. Let's consider that in the search tree, A is the optimal solution and B is the suboptimal solution. Say we have B in fringe and not A at a point during search. We consider that an ancestor of A, n, is on the fringe. Since heuristic h is admissible, f(n)=g(n)+h(n) will be less than g(A) which is cumulative backward cost till A, and g(A) will be equal to f(A) {h(A)=0}, so f(n)<=f(A). And f(A)<f(B) because g(A)<g(B) because A is optimal and B is suboptimal. So, all ancestors of A and A expand before B. Thus, A* search is optimal.
Now, we'll discuss some properties of A*. Unlike UCS, A* search expands toward the goal whereas UCS expands in all directions equally. A* search just like UCS also ensures optimality. If we compare the Greedy Search, UCS and A* search in Pac-Man example, it can be seen that in greedy search agent explores very less wrong plans before it reaches the goal, A* explores a little more wrong plans before it reaches goal but doesn't explore away from the goal, whereas UCS explores almost all the paths, before it reaches the goal. A* search is also useful in designing automated opponents un video games, robot motion planning, etc.
Now, we take a look at how we create heuristics. Creating Admissible Heuristics is the major part of solving hard search problems. One way to come up with admissible heuristics that we think of relaxed forms of those problems. If we take the example of Manhattan distances in Pac-Man example, We can think of a relaxed form of the problem as a problem where there are no mazes around, so the best way to reach the goal would be by covering the Manhattan distance in terms of least distance travelled. In the "Travelling in Romania" problem, we can see straight line distance as a heuristic which is the outcome of a relaxed problem flying between the initial and final cities. 
We take a look at 8 puzzle problem in which we have a block-shifting puzzle with 9 slots, * blocks namely 1 to 8 and one empty space to enable shifting. Any state can have 2 to 4 successors where actions include shifting blocks into empty spaces. We can define cost as the no. of actions we make. And the goal test will be if they are arranged in order. We look at one type of heuristic in this, in which we take heuristic= no. of tiles misplaced. And it will be admissible, because if you relax the problem of taking out, shuffling and putting it back in that's the least no. of actions you'll have to perform. In this, we can see that this A* search takes significantly less no. of paths explored before we reach goal.

References - 
1) https://www.youtube.com/watch?feature=player_embedded&v=afwPe_OqPX0 
2) https://www.youtube.com/watch?feature=player_embedded&v=OGcG4jSKOVA 


#Admissible Heuristics
An Admissible Heuristic function is a non-negative function is a non-negative function that never overestimates the cost to reach the goal.
One way to construct an admissible heuristic function is to find a solution to a relaxed problem (one with fewer concentrations). Since a solution to the full problem is a solution to the simpler problem, optimal solution to a simpler problem would not be higher than optimal solution to the simpler problem.

#Dominance in heuristics
Given two admissible Heuristic function, if one is greater or equal to another one for every state, then that function Dominates the other function. Greater the dominance, more closer the function to the exact heuristic. We can also combine admissible Heuristics with no dominance to give the max of the set for every state, to get a  function that dominates its set.

#Graph Search
A problem we face in tree search is repetition of expanding the same state more than one time in case of a loop. A way to avoid expanding twice is to have a set of states that are already expanded and skip them is encountered twice. This will not wreck the completeness, since all the viable options are already being considered with the first expansion.

#Consistency of Heuristics
A heuristic is consistent when the Heuristics of a state is less than the sum of the Heuristics of any neighbour state and the actual cost from neighbour to the state. A Consistent heuristic is also admissible and f value of the path never decrease.

#Optimality of graph search
If we use a consistent Heuristic for an A* search problem, the path selected will always have increasing f value; hence the optimal states would be expanded first than the sub-optimal states.
{Proof}

#Planning and Identification problems
While defining a problem, we assume a single agent with deterministic actions is in play, with well-defined state and discrete state space.
A planning problem focuses on the path to the goal, keeping in mind the cost and depth of the path. Heuristics are used as guidance in Heuristic problems.
An Identification based problem focus on the goal instead of the path. Constraint Satisfaction problem is generally focused in these type of problems.

#Constraint Satisfaction Problem
CSP consists of three components, X (set of variables), D(set of allowable value for each variable) and constraint C (set of pair <scope, rel>, where the scope is a tuple of variables that participate in the rel). Constraints can be divided into 3 categories, unary constraints, binary constraints and Higher-order constraints. 
A CSP can be represented by an undirected graph, called Constraint Problem. The nodes represent the variables and are connected to variables that can participate in the given constraint, C. Representing the problem in CSP format makes it easier to solve than create a custom algorithm for each problem. Also, the CSP can eliminate the large swatches of a state-space search.
Let us take the example of a crossword puzzle in which start location and direction of the words are given, along with the list of words to be placed. The variable would be the start location of the word with the direction and its domain would be the accepted words for that variable. The constraints would be the locations where the words intersect should have the same letter.

#Standard Search Formulation
Standard Search Formulation for a CSP consists of 3 components, initial state, the successor function (assigning value to unassigned variables from its domain), and goal test (to check is the current assignment is complete). The basic and slow method to solve the problem would be to generate one by one all the possible variable assignment satisfying the constraints and checking if its the goal state.

#Backtracking and Filtering
A better approach to this would be using a depth-first search algorithm with proper ordering and constraint checking as we go. Trying to place the variables that are highly constrained or with a smaller range (minimum remaining Value(MRV)) and checking the constraint satisfaction as early as possible would make the algorithm more efficient. 
Filtering/Forward Checking the domains of the unassigned variables of bad options added by the constraints added by the existing assignment can help us avoid some inevitable failures which we may encounter later. For eg., in case of the crossword problem, the domain of unassigned words intersecting the current word could be narrowed down early.

#Consistency Driven
A problem is i-consistent if for any choice of domains Dj1, Dj2, .., Dji and for any choice of values in the first i-1 domains there is a value in Dji such that the i-tuple consisting of all these values will not violate any constraint. A problem with i equal to 1 is called Node-consistent. Other problems are called Arc-consistent.
Any problem with unary and binary constraints can be reduced to 1-consistent and 2-consistent. This is the basic assumptions we take while constructing the AC-3 algorithm[refer slide for the algorithm]. After we make the problem consistent, we run a backtracking algorithm to find a solution, if it exists. The limitation here is that the backtracking algorithm still needs to be run even if they have one or no solution left.
