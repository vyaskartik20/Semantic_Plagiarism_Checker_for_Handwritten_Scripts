Weekly Notes 1
Name- Diya Sankhla(B18BB011)
Artificial Intelligence has become one of the favorite storylines for many Hollywood movies and even though it is imaginary and we don’t find it happening in the real world, still people find it very fascinating as we see machines which has thinking powers like Humans, have reasoning and problems solving skills which can make our life better. Broadly speaking Artificial intelligence is intended to make smart and powerful machines that possess intelligence which means thinking power in similar ways like human brains and behave accordingly in a productive manner than humans in some tasks. Computational complexities arise while making decisions Reasonably and sensibly. Arriving at a Rational decision simply involves making choices after reviewing full available information, even minute details, and evaluating all options to choose the best possible alternative to achieve the goals with the highest associated utility. Rational choices aim at maximizing the utility. Inspiration to make intelligent and smart machines that can think and behave rationally like humans are somehow motivated by the functioning of the Human Brain. Neurons in Humans' brains are good at learning and remembering from past experiences but are not always perfect, this helps in making Rational choices. Similarly, Intelligent machines should be designed in a way such that they have a memory to remember information and to create computational simulations.AI can perform several things like working as a bot, good at facial features recognition, speech recognition, and can even perform some household chores, but it still has some limitations like driving in crowded areas or talking successfully with another person for an hour or more[1]. When we are talking about AI, we are defining an Agent who can perform such tasks. An agent can be defined as an artificially intelligent machine, robot, or a computer who can perceive information or signal from an environment and can act accordingly[1]. The selection of rational actions is determined by three factors first, what an agent perceives from the environment, features of the environment, and availability of possible actions. And AI Aims in designing such a Rational agent. How agent plans are classified into two types Reflex Agent and Planning Agent. Reflex agents are the ones who perceive Information from the environment and act based on the current scenario. They don't think about the future consequences of their actions, they just behave[1]. We can't assure that they have a memory to store past decisions. The reflex agent being rational will ultimately depend on the environment and actions taken. While on the other hand Planning agents do not randomly choose any path, they understand the consequences of their actions. They are somewhat aware of their working environment and try to keep track of their path.
Example -Let’s take a simple Pac-man game without a ghost. Here agent aims to eat all the dots. So the agent will choose arbitrarily the path and will start eating dots encountered. But it might be possible that the chosen path at the end has a loop(P shaped path), so the agent will start eating dots on the straight path but after completion of a loop, it will be stuck because no dot left on the traveled path. As a result, it won’t be able to eat all dots in the environment. In this example the agent behaved in a reflex manner, he had not considered the future consequences that he might get stuck after taking this path. While on the other hand planning agent would have chosen a path, considering future outcomes.
Optimal planning implies choosing the best possible path to achieve the goal. While Complete Planning focuses on achieving the goal, not on the path followed to reach the goal therefore the chosen path might not be the best possible path. Example - suppose we have three coordinate locations on a plane A, B, C forming a triangular shape, reaching point C from A can be optimally achieved by taking Euclidean Distance between A and C, instead of taking a path from A to B then B to C.Note that Optimal planning is always complete planning[1].Ideally, we should aim in achieving optimal solutions, and this is where the complexity arises. Planning normally executes the whole plan at a time, while replanning comes with many different sub-plans executed one after another, and such an agent which plans continuously is called a Replanning Agent[1].
To behave rationally agents need to perform some tasks using search algorithms. While Formulating Search Problem different parameters need to be considered, first is state space, which depicts all the available possibilities or states in an environment. The start state is referred to as the initial or starting state and the goal state is a final state which an agent aims to achieve, it could be many. A goal test is some type of function that tells us whether or not the state reached is a goal state[2]. The successor function accounts for actions and the cost associated with the actions, It describes how agents act in given state space. The solution is described as step by step actions taken to achieve the Goal from the start state[2]. While doing search tasks we first take approximations to model the environment. Let’s consider an example of traveling in Romania and here we aim to travel from Arad to Bucharest with the shortest path. In the map all the state space is Cities, while the start state is Arad, Goal Test is Bucharest, and the successor function will be defined as roads which are connecting adjacent cities and cost as distance covered, the solution can be traveling sequentially from Arad-Sibiu-Rimnicu Vâlcea -Pitesti-Bucharest since this path has the lowest Distance[3]. Every Search problem we are trying to approach is determined in the context of the World State. And the World State keeps all possible and minute details of the environment while the search state only considers information needed for planning the path[1]. Let’s take an example based on Pac-Man. Let’s assume the world state consists of possible positions of Pac-Man agent, counts of power dots and normal dots, three ghosts, and their position and agent’s possible actions. And here we have two problems, one which aims in the planning path and another which aims to eat all the dots. While in the first problem, states are location coordinates(x,y), actions can be taken in NSEW directions, the successor function will monitor and update the location as we need to trace our path. The goal test is the end location(x,y). While in the second problem states additionally consists of booleans representing the presence or absence of dots. Actions will be the same as NSEW, the successor function will also include the process of updating boolean to keep track of dots eaten and the dots remaining. Here the Goal test will change and it will represent all eaten dots, while the state could be anything. So we can say that the goal test is not always the same as a goal state[1].
The search problem can be approached either by creating a state-space graph or a Search tree. In the state-space graph, all possible states are represented by nodes, and arrows connecting nodes represent the resulting state as a consequence of actions. Each state will occur only once while constructing a state-space graph, nodes might have multiple edges and they can be approached from several directions.Building a full state space graph is not always feasible and requires large memory for its storage but it can also give us vast information.The search tree begins from the root node which is the initial node and their branches denote child nodes which can be further be explored. Unlike the graph, there might be an iteration of some states. While searching the tree, the path taken to reach the Goal state can be easily traced out. In graphs, if loops are not taken care then it will ultimately lead to an infinite search tree and we won’t be able to reach our goal, so cycles in the graph should be avoided with some constraints. Because of restraint on memory and computational complexity we might never explore a complete search tree. While searching across the Tree we need to maintain a fringe, which will keep a track of nodes that were seen but not yet expanded. So basically fringe consists of partial or sub plans. General Search Tree algorithms will be in such a manner, first, we will start with a frontier containing the root node. Second, we will repeat the following steps, if the frontier does not contain any node then return no solutions. Choose the node from the frontier based on the strategy. If that node contains a goal state then return solution, else expand the node and add it to the frontier. And what sub-plan to be picked from the fringe will completely depend on the Algorithm to be used[1]. Eg if we are implementing a Uniform cost search then the path with the least cumulative cost will be explored first from the fringe.While in the DFS path which will lead to the deepest nodes will be explored first.The tree search algorithm has been broadly classified into two sub-parts Uninformed and Informed Search Algorithm. Let’s first begin with the Uninformed Search Algorithm. The depth-first search is an Uninformed search Algorithm that will begin from the root node and will expand the deepest node in the tree[4].In DFS implementation Last in First Out(LIFO)  principle is followed, which is standard stack. So basically DFS will pick one path and explore until dead-end and then it will backtrack and explore other nodes. If a solution exists and loops are taken care then DFS will always lead to a solution, therefore it is complete. We are not always sure to get an optimal solution as we don’t perform an exhaustive search. If on the first path only we found the goal then the search will stop and it won’t be looking for other solutions that might be optimal. If we define m as max depth and b as branching factor then the Time complexity for search will be O(b^m).DFS has a very high time complexity because it will search for the deepest node in the search tree. The space complexity of DFS will be O(bm), because we don’t need to store every node, but we need to keep track of memory. Breadth-first search is another Blind search algorithm that will always expand the shallowest node simply means it will expand nodes at the same level and then goes to the next level. For maintaining a fringe queue can be used, which follows the First in first out principle. If s is the depth of solution then Time complexity is defined as O(b^s) and space complexity will also remain the same as O(b^s) as here we need to keep track of every expanded node. Yes BFS is complete if the solution will exist and a number of levels will be finite( s will be finite).BFS can only be optimal in a scenario where the cost associated with an action is 1 while moving from one node to another node because later on, the cost will increase as it is cumulative. In average conditions, DFS has better space complexity than BFS.BFS will outperform DFS in a scenario where the branching factor would be less and the goal is near the root node so that it can be easily accessed.DFS can outperform BFS in a scenario that has a high branching factor and solutions would exist for an example in the left-most part of the tree at depth, and if the suppose solution exists at the depth at the rightmost branch then it won't be feasible to use a Depth-first search.Iterative deepening is another search algorithm and it is a combination of both DFS and BFS. It takes advantage of Low space complexity of DFS and BFS ability to find a shallow solution. First, we fix the level of depth (like suppose initially we fix depth to 1 )which we need to iterate, then we will search for a solution at a fixed depth. And if a solution is not found then we will move to the next level of depth and repeat the same process.It seems highly superfluous but still, it is better than DFS, as it finds a solution at the upper level instead of searching one path till the dead-end. This algorithm would work best and redundancy would be reduced when the goal is in the upper right portion of the tree.Another Uninformed search algorithm is Uniform Cost Search which paves the path based on the least cumulative cost. To maintain a fringe, the priority queue can be used, which will prioritize the least cost. While searching it will first explore the node with the least cumulative cost. Yes UCS is both complete(if finite steps and positive cost) and as well as optimal as it considers the least cost at each path.Normally UCS doesn’t explore the complete tree, but if  suppose cost turns out to be negative then we need to explore every node to find the optimal solution.UCS will have the same time and space complexity as DFS which is O(b^number of required steps).Example- Let's take a weighted tree.The first fringe will store the root node since the cumulative cost is 0.Then the Root node will be taken out from the fringe and all successor of the root node will be placed into fringe and the one with the least cumulative cost will be taken out from the fringe and explored further. In such a manner we will explore the current chosen node until a goal is reached with the least cost.As we have seen during searching, the agent tries to simulate all the possibilities to reach the goal and eventually chooses one plan.The accuracy of the model will determine the effectiveness of the search.Example- Google maps are simulations of real-world scenarios.And they sometimes show several paths to reach a particular destination.And suppose if they don’t specify that one of the paths is from someone’s personal property which is not right and here our search algorithm will fail.The issue with a model can affect search results.
One of the foremost limitations of Uninformed Search Algorithms is that their implementation is slow, as they don’t have prior information regarding the goal's location, so they search blindly in all possible directions. And this issue can be resolved by implementing informed search algorithms. Informed searches use a special function called the Heuristic function, it estimates how close or how far an agent is from the goal with respect to a particular state. It gives an estimate not actual value. For each problem, we need to design a unique Heuristic. The heuristic may not provide us an optimal solution but guides an agent towards a goal instead of searching in all directions. The heuristic value of the goal state is always zero. Greedy Search Algorithm is an Informed search algorithm that expands the nodes which look closer to the goal under the guidance of the Heuristic function. The greedy search algorithm is prioritized with the least Heuristic value. If finite nodes then the Greedy Algorithm is complete, but not assured to find an optimal solution. Example- let's take a weighted tree, with the heuristic value defined at each node.First, the initial state is explored and all sub plans are placed into the fringe. The one with the least heuristic value will be extracted from the fringe, not the one with the least cost and similarly, we would explore that current state until it reaches zero heuristic value that is the goal state. Greedy Search Algorithm turns out to be worse in a scenario where we reach the states near the goal, but since there is no path connecting that goal we might end up searching more number of nodes, because of badly guided heuristic function. This can be resolved by putting some constraints on heuristic functions, so to avoid misguidance. The disadvantage of Greedy Search is that it will always choose the path closer towards the goal from its current state and will not understand that moving slightly away from the goal can even give the best solution. So we can interpret that agents implementing greedy search somehow behave reflexively.On average Greedy Search, Algorithm performs better than DFS and BFS, since it has some guidance as compared to searching blindly in all directions.And the performance of the Greedy Search Algorithm will depend on the heuristic function as well as the working environment. At the end, we need an optimal solution with the least amount of exploration. And one such algorithm is A* search.A* search is the combination of both Uniform Cost Search and Greedy Algorithm.UCS has slow implementation but will ultimately reach an optimal solution.On other hand Greedy Search guides towards goals, with the faster implementation.
Example - suppose we have a weighted directed graph with the heuristic value associated with each node and cost associated with paths between two nodes. Earlier while implementing greedy search we only accounted for associated heuristic value, not the cost associated with paths. But here we will combine the strategy of UCS accounting for cumulative associated cost with the path or backward cost which will denote the actual path covered till now as g(n) and Greedy Algorithm strategy of prioritizing forward cost, which indicates a closeness to the goal as h(n). From here we can formulate a search tree where each node is associated with g  as cumulative cost value to reach a particular node and h as heuristic value.A* prioritizes based on the value of g(n) + h(n) . Here g(n) is the cumulative cost, and h(n) value is of a particular node. The main question arises that when we should stop searching, at the time of enqueueing or dequeue of the plan containing goal. While enqueuing the plan we are not considering the minimum cost, and we cannot assure that the chosen plan is optimal. But at the time of dequeue, we would have many sub plans in the fringe and the optimal one can be chosen accordingly. Therefore at the time of dequeue, we can ensure an optimal solution. All the Algorithms discussed so far only differ based on how they are prioritized.DFS is prioritized based on finding the deepest node in the frontier, BFS on finding the shallowest node at a level. While UCS is based on cumulative backward cost, in greedy search heuristic function is considered and in A* search, we use summation of both backward and forward cost.
Before commenting on the optimality of A* search, let’s consider an example. Suppose we have a directed graph defining heuristic value at each node. Two possible plans are connecting the start state and destination, one with the least cumulative cost but with very high heuristic value for the intermediate node even greater than the actual path cost. While others have higher cumulative value compared to path 1. In this, A* is expected to pick the optimal path, but it has selected the path with a higher cumulative cost.It went wrong because the heuristic function of an intermediate node of path 1 has a very high value than the actual path cost,so basically, the heuristic function started over-analyzing which led to a bad solution.A* can work optimally by putting some constraints on the heuristic function in such a way that the value of the Heuristic function at the particular node should be less than or equal to the actual cost required to reach the goal from that state.Such Heuristic functions which underestimate the actual distance to be covered is known as Admissible Heuristic. Inadmissible Heuristic overestimates the actual distance to be covered to reach the goal.If we have multiple nodes to reach the goal then the heuristic function h(n) is admissible if its value is less than or equal to the actual cost of the shortest path and greater or equal to zero.
Let’s try to prove the optimality of A* search when we might have multiple goals. Suppose we have a tree with two Goal nodes A and B.A is considered optimal while B is considered sub-optimal and h can be defined as an admissible heuristic function. Let n be the predecessor node of optimal goal node A. Here we need to prove that optimal node(A) will be considered first while dequeuing the fringe. Let’s presume B is already in the fringe and will proceed further where f(n) is the combination of both g(n) and h(n) , since h is an admissible, value of g(A) is greater than or equal to f(n) and at node A h value is 0 ,so f(A) is equal to g(A).So from here, we conclude that  f(A) is greater than or equal to f(n). The value of g(A) is greater than g(B) because B is a suboptimal node and h value at both goal states will be 0. Therefore we can say f(A) is also greater than f(B).From the above to proven point we can claim that node n is explored before node B.This proves that all the predecessors of the A will be expanded before B.Hence optimality of A* search is proved when h is admissible.
On Comparison with UCS ,A* is expected to search less for goals because UCS rely only on the backward cumulative cost and will explore uniformly in all directions but A* considers additional heuristic functions to avoid blind search which ultimately increases the computational cost of each node , and can be optimal if Admissible Heuristic function is used. Better Heuristic Functions which can provide higher accuracy in estimations are more complex and possess higher computational cost for each node.UCS will explore almost in all direction but end up finding the optimal solution, while greedy search explores nodes towards the direction of goal , but might not promise optimal solution but in the case of A*, it is expected to explore less than UCS but greater than Greedy search and will ultimately reach the optimal solution.
A* has several applications like designing video games eg-Pac Man, used in the analysis of language-searching complete sentences from the information of two or more words,recognition of speech and translation of some text based on available literature etc.
While creating Admissible Heuristic functions we somehow try to relax the problem by removing some constraints,so we can come up with different action plans.Eg supposes we have a map connecting two cities with several intermediate cities. Instead of traveling between different cities to reach the goal via road,we can try taking straight-line path to the goal directly from the start state via flight.In such scenario, we cover less distance compared to actual path distance, and this can be termed as an admissible heuristic. Even though Inadmissible heuristics might not provide optimal solution, we still use in certain problems where we need to find the solution quickly and where the solution is important not optimality.
Example- Suppose we have a 3*3 puzzle with 8 titles numbered 1 to 8 and one empty block.We aim to achieve a state where titles are arranged sequentially(1,2,3….) in a horizontal manner with an empty state at beginning.Here total states will be 9 factorial. Successors can be 2,3,4 in a particular state and actions will be sliding of nodes into empty space according to the strategy. And we can define less cost to the node nearer to the goal.We can define the heuristic function as a number of titles incorrectly placed from its ideal location. And this Heuristic can be admissible if we relax some constraints on tile movement.We can comment that average node expansion in the case of UCS will increase exponentially(because of blind search) when we are 4,8,12…. step away from ideal location compared to A* search(because of Heuristic Function). 


References:
1. http://ai.berkeley.edu/lecture_slides.html
2. https://www.geeksforgeeks.org/search-algorithms-in-ai
3. http://www.cse.iitd.ac.in/~mausam/courses/col333/autumn2019/lectures/02-search.pdf
4. http://how2examples.com/artificial-intelligence/tree-search#:~:text=Tree%20search%20algorithms%20attempt%20to,nodes%20in%20a%20systematic%20way.
﻿Artificial Intelligence Assignment (I referred the slide and explained in my own words)

What is Constraint satisfaction problem?

We always define the problem as mathematical models. There are many variables associated with the problem.A CSP consists of finite set of variables X1, X2, ..... xn, Nonempty domain of possible values for each variable.All variables
must satisfy various conditions. So, these problems are called Constraint satisfaction problems where all variables must satisfy some constraints.
These are a special subset of search problems.The optimal solution should satisfy all constraints.


There are many different Features of Constraint Satisfaction Problem:
There are various states in the Constraint satisfaction Problem. a state is defined by variables Xi with values from domain D(it may happen that sometimes D depends on i as well).
the path from the start state to the goal state is such that all the variables in the path satisfy all constraints of the problem. All the values in the domain D can't be assigned to any variable.
Otherwise, the constraints may not be satisfied.

Let's take an example of coloring problem.In this problem we are given a map of various cities and we want to assign some colour to each city from the domain D such that no two
adjacent cities have the same colour. there are many different algorithms to assign colour to each city.
we assign variables to each city and our domain D also has some colour in it.
One simple algorithm is Breadth-first search.we start from some start state and assign one colour to start state and while exploring the neighbours of current state, 
we check if this neighbour has already been visited or not. If already visited, we check if the colour assigned to this state is same as the colour assigned to current state or not. If same, we backtrack and assign some different colour to current state and proceed further.
if the neighbour is not already visited, we assign some different colour to neighbour and proceed further.after the end of the algorithm, we will have all the cities assigned some different colour to all cities.

Another example of constraint satisfaction problem is N queens problem.
consider a N * N matrix of cells with each cell either black or white.we want to place N queens on this board(one in each row) such that no two queens are attacking each other.
we assign variables Xij which represent the jth column of ith row of the matrix.The domain is a set containing two numbers 0 and 1. 0 means that no queen is present at that particular cell and 1 means that a queen is present at that particular cell.
Two queens are attacking each other if they are in the same column or in the same row or either they may be attacking diagonally as well.
We use backtracking to solve the problem.we place the queen in some cell if there is no other queen in the same column or in the same row or diagonally as well.we keeep checking each cell whether it is satisfying the constraints or not.
if it is satisfying the constraints, we place the queen in that cell otherwise we move ahead.
after the algorithm ends, we will have the matrix as a binary matrix where each cell is either 1 or 0.

Now , let's see what are Constraint Graph ?
As known graph is a set of vertices and edges where the edges represent the relation between two end vertices.
In this graph, each vertex satisfies the constraint given in the problem.  
There is a problem category called Binary CSP where each constraint relates (at most)two variables.
and binary constraints graph is the graph where nodes are variables and arcs show constraints.
general purpose CSP algorithm use the graph structure to speed up the search.

Another example of CSP is Cryptoarithmetic.
These are nothing but mathematical puzzles in which instead of digits, their values are represented by the letters of the alphabet.
and perform the operation on the letters of the alphabet instead of digits.
suppose for example if we were to add two numbers then we would have to take care of sum and carry.
we would do both of these operations here as well.The only difference being that instead of numbers, we
will see encrypted letters.so, here we will define some variables which will represent the letters of words.
The constraints of the problem is same as in case of digits where we will have sum and carry in terms of words.
say     T W O
      + T W O
      -----------
      F O U R 
Here in this example, we are simply adding two numbers 2 and 2 but using cryptoarithmetic.
the result is also encrypted.

One more example of constraint satisfaction problem is sudoku. we want to fill a 9 * 9 matrix with numbers from 1 to 9 such that no two numbers are same in any row as well as column.also the 3 * 3 submatrix will be filled with numbers from 1 to 9 such that each number
occur exactly once.The domain of the problem is numbers from 1 to 9.we can solve this problem using backtracking.we can start filling the matrix with numbers.we try to fill one cell with one particular number. after filling,we check
whether the constraints are voilated or not. if voilated we pick another number and again try to fill until the constraints are satisfied.we backtrack when the current cell can't be filled with any number and change the
previous cell which was filled.

Now let's discuss about Waltz algorithm.
Waltz algorithm is simply for interpreting line drawing of solid polyhedra as 3d objects.An early example of an AI computation was posed as a CSP.
The approach is very simple.Each intersection is a variable.Adjacent intersection impose constraints on each other.Solution
are physically realizable 3D interpretations.

Now There are various varieties of Constraint satisfaction Problem.
The first one is problem having Discretr variables.They have finite domains. Example of this may be Boolean CSP, including boolean satisfiability which is NP complete.
Another case in discrete variable only is the case of Infinite domain like Integer,string, etc.
example of this may include The very popular Job scheduling, in which the variables are the start and end times for each job.
Linear constraints are solvable while nonlinear are undecidable.

Now let's talk about the Continuous variables.Example of this include start and end times for the Hubble telescope observations.
In this case the linear constraints are solvable in polynomial times by LP method.
There are some varieties of constraints as well.so Unary constraints involve a single variable(which can be thought of to be equivalent to reducing domain).
and in case of binary constraints , they involve pairs of variables.and finally Higher-order constraints involve
3 or more variables.So these are some varieties of constraints.

Now if we see some example of Real world constraints satisfaction problem,
they are Assignment problem(who teaches what class),timetabling problem(which class is ordered when and where),Hardware configuration,Transportation scheduling etc.
Many of these real world problems involve real valued variables.

Now let's see the standard search Formulations of Constraint satisfaction problem.
In this the state are defined by the values assigned so far in.The initial state would be of course empty assignment,{}.
The successor function will assign a value to an unassigned variable.and the goal test is that the current assignment is complete and satisfies all constraints.

So, there are various search methods like BFS, DFS as seen before also.
we will use backtracking search which is the basic uniformed algorithm for solving CSP's.
The basic Idea is to take one variable at a time.since it is cumulative, fix the ordering.
We only need to consider assignments to a single variable at each step.
we also keep checking constraints as we move further.we only consider values which do not conflict.
Depth search technique with these two improvements is called the backtracking search(still not the best.)
It can solve N queens for n nearly equal to 25.

The backtracking example could be our city colouring problem in which we start with 
some city and keep assigning colour to all the neighbours of current city such that the constraint is satisfied. In case constraint is not satisfied,simply backtrack and go back.
The backtracking algorithm works something similar to this:
select unassigned variables and for each variable in order domain values, if value is consistent with assignment given constraint then add variable to assignment
and store the result in recursive backtracking. if result is not same as failure then return result otherwise remove value from the assignment and return failure.

Now let's see how we can improve backtracking.
general purpose Idea give huge gains speed.

we do ordering of the variables.it depends on which variable should be assigned next.and also in what order its values be tried.

another important question is can we detect inevitable failure ? can we exploit the problem structure ?
we use filtering which is simply forward checking. Filtering can be seen as to keep track of domains for unassigned variables and cross of bad options.Simply cross off values that violate the constraints when added.
Forward checking propagates information from assigned to unassigned variables, but does not provide early detection for all failures.

let's define something called as Consistency of a single arc. So, an Arc X -> Y is consistent iff for every x in the tail there is some y in the head which could be assigned without violating the constraints.
The forward checking here is enforcing consistency of the arcs pointing to each new assignment.
a simple form of propagation makes sure that all arcs are consistent.
The imprtant point is that if X loses a value, neighbours of X need to be rechecked.
arc consistency detects failures earlier than forward checking.It can be run as a preprocessor or after each assignment.
we always need to remember that we always delete from the tail not the head.

How to enforce arc consistency in a Constraint satisfaction problem ?
The input is a binary CSP with variable {X1,X2,.....Xn}.
we take a queue of arcs, initially all the arcs are in CSP.
while the queue is not empty:
remove the first node from the queue , then check if the value removed makes the constraints inconsistent, we again add the value to each neighbours from where we took the value out.

There are some limitations of Arc consistency as well.after enforcing arc consistency,it is possible that only one solution
is left.Multiple solution may also be left. and also can have no solution left(and we do not even known it).
Arc consistency still runs inside the backtracking search.

Now let's what is Ordering ?
Variable ordering simply means minimum remaining values.It chooses the variable with the fewest legal left values in its domain.
It is also called most constrained variable.
value ordering is least constraining value.Given a choice of a variable,choose the least constraining value, i.e, the one that rules out the fewest values in the remaining variable.Note that it may take some computation to determine this.(Example is rerunning the filtering).
One interesting fact to notice is that combining these ordering ideas makes even 1000 queens feasible.

What is Arc consistency of an Entire CSP ?
it is a simple form of propagation that makes sure all arcs are simultaneously consistent.Arc consistency detects failure earlier than forward checking.The important
point is that if X loses a value, neighbours of X need to be rechecked.It must be rerun after each assignment.

let's see what this local search is ?
Tree search keeps the unexplored alternatives on the fringe(ensures completeness.)local search improves the single options
until you can't make it better(no fringe is the best).
It is generally much faster and more memory-efficient(but at the same time incomplete and suboptimal.)

Now let's look at what is Hill climbing algorithm ?
It's a greedy algorithm where at each step we explore all the available options
and choose the best one if it better than that of current , otherwise the algorithm simply stops.
it move to the best neighbouring state.One disadvantage of Hill climbing is that it can get stuck in local maximum.
whenever it reaches the local maximum it thinks that it reached the optimal state but it may happen that the current state may not be global optimal
state and so it may not give the optimal answer. 

Let's see what is Simulated Annealing ? 
The idea here is very simple to escape the local maxima by allowing the downhill moves.The more downhill
steps you need to escape a local optimum, the less likely you are to ever make them all in a row.
People think hard about ridge operators which let you jump around the space in better ways.

Let's see what is Genetic Algorithms ?
genetic algorithm is a search heuristic that reflects the process of natural selection where the fittest individuals
are selected for reproduction in order to produce offspring of the next generation.
it keeps the best N hypothesis at each step based on the fitness function.it also has pairwise crossover operators,with optimal mutation to give variety.
It is possibly the most misunderstood, misapplied technique around.

Let's finally see what Beam search is ?
It is also like greedy hill climbing only ,but instead of keeping the only best state, it keeps  K states at all times.
It is the best choice in many practical settings.